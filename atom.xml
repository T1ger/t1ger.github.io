<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>t1ger的茶馆</title>
  <subtitle>头顶有光终是幻，足下生云未是仙</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://t1ger.github.io/"/>
  <updated>2018-09-03T10:22:33.977Z</updated>
  <id>https://t1ger.github.io/</id>
  
  <author>
    <name>t1ger</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>How to install CDH6.0 Cluster on Centos7.5</title>
    <link href="https://t1ger.github.io/2018/09/03/How-to-install-CDH6-0-Cluster-on-Centos7-5/"/>
    <id>https://t1ger.github.io/2018/09/03/How-to-install-CDH6-0-Cluster-on-Centos7-5/</id>
    <published>2018-09-03T02:21:26.000Z</published>
    <updated>2018-09-03T10:22:33.977Z</updated>
    
    <content type="html"><![CDATA[<h5 id="Cloudera-简介"><a href="#Cloudera-简介" class="headerlink" title="Cloudera 简介"></a><b>Cloudera 简介</b></h5><ul>
<li>Cloudera 官网：<a href="https://www.cloudera.com" target="_blank" rel="external">https://www.cloudera.com</a></li>
<li>Cloudera 官方文档： <a href="https://www.cloudera.com/documentation/enterprise/latest.html" target="_blank" rel="external">https://www.cloudera.com/documentation/enterprise/latest.html</a></li>
</ul>
<h5 id="安装Cloudera-Manager和CDH"><a href="#安装Cloudera-Manager和CDH" class="headerlink" title="安装Cloudera Manager和CDH"></a><b>安装Cloudera Manager和CDH</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">系统环境：CentOS7.5 </div><div class="line">软件环境：Oracle JDK、Cloudera Manager Server 和 Agent 、数据库、CDH各组件</div></pre></td></tr></table></figure>
<ul>
<li><p>系统初始化<br>关闭防火墙 禁用selinux,服务器之间免密，时间保持同步</p>
</li>
<li><p>Cloudera安装,官方文档参考<a href="https://www.cloudera.com/documentation/enterprise/6/6.0/topics/install_cm_cdh.html" target="_blank" rel="external">这里</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line">#Configure a Repository</div><div class="line">wget https://archive.cloudera.com/cm6/6.0.0/redhat7/yum/cloudera-manager.repo -P /etc/yum.repos.d/</div><div class="line">sudo rpm --import https://archive.cloudera.com/cm6/6.0.0/redhat7/yum/RPM-GPG-KEY-cloudera</div><div class="line"></div><div class="line">#Installing the JDK</div><div class="line">sudo yum install oracle-j2sdk1.8</div><div class="line"></div><div class="line">#Install Cloudera Manager Packages</div><div class="line">sudo yum install cloudera-manager-daemons cloudera-manager-agent cloudera-manager-server</div><div class="line"></div><div class="line">#Install Databases</div><div class="line">wget http://repo.mysql.com/mysql-community-release-el7-5.noarch.rpm</div><div class="line">sudo rpm -ivh mysql-community-release-el7-5.noarch.rpm</div><div class="line">sudo yum update</div><div class="line">sudo yum install mysql-server</div><div class="line">sudo systemctl start mysqld</div><div class="line"></div><div class="line">#Set up the Cloudera Manager Database</div><div class="line">1. /opt/cloudera/cm/schema/scm_prepare_database.sh \</div><div class="line">[options] &lt;databaseType&gt; &lt;databaseName&gt; &lt;databaseUser&gt; &lt;password&gt;</div><div class="line">2. If it exists, remove the embedded PostgreSQL properties file:</div><div class="line">sudo rm /etc/cloudera-scm-server/db.mgmt.properties</div><div class="line"></div><div class="line">Example：</div><div class="line">sudo /opt/cloudera/cm/schema/scm_prepare_database.sh mysql scm scm</div><div class="line"></div><div class="line">#Install CDH and Other Software</div><div class="line">sudo systemctl start cloudera-scm-server</div><div class="line"></div><div class="line">we go to browser http://&lt;server_host&gt;:7180,login admin/admin</div></pre></td></tr></table></figure>
</li>
<li><p>创建必需的数据库，可以参考<a href="https://www.cloudera.com/documentation/enterprise/latest/topics/install_cm_mariadb.html" target="_blank" rel="external">这里</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">create database metastore DEFAULT CHARACTER SET utf8;</div><div class="line">grant all on metastore.* TO &apos;hive&apos;@&apos;%&apos; IDENTIFIED BY &apos;hive&apos;;</div><div class="line"></div><div class="line">create database amon DEFAULT CHARACTER SET utf8;</div><div class="line">grant all on amon.* TO &apos;amon&apos;@&apos;%&apos; IDENTIFIED BY &apos;amon&apos;;</div><div class="line"></div><div class="line">create database hue DEFAULT CHARACTER SET utf8;</div><div class="line">grant all on hue.* TO &apos;hue&apos;@&apos;%&apos; IDENTIFIED BY &apos;hue&apos;;</div><div class="line"></div><div class="line">create database rman DEFAULT CHARACTER SET utf8;</div><div class="line">grant all on rman.* TO &apos;rman&apos;@&apos;%&apos; IDENTIFIED BY &apos;rman&apos;;</div><div class="line"></div><div class="line">create database navms DEFAULT CHARACTER SET utf8;</div><div class="line">grant all on navms.* TO &apos;navms&apos;@&apos;%&apos; IDENTIFIED BY &apos;navms&apos;;</div><div class="line"></div><div class="line">create database nas DEFAULT CHARACTER SET utf8;</div><div class="line">grant all on nas.* TO &apos;nas&apos;@&apos;%&apos; IDENTIFIED BY &apos;nas&apos;;</div><div class="line"></div><div class="line">create database oos DEFAULT CHARACTER SET utf8;</div><div class="line">grant all on oos.* TO &apos;oos&apos;@&apos;%&apos; IDENTIFIED BY &apos;oos&apos;;</div></pre></td></tr></table></figure>
</li>
</ul>
<h5 id="卸载重装CM服务"><a href="#卸载重装CM服务" class="headerlink" title="卸载重装CM服务"></a><b>卸载重装CM服务</b></h5><p>如果，第一次没有安装成功，那这部分就对你有帮助了<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"># 安装CDH manager的服务器上面执行</div><div class="line">yum remove cloudera-manager-server -y</div><div class="line"></div><div class="line"># 在所有的服务器执行下面操作</div><div class="line">systemctl stop  cloudera-scm-agent</div><div class="line">yum remove cloudera-manager-agennt-y</div><div class="line">ps -ef | grep cmf | grep -v grep | awk &apos;&#123;print $2&#125;&apos; | xargs kill -9</div><div class="line">find / -name clouder* | xargs rm -rf </div><div class="line">find / -name cmf* | xargs rm -rf</div></pre></td></tr></table></figure></p>
<h5 id="Custom-Installation-Solutions"><a href="#Custom-Installation-Solutions" class="headerlink" title="Custom Installation Solutions"></a><b>Custom Installation Solutions</b></h5><p>如果在线安装很慢，我们可以通过以下方式来加速安装</p>
<ul>
<li><p>Creating a Permanent Internal Repository(Option)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">#Setting Up a Web Server</div><div class="line">sudo yum install httpd</div><div class="line">sudo systemctl start httpd</div><div class="line"></div><div class="line">#Edit the Apache HTTP Server configuration file (/etc/httpd/conf/httpd.conf by default) to add or edit the following line in the &lt;IfModule mime_module&gt; section:</div><div class="line">AddType application/x-gzip .gz .tgz .parcel</div><div class="line"></div><div class="line">#Downloading and Publishing the Package Repository</div><div class="line"></div><div class="line">#Cloudera Manager 6</div><div class="line">sudo mkdir -p /var/www/html/cloudera-repos</div><div class="line">sudo wget --recursive --no-parent --no-host-directories https://archive.cloudera.com/cm6/6.0.0/redhat7/ -P /var/www/html/cloudera-repos</div><div class="line">sudo chmod -R ugo+rX /var/www/html/cloudera-repos/cm6</div><div class="line"></div><div class="line">#CDH 6</div><div class="line">sudo mkdir -p /var/www/html/cloudera-repos</div><div class="line">sudo wget --recursive --no-parent --no-host-directories https://archive.cloudera.com/cdh6/6.0.0/redhat7/ -P /var/www/html/cloudera-repos</div><div class="line"></div><div class="line">sudo wget --recursive --no-parent --no-host-directories https://archive.cloudera.com/gplextras6/6.0.0/redhat7/ -P /var/www/html/cloudera-repos</div><div class="line"></div><div class="line">sudo chmod -R ugo+rX /var/www/html/cloudera-repos/cdh6</div><div class="line">sudo chmod -R ugo+rX /var/www/html/cloudera-repos/gplextras6</div></pre></td></tr></table></figure>
</li>
<li><p>Creating a Temporary Internal Repository(Option)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"># Download the repository you need following the instructions in Downloading and Publishing the Package Repository.</div><div class="line">cd /var/www/html</div><div class="line">python -m SimpleHTTPServer 8900</div><div class="line"></div><div class="line">#Visit the Repository URL http://&lt;web_server&gt;:8900/cloudera-repos/</div></pre></td></tr></table></figure>
</li>
<li><p>Configuring Hosts to Use the Internal Repository</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"># Create /etc/yum.repos.d/cloudera-repo.repo files on cluster hosts with the following content, </div><div class="line">where &lt;web_server&gt; is the hostname of the web server:</div><div class="line"></div><div class="line">[cloudera-repo]</div><div class="line">name=cloudera-repo</div><div class="line">baseurl=http://&lt;web_server&gt;/cm/5</div><div class="line">enabled=1</div><div class="line">gpgcheck=0</div></pre></td></tr></table></figure>
</li>
</ul>
<h5 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a><b>遇到的问题</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"># jdbc驱动未找到</div><div class="line">mkdir -p /usr/share/java</div><div class="line">rz mysql-connector-java-5.1.45-bin.jar</div><div class="line">ln -s mysql-connector-java-5.1.45-bin.jar mysql-connector-java.jar</div><div class="line">systemctl restart cloudera-scm-server.service</div><div class="line"></div><div class="line"># host命令为找到</div><div class="line">yum install bind-utils -y</div></pre></td></tr></table></figure>
<p>ref<br><a href="https://gist.github.com/lilongen/b179b3868d2c2839ca7303b7605ce16b" target="_blank" rel="external">deploy-cm-cdh-on-centos7</a><br><a href="https://www.cloudera.com/documentation/enterprise/6/6.0/topics/cm_ig_create_local_package_repo.html#download_publish_package_repo" target="_blank" rel="external">Custom Installation Solutions</a><br><a href="https://www.cloudera.com/documentation/enterprise/6/latest/topics/cm_ig_create_local_parcel_repo.html#" target="_blank" rel="external">Using an Internally Hosted Remote Parcel Repository</a></p>
<hr>
<p>您的鼓励是我写作最大的动力</p>
<p>俗话说，投资效率是最好的投资。 如果您感觉我的文章质量不错，读后收获很大，预计能为您提高 10% 的工作效率，不妨小额捐助我一下，让我有动力继续写出更多好文章。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;Cloudera-简介&quot;&gt;&lt;a href=&quot;#Cloudera-简介&quot; class=&quot;headerlink&quot; title=&quot;Cloudera 简介&quot;&gt;&lt;/a&gt;&lt;b&gt;Cloudera 简介&lt;/b&gt;&lt;/h5&gt;&lt;ul&gt;
&lt;li&gt;Cloudera 官网：&lt;a href=&quot;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Elasticsearch6.3的新特性</title>
    <link href="https://t1ger.github.io/2018/06/15/Elasticsearch6-3%E7%9A%84%E6%96%B0%E7%89%B9%E6%80%A7/"/>
    <id>https://t1ger.github.io/2018/06/15/Elasticsearch6-3的新特性/</id>
    <published>2018-06-15T03:02:17.000Z</published>
    <updated>2018-06-20T06:57:05.131Z</updated>
    
    <content type="html"><![CDATA[<p>Elasticsearch 官方推出了6.3.0，今天，让我们看一下 Elasticsearch6.3.0给我们带来的新特性吧，如果想看官网的同学可以参考<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/release-notes-6.3.0.html" target="_blank" rel="external">这里</a></p>
<ul>
<li>Elasticsearch6.3默认包含了X-Pack,X-Pack包括APM,Canvas</li>
<li>支持SQL </li>
<li>支持Java 10</li>
<li>汇总统计</li>
<li>安全更新</li>
</ul>
<h5 id="支持SQL"><a href="#支持SQL" class="headerlink" title="支持SQL"></a><b>支持SQL</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div></pre></td><td class="code"><pre><div class="line">#创建index并添加数据</div><div class="line">[root@localhost ~]# curl -X PUT &quot;localhost:9200/library/book/_bulk?refresh&quot; -H &apos;Content-Type: application/json&apos; -d&apos;</div><div class="line">&gt; &#123;&quot;index&quot;:&#123;&quot;_id&quot;: &quot;Leviathan Wakes&quot;&#125;&#125;</div><div class="line">&gt; &#123;&quot;name&quot;: &quot;Leviathan Wakes&quot;, &quot;author&quot;: &quot;James S.A. Corey&quot;, &quot;release_date&quot;: &quot;2011-06-02&quot;, &quot;page_count&quot;: 561&#125;</div><div class="line">&gt; &#123;&quot;index&quot;:&#123;&quot;_id&quot;: &quot;Hyperion&quot;&#125;&#125;</div><div class="line">&gt; &#123;&quot;name&quot;: &quot;Hyperion&quot;, &quot;author&quot;: &quot;Dan Simmons&quot;, &quot;release_date&quot;: &quot;1989-05-26&quot;, &quot;page_count&quot;: 482&#125;</div><div class="line">&gt; &#123;&quot;index&quot;:&#123;&quot;_id&quot;: &quot;Dune&quot;&#125;&#125;</div><div class="line">&gt; &#123;&quot;name&quot;: &quot;Dune&quot;, &quot;author&quot;: &quot;Frank Herbert&quot;, &quot;release_date&quot;: &quot;1965-06-01&quot;, &quot;page_count&quot;: 604&#125;</div><div class="line">&gt; &apos;</div><div class="line">&#123;&quot;took&quot;:426,&quot;errors&quot;:false,&quot;items&quot;:[&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;library&quot;,&quot;_type&quot;:&quot;book&quot;,&quot;_id&quot;:&quot;Leviathan Wakes&quot;,&quot;_version&quot;:1,&quot;result&quot;:&quot;created&quot;,&quot;forced_refresh&quot;:true,&quot;_shards&quot;:&#123;&quot;total&quot;:2,&quot;successful&quot;:1,&quot;failed&quot;:0&#125;,&quot;_seq_no&quot;:0,&quot;_primary_term&quot;:1,&quot;status&quot;:201&#125;&#125;,&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;library&quot;,&quot;_type&quot;:&quot;book&quot;,&quot;_id&quot;:&quot;Hyperion&quot;,&quot;_version&quot;:1,&quot;result&quot;:&quot;created&quot;,&quot;forced_refresh&quot;:true,&quot;_shards&quot;:&#123;&quot;total&quot;:2,&quot;successful&quot;:1,&quot;failed&quot;:0&#125;,&quot;_seq_no&quot;:0,&quot;_primary_term&quot;:1,&quot;status&quot;:201&#125;&#125;,&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;library&quot;,&quot;_type&quot;:&quot;book&quot;,&quot;_id&quot;:&quot;Dune&quot;,&quot;_version&quot;:1,&quot;result&quot;:&quot;created&quot;,&quot;forced_refresh&quot;:true,&quot;_shards&quot;:&#123;&quot;total&quot;:2,&quot;successful&quot;:1,&quot;failed&quot;:0&#125;,&quot;_seq_no&quot;:1,&quot;_primary_term&quot;:1,&quot;status&quot;:201&#125;&#125;]&#125;</div><div class="line"></div><div class="line">#通过SQL REST API执行SQL</div><div class="line">[root@localhost ~]# curl -X POST &quot;localhost:9200/_xpack/sql?format=txt&quot; -H &apos;Content-Type: application/json&apos; -d&apos;</div><div class="line">&gt; &#123;</div><div class="line">&gt;     &quot;query&quot;: &quot;SELECT * FROM library WHERE release_date &lt; \u00272000-01-01\u0027&quot;</div><div class="line">&gt; &#125;</div><div class="line">&gt; &apos;</div><div class="line">    author     |     name      |  page_count   |      release_date      </div><div class="line">---------------+---------------+---------------+------------------------</div><div class="line">Dan Simmons    |Hyperion       |482            |1989-05-26T00:00:00.000Z</div><div class="line">Frank Herbert  |Dune           |604            |1965-06-01T00:00:00.000Z</div><div class="line"></div><div class="line"></div><div class="line">#支持SQL REST API</div><div class="line">[root@localhost ~]# curl -X POST &quot;localhost:9200/_xpack/sql/translate&quot; -H &apos;Content-Type: application/json&apos; -d&apos;</div><div class="line">&gt; &#123;</div><div class="line">&gt;     &quot;query&quot;: &quot;SELECT * FROM library ORDER BY page_count DESC&quot;,</div><div class="line">&gt;     &quot;fetch_size&quot;: 10</div><div class="line">&gt; &#125;</div><div class="line">&gt; &apos;</div><div class="line">&#123;&quot;size&quot;:10,&quot;_source&quot;:&#123;&quot;includes&quot;:[&quot;author&quot;,&quot;name&quot;],&quot;excludes&quot;:[]&#125;,&quot;docvalue_fields&quot;:[&quot;page_count&quot;,&quot;release_date&quot;],&quot;sort&quot;:[&#123;&quot;page_count&quot;:&#123;&quot;order&quot;:&quot;desc&quot;&#125;&#125;]&#125;</div><div class="line"></div><div class="line">#支持SQL CLI</div><div class="line">[root@localhost ~]# /usr/share/elasticsearch/bin/elasticsearch-sql-cli</div><div class="line">     .sssssss.`                     .sssssss.</div><div class="line">  .:sXXXXXXXXXXo`                `ohXXXXXXXXXho.</div><div class="line"> .yXXXXXXXXXXXXXXo`            `oXXXXXXXXXXXXXXX-</div><div class="line">.XXXXXXXXXXXXXXXXXXo`        `oXXXXXXXXXXXXXXXXXX.</div><div class="line">.XXXXXXXXXXXXXXXXXXXXo.    .oXXXXXXXXXXXXXXXXXXXXh</div><div class="line">.XXXXXXXXXXXXXXXXXXXXXXo``oXXXXXXXXXXXXXXXXXXXXXXy</div><div class="line">`yXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX.</div><div class="line"> `oXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXo`</div><div class="line">   `oXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXo`</div><div class="line">     `oXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXo`</div><div class="line">       `oXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXo`</div><div class="line">         `oXXXXXXXXXXXXXXXXXXXXXXXXXXXXo`</div><div class="line">           .XXXXXXXXXXXXXXXXXXXXXXXXXo`</div><div class="line">         .oXXXXXXXXXXXXXXXXXXXXXXXXo`</div><div class="line">       `oXXXXXXXXXXXXXXXXXXXXXXXXo`   `odo`</div><div class="line">     `oXXXXXXXXXXXXXXXXXXXXXXXXo`   `oXXXXXo`</div><div class="line">   `oXXXXXXXXXXXXXXXXXXXXXXXXo`   `oXXXXXXXXXo`</div><div class="line"> `oXXXXXXXXXXXXXXXXXXXXXXXXo`   `oXXXXXXXXXXXXXo`</div><div class="line">`yXXXXXXXXXXXXXXXXXXXXXXXo`    oXXXXXXXXXXXXXXXXX.</div><div class="line">.XXXXXXXXXXXXXXXXXXXXXXo`   `oXXXXXXXXXXXXXXXXXXXy</div><div class="line">.XXXXXXXXXXXXXXXXXXXXo`     /XXXXXXXXXXXXXXXXXXXXX</div><div class="line">.XXXXXXXXXXXXXXXXXXo`        `oXXXXXXXXXXXXXXXXXX-</div><div class="line"> -XXXXXXXXXXXXXXXo`            `oXXXXXXXXXXXXXXXo`</div><div class="line">  .oXXXXXXXXXXXo`                `oXXXXXXXXXXXo.</div><div class="line">    `.sshXXyso`        SQL         `.sshXhss.`</div><div class="line"></div><div class="line">sql&gt; SELECT * FROM library WHERE release_date &lt; &apos;2000-01-01&apos;;</div><div class="line">    author     |     name      |  page_count   |      release_date      </div><div class="line">---------------+---------------+---------------+------------------------</div><div class="line">Dan Simmons    |Hyperion       |482            |1989-05-26T00:00:00.000Z</div><div class="line">Frank Herbert  |Dune           |604            |1965-06-01T00:00:00.000Z</div><div class="line"></div><div class="line">#支持SQL JDBC</div><div class="line"></div><div class="line">String address = &quot;jdbc:es://&quot; + elasticsearchAddress;     </div><div class="line">Properties connectionProperties = connectionProperties(); </div><div class="line">Connection connection = DriverManager.getConnection(address, connectionProperties);</div><div class="line">try (Statement statement = connection.createStatement();</div><div class="line">        ResultSet results = statement.executeQuery(</div><div class="line">            &quot;SELECT name, page_count FROM library ORDER BY page_count DESC LIMIT 1&quot;)) &#123;</div><div class="line">    assertTrue(results.next());</div><div class="line">    assertEquals(&quot;Don Quixote&quot;, results.getString(1));</div><div class="line">    assertEquals(1072, results.getInt(2));</div><div class="line">    SQLException e = expectThrows(SQLException.class, () -&gt; results.getInt(1));</div><div class="line">    assertTrue(e.getMessage(), e.getMessage().contains(&quot;unable to convert column 1 to an int&quot;));</div><div class="line">    assertFalse(results.next());</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h5 id="支持Java-10"><a href="#支持Java-10" class="headerlink" title="支持Java 10"></a><b>支持Java 10</b></h5><p>Elasticsearch 支持Java9 和Java10，保持和Java快速发布周期一致。但是官方建议大多数用户使用java8，有兴趣的同学可以看<a href="https://jaxenter.com/no-more-public-updates-java-8-143703.html" target="_blank" rel="external">这里</a></p>
<h5 id="汇总统计"><a href="#汇总统计" class="headerlink" title="汇总统计"></a><b>汇总统计</b></h5><p>用户可以建立汇总统计job，job会汇聚统计最近搜索的更新数据。这个功能和SQl功能一样，都是实验性质的功能。</p>
<h5 id="安全更新"><a href="#安全更新" class="headerlink" title="安全更新"></a><b>安全更新</b></h5><p>XPackExtension  扩展机制被移除了，引入SPI扩展机制。</p>
<h5 id="Bug-fixes"><a href="#Bug-fixes" class="headerlink" title="Bug fixes"></a><b>Bug fixes</b></h5><p>具体可以参考<a href="https://www.elastic.co/guide/en/logstash/6.3/logstash-6-3-0.html" target="_blank" rel="external">这里</a></p>
<p>说了这么多，也许有朋友问了，怎么才能升级到Elasticsearch6.3呢，官方建议是：、<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">1. 6.x到6.y - 可以通过一次升级一个节点来执行</div><div class="line">2. 5.x至6.x - 需要完全重启群集</div><div class="line">3. 2.x至6.x - 不支持</div><div class="line"></div><div class="line">[root@localhost ~]# curl -XGET &apos;http://localhost:9200/&apos;</div><div class="line">&#123;</div><div class="line">  &quot;name&quot; : &quot;99NPxaU&quot;,</div><div class="line">  &quot;cluster_name&quot; : &quot;elasticsearch&quot;,</div><div class="line">  &quot;cluster_uuid&quot; : &quot;gW7bp0I3RNKvZI50SAsjeg&quot;,</div><div class="line">  &quot;version&quot; : &#123;</div><div class="line">    &quot;number&quot; : &quot;6.3.0&quot;,</div><div class="line">    &quot;build_flavor&quot; : &quot;default&quot;,</div><div class="line">    &quot;build_type&quot; : &quot;rpm&quot;,</div><div class="line">    &quot;build_hash&quot; : &quot;424e937&quot;,</div><div class="line">    &quot;build_date&quot; : &quot;2018-06-11T23:38:03.357887Z&quot;,</div><div class="line">    &quot;build_snapshot&quot; : false,</div><div class="line">    &quot;lucene_version&quot; : &quot;7.3.1&quot;,</div><div class="line">    &quot;minimum_wire_compatibility_version&quot; : &quot;5.6.0&quot;,</div><div class="line">    &quot;minimum_index_compatibility_version&quot; : &quot;5.0.0&quot;</div><div class="line">  &#125;,</div><div class="line">  &quot;tagline&quot; : &quot;You Know, for Search&quot;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<hr>
<p>您的鼓励是我写作最大的动力</p>
<p>俗话说，投资效率是最好的投资。 如果您感觉我的文章质量不错，读后收获很大，预计能为您提高 10% 的工作效率，不妨小额捐助我一下，让我有动力继续写出更多好文章。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Elasticsearch 官方推出了6.3.0，今天，让我们看一下 Elasticsearch6.3.0给我们带来的新特性吧，如果想看官网的同学可以参考&lt;a href=&quot;https://www.elastic.co/guide/en/elasticsearch/refer
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>how to use sqoop in hadoop</title>
    <link href="https://t1ger.github.io/2018/06/11/how-to-use-sqoop-in-hadoop/"/>
    <id>https://t1ger.github.io/2018/06/11/how-to-use-sqoop-in-hadoop/</id>
    <published>2018-06-11T03:27:36.000Z</published>
    <updated>2018-06-12T10:12:39.676Z</updated>
    
    <content type="html"><![CDATA[<h5 id="install"><a href="#install" class="headerlink" title="install"></a><b>install</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">#download Sqoop 1.4.7 version</div><div class="line">[root@localhost ~]# wget https://mirrors.tuna.tsinghua.edu.cn/apache/sqoop/1.4.7/sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz</div><div class="line">[root@localhost ~]# tar zxf sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz</div><div class="line">[root@localhost ~]# wget https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-8.0.11.tar.gz</div><div class="line">#after untar file, move jar package to Sqoop lib directory </div><div class="line">[root@localhost ~]# cd /usr/local/sqoop/conf &amp;&amp; cp sqoop-env-template.sh sqoop-env.sh</div><div class="line">[root@localhost ~]# cat sqoop-env.sh</div><div class="line">#Set path to where bin/hadoop is available</div><div class="line">export HADOOP_COMMON_HOME=/usr/local/hadoop</div><div class="line">#Set path to where hadoop-*-core.jar is available</div><div class="line">export HADOOP_MAPRED_HOME=/usr/local/hadoop/share/hadoop/mapreduce</div><div class="line">#set the path to where bin/hbase is available</div><div class="line">#export HBASE_HOME=</div><div class="line">#Set the path to where bin/hive is available</div><div class="line">#export HIVE_HOME=</div><div class="line">#Set the path for where zookeper config dir is</div><div class="line">#export ZOOCFGDIR=</div><div class="line"></div><div class="line">#test </div><div class="line"> /usr/local/sqoop/bin/sqoop list-databases \</div><div class="line">  --connect jdbc:mysql://&lt;dburi&gt;/&lt;dbname&gt;  \</div><div class="line">  --username &lt;username&gt; --password &lt;password&gt; </div><div class="line"> /usr/local/sqoop/bin/sqoop list-tables  </div><div class="line"> --connect jdbc:mysql://&lt;dburi&gt;/&lt;dbname&gt; \</div><div class="line"> --username &lt;username&gt; --password &lt;password&gt;</div></pre></td></tr></table></figure>
<h5 id="application-scenarios"><a href="#application-scenarios" class="headerlink" title="application scenarios"></a><b>application scenarios</b></h5><p>tips: before you use command,make sure to su hadoop</p>
<ol>
<li>mysql -&gt; hdfs</li>
<li>hdfs  -&gt; mysql</li>
<li>mysql -&gt; hive</li>
<li>hive  -&gt; mysql</li>
<li>use sql as import condition</li>
</ol>
<ul>
<li><p>from mysql to hdfs<br>–check-column (col): Specifies the column to be examined when determining which rows to import. (the column should not be of type CHAR/NCHAR/VARCHAR/VARNCHAR/ LONGVARCHAR/LONGNVARCHAR)<br>–incremental (mode): Specifies how Sqoop determines which rows are new. Legal values for mode include append and lastmodified.<br>–last-value (value): Specifies the maximum value of the check column from the previous import</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div></pre></td><td class="code"><pre><div class="line">sqoop import --connect jdbc:mysql://&lt;dburi&gt;/&lt;dbname&gt; \</div><div class="line">--username &lt;username&gt; --password &lt;password&gt; \</div><div class="line">--table &lt;tablename&gt; --check-column &lt;col&gt; --incremental &lt;mode&gt; --last-value &lt;value&gt; --target-dir &lt;hdfs-dir&gt;</div><div class="line"></div><div class="line">/usr/local/sqoop/bin/sqoop import \</div><div class="line">--connect jdbc:mysql://192.168.100.112/datbase?zeroDateTimeBehavior=CONVERT_TO_NULL \</div><div class="line">--username username --password password \</div><div class="line">--table pv_daxue \</div><div class="line">--target-dir /usr/sqoop/daxue</div><div class="line"></div><div class="line">#save as parquet(textfile,orcfile,parquet)</div><div class="line">/usr/local/sqoop/bin/sqoop import \</div><div class="line">--connect jdbc:mysql://192.168.100.112/datbase?zeroDateTimeBehavior=CONVERT_TO_NULL \</div><div class="line">--username username --password password \</div><div class="line">--table pv_daxue  --target-dir /usr/sqoop/daxue \</div><div class="line">--as-parquetfile</div><div class="line"></div><div class="line">#save columns id,account </div><div class="line">/usr/local/sqoop/bin/sqoop import \</div><div class="line">--connect jdbc:mysql://192.168.100.112/datbase?zeroDateTimeBehavior=CONVERT_TO_NULL \</div><div class="line">--username username --password password \</div><div class="line">--table pv_daxue  --target-dir /usr/sqoop/daxue \</div><div class="line">--columns id,account \</div><div class="line">--as-textfile</div><div class="line"></div><div class="line">tips: Parameters --as-sequencefile --as-avrodatafile and --as-parquetfile are not supported with --direct params in MySQL case. </div><div class="line"># after insert one record ,append import again</div><div class="line"># Append mode</div><div class="line">/usr/local/sqoop/bin/sqoop import \</div><div class="line">--connect jdbc:mysql://192.168.100.112/datbase?zeroDateTimeBehavior=CONVERT_TO_NULL \</div><div class="line">--username username --password password \</div><div class="line">--table pv_daxue --check-column id \</div><div class="line">--incremental append --target-dir /usr/sqoop/daxue \</div><div class="line">-last-value 5</div><div class="line"></div><div class="line"># Lastmodified mode</div><div class="line">#first import</div><div class="line">/usr/local/sqoop/bin/sqoop import \</div><div class="line">--connect jdbc:mysql://192.168.100.112/datbase?zeroDateTimeBehavior=CONVERT_TO_NULL \</div><div class="line">--username username --password password \</div><div class="line">--table pv_daxue --target-dir /usr/sqoop/daxue -m1</div><div class="line"></div><div class="line">bash-4.1$ hadoop fs -cat /usr/sqoop/daxue/part-m-00000</div><div class="line">1,hello,2018-06-12 23:48:32.0</div><div class="line">2,word,2018-06-12 23:48:32.0</div><div class="line">3,marry,2018-06-12 23:48:32.0</div><div class="line">4,tony,2018-06-12 23:48:32.0</div><div class="line">5,jack,2018-06-12 23:48:33.0</div><div class="line">6,james,2018-06-12 23:52:03.0</div><div class="line">#after insert one record , Lastmodified  import again</div><div class="line">/usr/local/sqoop/bin/sqoop import \</div><div class="line">--connect jdbc:mysql://192.168.100.112/datbase?zeroDateTimeBehavior=CONVERT_TO_NULL \</div><div class="line">--username username --password password \</div><div class="line">--table pv_daxue --check-column last_mod  --incremental lastmodified --last-value &quot;2018-06-12 10:52:03&quot; \</div><div class="line">--target-dir /usr/sqoop/daxue -m 1 --append </div><div class="line"></div><div class="line">18/06/12 10:59:48 INFO mapreduce.ImportJobBase: Transferred 60 bytes in 4.2309 seconds (14.1813 bytes/sec)</div><div class="line">18/06/12 10:59:48 INFO mapreduce.ImportJobBase: Retrieved 2 records</div><div class="line">bash-4.1$ hadoop fs -cat /usr/sqoop/daxue/part-m-00001</div><div class="line">6,james,2018-06-12 23:52:03.0</div><div class="line">7,hello,2018-06-12 23:58:08.0</div><div class="line"></div><div class="line">#merage by mode, after execute sql &quot;update customertest set name = &apos;Hello&apos; where id = 1;&quot;</div><div class="line">/usr/local/sqoop/bin/sqoop import \</div><div class="line">--connect jdbc:mysql://192.168.100.112/datbase?zeroDateTimeBehavior=CONVERT_TO_NULL \</div><div class="line">--username username --password password \</div><div class="line">--table pv_daxue  --check-column last_mod  --incremental lastmodified --last-value &quot;2018-06-12 23:52:03&quot; \</div><div class="line">--target-dir /usr/sqoop/daxue -m 1 --merge-key id </div><div class="line"></div><div class="line">bash-4.1$ hadoop fs -cat /usr/sqoop/daxue/part-r-00000</div><div class="line">1,Hello,2018-06-13 00:07:41.0</div><div class="line">2,word,2018-06-12 23:48:32.0</div><div class="line">3,marry,2018-06-12 23:48:32.0</div><div class="line">4,tony,2018-06-12 23:48:32.0</div><div class="line">5,jack,2018-06-12 23:48:33.0</div><div class="line">6,james,2018-06-12 23:52:03.0</div><div class="line">7,me,2018-06-12 23:58:08.0</div></pre></td></tr></table></figure>
</li>
<li><p>from hdfs to mysql<br>According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn’t set</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">sqoop export --connect jdbc:mysql://&lt;dburi&gt;/&lt;dbname&gt; \</div><div class="line">--username &lt;username&gt; --password &lt;password&gt; \</div><div class="line">--table &lt;tablename&gt; --export-dir &lt;hdfs-dir&gt;</div><div class="line"></div><div class="line">/usr/local/sqoop/bin/sqoop export  \</div><div class="line">--connect &quot;jdbc:mysql://192.168.100.112/datbase?useSSL=false&amp;useUnicode=true&amp;characterEncoding=utf-8&quot; \</div><div class="line">--username username --password password \</div><div class="line">--table pv_daxue  --export-dir /usr/sqoop/daxue</div></pre></td></tr></table></figure>
</li>
<li><p>from mysql to hive</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line">sqoop import --connect jdbc:mysql://&lt;dburi&gt;/&lt;dbname&gt; \</div><div class="line">--username &lt;username&gt; --password &lt;password&gt; \</div><div class="line">--table &lt;tablename&gt; --check-column &lt;col&gt; --incremental &lt;mode&gt; --last-value &lt;value&gt; \</div><div class="line">--fields-terminated-by &quot;\t&quot; --lines-terminated-by &quot;\n&quot; \</div><div class="line">--hive-import --target-dir &lt;hdfs-dir&gt; --hive-table &lt;hive-tablename&gt;</div><div class="line"></div><div class="line">/usr/local/sqoop/bin/sqoop import \</div><div class="line">--connect &quot;jdbc:mysql://192.168.100.112/datbase?useSSL=false&amp;useUnicode=true&amp;characterEncoding=utf-8&quot; \</div><div class="line">--username username --password password \</div><div class="line">–-table hive_table  -–hive-import  --hive-database database –-hive-table hive_test or -–create-hive-table hive_test \  --delete-target-dir  --split-by id</div><div class="line"></div><div class="line"></div><div class="line"># --map-column-hive </div><div class="line">MySQL(bigint) --&gt; Hive(bigint) </div><div class="line">MySQL(tinyint) --&gt; Hive(tinyint) </div><div class="line">MySQL(int) --&gt; Hive(int) </div><div class="line">MySQL(double) --&gt; Hive(double) </div><div class="line">MySQL(bit) --&gt; Hive(boolean) </div><div class="line">MySQL(varchar) --&gt; Hive(string) </div><div class="line">MySQL(decimal) --&gt; Hive(double) </div><div class="line">MySQL(date/timestamp) --&gt; Hive(string)</div><div class="line"></div><div class="line"></div><div class="line">/usr/local/sqoop/bin/sqoop import \</div><div class="line">--connect &quot;jdbc:mysql://192.168.100.112/datbase?useSSL=false&amp;useUnicode=true&amp;characterEncoding=utf-8&quot; \</div><div class="line">--username username --password password \</div><div class="line">–-table hive_table  -–hive-import  \</div><div class="line">--map-column-hive cost=&quot;DECIMAL&quot;,date=&quot;DATE&quot; \ </div><div class="line">--hive-database database –-hive-table hive_test or -–create-hive-table hive_test \  </div><div class="line">--delete-target-dir  --split-by id</div></pre></td></tr></table></figure>
</li>
<li><p>from hive to mysql</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">#Refer above from hdfs to mysql,only need specify the HDFS path corresponding to the Hive table</div><div class="line">/usr/local/sqoop/bin/sqoop export  \</div><div class="line">--connect &quot;jdbc:mysql://192.168.100.112/datbase?useSSL=false&amp;useUnicode=true&amp;characterEncoding=utf-8&quot; \</div><div class="line">--username username --password password</div><div class="line">--table customer --export-dir /user/hive/warehouse/user.db/customer --fields-terminated-by &apos;\001&apos;</div></pre></td></tr></table></figure>
</li>
<li><p>use sql as import condition</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">sqoop import --connect jdbc:mysql://&lt;dburi&gt;/&lt;dbname&gt; --username &lt;username&gt; --password &lt;password&gt; --query &lt;query-sql&gt; --split-by &lt;sp-column&gt; --hive-import --hive-table &lt;hive-tablename&gt; --target-dir &lt;hdfs-dir&gt;</div><div class="line"></div><div class="line">/usr/local/sqoop/bin/sqoop import \</div><div class="line">--connect jdbc:mysql://192.168.100.112/datbase?zeroDateTimeBehavior=CONVERT_TO_NULL \</div><div class="line">--username username --password password --table pv_daxue  --target-dir /usr/sqoop/daxue --delete-target-dir \</div><div class="line">--query &apos;select id,account from version where account=&quot;ddd&quot; and $CONDITIONS &apos; \</div><div class="line">--as-parquetfile</div></pre></td></tr></table></figure>
</li>
<li><p>from mysql to hbase</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">/usr/local/sqoop/bin/sqoop import \</div><div class="line">--connect jdbc:mysql://192.168.100.112/datbase?zeroDateTimeBehavior=CONVERT_TO_NULL \</div><div class="line">--username username --password password </div><div class="line">--query &apos;select id,account from version where account=&quot;ddd&quot; and $CONDITIONS &apos; \</div><div class="line">--hbase-table pv_daxue  --hbase-create-table \ </div><div class="line">--hbase-row-key id --split-by date -m 7 \ </div><div class="line">--column-family tiger</div></pre></td></tr></table></figure>
</li>
</ul>
<h5 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a><b>遇到的问题</b></h5><ul>
<li><p>ERROR tool.ImportTool: Import failed: java.io.FileNotFoundException</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">18/06/11 15:42:43 ERROR tool.ImportTool: Import failed: java.io.FileNotFoundException: File does not exist: hdfs://172.16.56.143:8020/usr/local/sqoop-1.4.7.bin__hadoop-2.6.0/lib/parquet-jackson-1.6.0.jar</div><div class="line"></div><div class="line">[hadoop@node1 conf]$ /usr/local/hadoop/bin/hadoop fs  -mkdir -p /usr/local/sqoop-1.4.7.bin__hadoop-2.6.0/lib</div><div class="line">[hadoop@node1 conf]$ /usr/local/hadoop/bin/hadoop fs  -put  /usr/local/sqoop-1.4.7.bin__hadoop-2.6.0/lib/* hdfs://172.16.56.143:8020/usr/local/sqoop-1.4.7.bin__hadoop-2.6.0/lib</div></pre></td></tr></table></figure>
</li>
<li><p>Caused by: com.mysql.cj.exceptions.CJException: The connection property ‘zeroDateTimeBehavior’ acceptable values are: ‘CONVERT_TO_NULL’, ‘EXCEPTION’ or ‘ROUND’. The value ‘convertToNull’ is not acceptable.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">#using the following code below:</div><div class="line">jdbc:mysql://localhost:3306/database?zeroDateTimeBehavior=CONVERT_TO_NULL</div></pre></td></tr></table></figure>
<p>  if config lzo ,you  perhaps see ,use command “hadoop checknative” check</p>
</li>
<li><p>ERROR sqoop.Sqoop: Got exception running Sqoop: java.lang.IllegalArgumentException: Compression codec com.hadoop.compression.lzo.LzoCodec not found.<br>java.lang.IllegalArgumentException: Compression codec com.hadoop.compression.lzo.LzoCodec not found.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"># install lzo support</div></pre></td></tr></table></figure>
</li>
<li><p>No primary key could be found for tablescore</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">specify one with --split-by or perform a sequential import with&apos;-m 1&apos;</div></pre></td></tr></table></figure>
</li>
<li><p>ERROR hive.HiveConfig: Could not load org.apache.hadoop.hive.conf.HiveConf. Make sure HIVE_CONF_DIR is set correctly.<br>ERROR tool.ImportTool: Import failed: java.io.IOException: java.lang.ClassNotFoundException: org.apache.hadoop.hive.conf.HiveConf</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">#add this one in .bash_profile:</div><div class="line">export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:/usr/lib/hive/lib/*</div><div class="line">source ~/.bash_profile</div></pre></td></tr></table></figure>
</li>
<li><p>FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException(message:For direct MetaStore DB connections, we don’t support retries at the client level.)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">mysql&gt; alter database hive character set latin1;</div></pre></td></tr></table></figure>
</li>
<li><p>java.lang.RuntimeException: Can’t parse input data: ‘1Hello2018-06-13 00:07:41.0’</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">--fields-terminated-by &apos;\001&apos;</div></pre></td></tr></table></figure>
</li>
</ul>
<p>ref<br><a href="http://sqoop.apache.org/docs/1.4.7/SqoopUserGuide.html#_incremental_imports" target="_blank" rel="external">Incremental Imports</a><br><a href="https://www.cnblogs.com/ljy2013/p/4872126.html" target="_blank" rel="external">sqoop的增量导入（increment import）</a><br><a href="https://github.com/kevinweil/hadoop-lzo" target="_blank" rel="external">hadoop-lzo</a><br><a href="http://www.oberhumer.com/opensource/lzo/#download" target="_blank" rel="external">lzo</a><br><a href="https://www.zybuluo.com/aitanjupt/note/209968#%E4%BD%BF%E7%94%A8sqoop%E4%BB%8Emysql%E5%AF%BC%E5%85%A5%E6%95%B0%E6%8D%AE%E5%88%B0hive" target="_blank" rel="external">Sqoop从MySQL导入数据</a></p>
<hr>
<p>您的鼓励是我写作最大的动力</p>
<p>俗话说，投资效率是最好的投资。 如果您感觉我的文章质量不错，读后收获很大，预计能为您提高 10% 的工作效率，不妨小额捐助我一下，让我有动力继续写出更多好文章。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;install&quot;&gt;&lt;a href=&quot;#install&quot; class=&quot;headerlink&quot; title=&quot;install&quot;&gt;&lt;/a&gt;&lt;b&gt;install&lt;/b&gt;&lt;/h5&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Rsyslog 连接 Kafka 指北</title>
    <link href="https://t1ger.github.io/2018/05/22/Rsyslog-%E8%BF%9E%E6%8E%A5-Kafka-%E6%8C%87%E5%8C%97/"/>
    <id>https://t1ger.github.io/2018/05/22/Rsyslog-连接-Kafka-指北/</id>
    <published>2018-05-22T07:20:48.000Z</published>
    <updated>2018-05-22T06:51:56.400Z</updated>
    
    <content type="html"><![CDATA[<h5 id="运行环境"><a href="#运行环境" class="headerlink" title="运行环境"></a><b>运行环境</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">[root@bogon ~]# cat /etc/redhat-release </div><div class="line">CentOS Linux release 7.4.1708 (Core) </div><div class="line"></div><div class="line">[root@bogon ~]# rpm -qa|grep rsyslog</div><div class="line">rsyslog-kafka-8.28.0-1.el7.x86_64</div><div class="line">rsyslog-8.28.0-1.el7.x86_64</div></pre></td></tr></table></figure>
<h5 id="安装"><a href="#安装" class="headerlink" title="安装"></a><b>安装</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">wget -O /etc/yum.repos.d/rsyslog.repo http://rpms.adiscon.com/v8-stable/rsyslog.repo</div><div class="line">yum install rsyslog rsyslog-kafka.x86_64</div></pre></td></tr></table></figure>
<p>国内的同学可能无法安装，同学们也可以通过<a href="http://rpms.adiscon.com/v8-stable/epel-7/x86_64/RPMS/" target="_blank" rel="external">这里</a>下载安装</p>
<h5 id="配置"><a href="#配置" class="headerlink" title="配置"></a><b>配置</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">[root@bogon ~]# cat /etc/rsyslog.d/kafka.conf</div><div class="line">module(load=&quot;omkafka&quot;)</div><div class="line">action (</div><div class="line">        type=&quot;omkafka&quot;</div><div class="line">        topic=&quot;topicA&quot;</div><div class="line">        broker=&quot;cdh1:9092,cdh2:9092,cdh3:9092&quot;</div><div class="line">    )</div><div class="line"></div><div class="line"></div><div class="line">#如果保存到本地</div><div class="line">[root@bogon ~]# cat /etc/rsyslog.d/router.conf</div><div class="line">template (name=&quot;DynFile&quot; type=&quot;string&quot; string=&quot;/data/%fromhost-ip%.log&quot;)</div><div class="line">if $fromhost-ip startswith &apos;192.168.100.2&apos; and $programname != &apos;Type=SESSION;&apos; and $programname != &apos;Type=Login;&apos; and $programname != &apos;Type=AuthLog;&apos; and $programname != &apos;Type=Ftp&apos; then &#123;</div><div class="line">    action(type=&quot;omfile&quot; dynaFile=&quot;DynFile&quot;)</div><div class="line">    stop</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>低版本的rsyslog保存到本地配置如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">[root@localhost ~]# rpm -qa|grep rsyslog</div><div class="line">rsyslog-5.8.10-6.el6.x86_64</div><div class="line"></div><div class="line">添加到 /etc/rsyslog.conf </div><div class="line">#### GLOBAL DIRECTIVES ####</div><div class="line">$template RemoteLogs,&quot;/data/var/log/%HOSTNAME%/%fromhost-ip%_%$YEAR%-%$MONTH%-%$DAY%.log&quot; *</div><div class="line">*.* ?RemoteLogs</div><div class="line">&amp;~</div></pre></td></tr></table></figure></p>
<p>通过kafka查看消息<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">cd /usr/local/kafka&amp;&amp; bin/kafka-console-consumer.sh  --bootstrap-server cdh1:9092,cdh2:9092,cdh3:9092   --topic topicA</div></pre></td></tr></table></figure></p>
<p>ref<br><a href="https://doc.yonyoucloud.com/doc/logstash-best-practice-cn/ecosystem/rsyslog.html" target="_blank" rel="external">Rsyslog</a><br><a href="http://wdxtub.com/2016/08/17/rsyslog-kafka-guide/" target="_blank" rel="external">Rsyslog 连接 Kafka 指南</a><br><a href="https://serverfault.com/questions/807108/how-to-call-template-so-rsyslog-8-creates-one-log-file-per-client" target="_blank" rel="external">How to call template so rsyslog 8 creates one log file per client</a><br><a href="http://wiki.rsyslog.com/index.php/DailyLogRotation" target="_blank" rel="external">DailyLogRotation</a><br><a href="http://blog.kompaz.win/2018/01/11/20180111%20CentOS7%20rsyslog%20+loganalyzer%E9%85%8D%E7%BD%AE/" target="_blank" rel="external">CentOS7 rsyslog +loganalyzer配置</a></p>
<hr>
<p>您的鼓励是我写作最大的动力</p>
<p>俗话说，投资效率是最好的投资。 如果您感觉我的文章质量不错，读后收获很大，预计能为您提高 10% 的工作效率，不妨小额捐助我一下，让我有动力继续写出更多好文章。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;运行环境&quot;&gt;&lt;a href=&quot;#运行环境&quot; class=&quot;headerlink&quot; title=&quot;运行环境&quot;&gt;&lt;/a&gt;&lt;b&gt;运行环境&lt;/b&gt;&lt;/h5&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutt
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Spark-Streaming with Kafka Programming</title>
    <link href="https://t1ger.github.io/2018/05/08/Spark-Streaming-with-Kafka-Programming/"/>
    <id>https://t1ger.github.io/2018/05/08/Spark-Streaming-with-Kafka-Programming/</id>
    <published>2018-05-08T07:55:06.000Z</published>
    <updated>2018-05-10T10:31:07.231Z</updated>
    
    <content type="html"><![CDATA[<h5 id="运行环境"><a href="#运行环境" class="headerlink" title="运行环境"></a><b>运行环境</b></h5><ol>
<li><p>jdk环境</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[root@cdh1 kafka]# java -version</div><div class="line">java version &quot;1.8.0_112&quot;</div><div class="line">Java(TM) SE Runtime Environment (build 1.8.0_112-b15)</div><div class="line">Java HotSpot(TM) 64-Bit Server VM (build 25.112-b15, mixed mode)</div></pre></td></tr></table></figure>
</li>
<li><p>引入maven</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">&lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</div><div class="line">&lt;artifactId&gt;spark-streaming_2.11&lt;/artifactId&gt;</div><div class="line">&lt;version&gt;2.3.0&lt;/version&gt;</div><div class="line"></div><div class="line">&lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</div><div class="line">&lt;artifactId&gt;spark-streaming-kafka-0-10_2.11&lt;/artifactId&gt;</div><div class="line">&lt;version&gt;2.3.0&lt;/version&gt;</div></pre></td></tr></table></figure>
</li>
</ol>
<h5 id="示例WordCount"><a href="#示例WordCount" class="headerlink" title="示例WordCount"></a><b>示例WordCount</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div></pre></td><td class="code"><pre><div class="line">package cn.spark.streaming;</div><div class="line"></div><div class="line"></div><div class="line">import java.util.Arrays;</div><div class="line">import java.util.Collection;</div><div class="line">import java.util.HashMap;</div><div class="line">import java.util.HashSet;</div><div class="line">import java.util.Iterator;</div><div class="line">import java.util.Map;</div><div class="line">import org.apache.kafka.clients.consumer.ConsumerRecord;</div><div class="line">import org.apache.spark.SparkConf;</div><div class="line">import org.apache.spark.TaskContext;</div><div class="line">import org.apache.spark.api.java.function.FlatMapFunction;</div><div class="line">import org.apache.spark.api.java.function.Function2;</div><div class="line">import org.apache.spark.api.java.function.PairFunction;</div><div class="line">import org.apache.spark.streaming.Durations;</div><div class="line">import org.apache.spark.streaming.api.java.JavaDStream;</div><div class="line">import org.apache.spark.streaming.api.java.JavaInputDStream;</div><div class="line">import org.apache.spark.streaming.api.java.JavaPairDStream;</div><div class="line">import org.apache.spark.streaming.api.java.JavaStreamingContext;</div><div class="line">import org.apache.spark.streaming.kafka010.CanCommitOffsets;</div><div class="line">import org.apache.spark.streaming.kafka010.ConsumerStrategies;</div><div class="line">import org.apache.spark.streaming.kafka010.HasOffsetRanges;</div><div class="line">import org.apache.spark.streaming.kafka010.KafkaUtils;</div><div class="line">import org.apache.spark.streaming.kafka010.LocationStrategies;</div><div class="line">import org.apache.spark.streaming.kafka010.OffsetRange;</div><div class="line"></div><div class="line">import scala.Tuple2;</div><div class="line"></div><div class="line">public class KafkaDirectWordCount &#123;</div><div class="line"></div><div class="line">	public static void main(String[] args) throws InterruptedException &#123;</div><div class="line">		// TODO Auto-generated method stub</div><div class="line">		</div><div class="line">		SparkConf  conf = new SparkConf()</div><div class="line">				.setAppName(&quot;KafkaReceiveWordCount&quot;)</div><div class="line">				.setMaster(&quot;local[2]&quot;);</div><div class="line">		JavaStreamingContext jssc = new JavaStreamingContext(conf,Durations.seconds(5));</div><div class="line">		</div><div class="line">		String brokers = &quot;192.168.10.140:9092,192.168.10.141:9092,192.168.10.142:9092&quot;;</div><div class="line">		</div><div class="line">		Map&lt;String, Object&gt; kafkaparams = new HashMap&lt;&gt;();</div><div class="line">		kafkaparams.put(&quot;metadata.broker.list&quot;, brokers);</div><div class="line">		kafkaparams.put(&quot;bootstrap.servers&quot;, brokers);</div><div class="line">		kafkaparams.put(&quot;group.id&quot;, &quot;KafkaDirectWordCount&quot;);</div><div class="line">		kafkaparams.put(&quot;key.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);</div><div class="line">		kafkaparams.put(&quot;key.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;);</div><div class="line">		kafkaparams.put(&quot;value.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;);</div><div class="line">		kafkaparams.put(&quot;enable.auto.commit&quot;, false);</div><div class="line">		kafkaparams.put(&quot;auto.offset.reset&quot;, &quot;latest&quot;); // earliest latest none </div><div class="line">		kafkaparams.put(&quot;offsets.storage&quot;, &quot;kafka&quot;);</div><div class="line">		</div><div class="line">		Collection&lt;String&gt; topics = new HashSet&lt;String&gt;();</div><div class="line">		topics.add(&quot;topicA&quot;);		</div><div class="line">		</div><div class="line">//		Map&lt;TopicPartition,Long&gt; offsets = new HashMap&lt;&gt;();</div><div class="line">//		offsets.put(new TopicPartition(&quot;topicA&quot;,0),2L);</div><div class="line">		</div><div class="line">		JavaInputDStream&lt;ConsumerRecord&lt;String,String&gt;&gt; lines = KafkaUtils.createDirectStream(</div><div class="line">				jssc,</div><div class="line">				LocationStrategies.PreferConsistent(),</div><div class="line">				ConsumerStrategies.Subscribe(topics, kafkaparams)</div><div class="line">				);</div><div class="line">		</div><div class="line">			lines.foreachRDD(rdd -&gt; &#123;</div><div class="line">			  OffsetRange[] offsetRanges = ((HasOffsetRanges) rdd.rdd()).offsetRanges();</div><div class="line">			  rdd.foreachPartition(consumerRecords -&gt; &#123;</div><div class="line">			    OffsetRange o = offsetRanges[TaskContext.get().partitionId()];</div><div class="line">			    System.out.println(</div><div class="line">			      o.topic() + &quot; &quot; + o.partition()  + &quot; &quot; + o.fromOffset() + &quot; &quot; + o.untilOffset());</div><div class="line">			  &#125;);</div><div class="line">			&#125;);</div><div class="line">		</div><div class="line">	    JavaDStream&lt;String&gt; words = lines.flatMap(new FlatMapFunction&lt;ConsumerRecord&lt;String,String&gt;,String&gt;()&#123;</div><div class="line">			private static final long serialVersionUID = 1L;</div><div class="line"></div><div class="line">			@Override</div><div class="line">			public Iterator&lt;String&gt; call(ConsumerRecord&lt;String, String&gt; line) throws Exception &#123;</div><div class="line">				// TODO Auto-generated method stub</div><div class="line">				return Arrays.asList(line.value().split(&quot; &quot;)).iterator();</div><div class="line">			&#125;</div><div class="line">	    	</div><div class="line">	    &#125;);</div><div class="line"></div><div class="line">		</div><div class="line">		JavaPairDStream&lt;String,Integer&gt; paris = words.mapToPair(new PairFunction&lt;String,String,Integer&gt;()&#123;</div><div class="line">			private static final long serialVersionUID = 1L;</div><div class="line"></div><div class="line">			@Override</div><div class="line">			public Tuple2&lt;String, Integer&gt; call(String word) throws Exception &#123;</div><div class="line">				// TODO Auto-generated method stub</div><div class="line">				return new Tuple2&lt;String,Integer&gt;(word,1);</div><div class="line">			&#125;</div><div class="line">			</div><div class="line">		&#125;);</div><div class="line">		</div><div class="line">	 JavaPairDStream&lt;String,Integer&gt; wordcount= paris.reduceByKey(new Function2&lt;Integer,Integer,Integer&gt;()&#123;</div><div class="line">		private static final long serialVersionUID = 1L;</div><div class="line"></div><div class="line">		@Override</div><div class="line">		public Integer call(Integer v1, Integer v2) throws Exception &#123;</div><div class="line">			// TODO Auto-generated method stub</div><div class="line">			return v1 + v2;</div><div class="line">		&#125;</div><div class="line">		 </div><div class="line">	 &#125;);</div><div class="line">	 </div><div class="line">	 </div><div class="line">	 wordcount.print();</div><div class="line">	 jssc.start();</div><div class="line">	 jssc.awaitTermination();</div><div class="line">	 jssc.close();</div><div class="line">	 </div><div class="line">	&#125;</div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>运行之前开启生产者：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">cd /usr/local/kafka</div><div class="line">bin/kafka-console-producer.sh --broker-list 192.168.10.140:9092,192.168.10.141:9092,192.168.10.142:9092 --topic topicA</div><div class="line">&gt; hello word hello me</div></pre></td></tr></table></figure></p>
<p>ref<br><a href="https://github.com/jaceklaskowski/spark-streaming-notebook/blob/master/spark-streaming-kafka-DirectKafkaInputDStream.adoc" target="_blank" rel="external">DirectKafkaInputDStream — Direct Kafka DStream</a><br><a href="http://spark.apache.org/docs/latest/streaming-kafka-0-10-integration.html" target="_blank" rel="external">Creating a Direct Stream</a><br><a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html#input-dstreams-and-receivers" target="_blank" rel="external">Spark Streaming Programming Guide</a><br><a href="http://blog.cloudera.com/blog/2017/06/offset-management-for-apache-kafka-with-apache-spark-streaming/" target="_blank" rel="external">Offset Management For Apache Kafka With Apache Spark Streaming</a><br><a href="https://blog.csdn.net/xueba207/article/details/51135423" target="_blank" rel="external">Spark Streaming ‘numRecords must not be negative’问题解决</a><br><a href="https://blog.csdn.net/lishuangzhe7047/article/details/74530417" target="_blank" rel="external">Kafka auto.offset.reset值详解</a><br><a href="https://blog.csdn.net/Dax1n/article/details/61614379" target="_blank" rel="external">Spark整合kafka0.10.0新特性(一)</a><br><a href="https://blog.csdn.net/sinat_27545249/article/details/78090872" target="_blank" rel="external">kafka0.8版本和sparkstreaming整合的两种不同方式</a><br><a href="https://blog.csdn.net/qfwyp0714/article/details/73998293" target="_blank" rel="external">Spark streaming 跟踪kafka offset的问题研究</a></p>
<hr>
<p>您的鼓励是我写作最大的动力</p>
<p>俗话说，投资效率是最好的投资。 如果您感觉我的文章质量不错，读后收获很大，预计能为您提高 10% 的工作效率，不妨小额捐助我一下，让我有动力继续写出更多好文章。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;运行环境&quot;&gt;&lt;a href=&quot;#运行环境&quot; class=&quot;headerlink&quot; title=&quot;运行环境&quot;&gt;&lt;/a&gt;&lt;b&gt;运行环境&lt;/b&gt;&lt;/h5&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;jdk环境&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;tab
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>how to config Filebeat6 quickly</title>
    <link href="https://t1ger.github.io/2018/04/11/how-to-config-Filebeat6-quickly/"/>
    <id>https://t1ger.github.io/2018/04/11/how-to-config-Filebeat6-quickly/</id>
    <published>2018-04-11T11:39:00.000Z</published>
    <updated>2018-04-19T08:04:50.199Z</updated>
    
    <content type="html"><![CDATA[<h5 id="环境介绍"><a href="#环境介绍" class="headerlink" title="环境介绍"></a><b>环境介绍</b></h5><p>系统为Centos6.8,相关软件版本如下：<br>filebeat-6.2.3<br>redis-3.0.7<br>logstash-6.2.3<br>kibana-6.2.3</p>
<p>架构为前端filebeat 读取nginx日志或其他日志（json格式），输出到中间redis，后端logstash从redis读取并解析</p>
<h5 id="filebeat-yml配置"><a href="#filebeat-yml配置" class="headerlink" title="filebeat.yml配置"></a><b>filebeat.yml配置</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div></pre></td><td class="code"><pre><div class="line">#filebeat配置</div><div class="line">cat /etc/filebeat/filebeat.yml </div><div class="line">filebeat.config_dir: prospectors.d</div><div class="line">filebeat.config.prospectors:</div><div class="line">  enabled: true</div><div class="line">  path: prospectors.d/*.yml</div><div class="line">  reload.enabled: true</div><div class="line">  reload.period: 10s </div><div class="line">filebeat.prospectors:</div><div class="line">- type: log</div><div class="line">  enabled: false</div><div class="line">  paths:</div><div class="line">    - /var/log/message</div><div class="line">filebeat.config.modules:</div><div class="line">  path: $&#123;path.config&#125;/modules.d/*.yml</div><div class="line">  reload.enabled: true</div><div class="line">  reload.period: 10s</div><div class="line">setup.template.settings:</div><div class="line">  index.number_of_shards: 3</div><div class="line">setup.kibana:</div><div class="line"> </div><div class="line">output.file:   #主要用于调试</div><div class="line">   path: &quot;/tmp&quot;</div><div class="line">   filename: filebeat.out</div><div class="line">   number_of_files: 7</div><div class="line">   rotate_every_kb: 10000 </div><div class="line">   enabled: false   #关闭输出</div><div class="line">output.redis:</div><div class="line">   hosts: [&quot;192.168.90.147:6379&quot;]</div><div class="line">   password: &quot;password&quot;</div><div class="line">   key: &quot;filebeat&quot;</div><div class="line">   db: 0</div><div class="line">   timeout: 60</div><div class="line">   max_retires: 3</div><div class="line">   bulk_max_size: 4096</div><div class="line">   datatype: list</div><div class="line">   keys:</div><div class="line">     - key: &quot;%&#123;[fields.log_source]&#125;&quot;</div><div class="line">       mapping:</div><div class="line">         &quot;bash_history&quot;: &quot;command-log&quot;</div><div class="line">         &quot;nginx&quot;  : &quot;nginx-log&quot;</div><div class="line"></div><div class="line">#bash历史记录</div><div class="line">[root@localhost ~]# cat /etc/filebeat/prospectors.d/history.yml </div><div class="line">- type: log</div><div class="line">  enabled: true</div><div class="line">  paths:</div><div class="line">    - /var/log/command.log  </div><div class="line">  fields:</div><div class="line">    log_source: command-log</div><div class="line">#  tags: &quot;bash_history&quot;</div><div class="line">  json.keys_under_root: true</div><div class="line">  json.add_error_key: true</div><div class="line">  json.message_key: TIME</div><div class="line">  </div><div class="line">#nginx日志配置</div><div class="line">[root@localhost ~]# cat /etc/filebeat/prospectors.d/nginx.yml</div><div class="line">- type: log</div><div class="line">  enabled: true</div><div class="line">  paths:</div><div class="line">    - /usr/local/openresty/nginx/logs/cms_log.log</div><div class="line">  fields:</div><div class="line">    log_source: nginx-log</div><div class="line">  exclude_lines: [&quot;helo.html&quot;]</div></pre></td></tr></table></figure>
<h5 id="JSON文件格式"><a href="#JSON文件格式" class="headerlink" title="JSON文件格式"></a><b>JSON文件格式</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line">#bash_history为json格式，添加到/etc/profile文件</div><div class="line">HISTDIR=&apos;/var/log/command.log&apos;</div><div class="line">if [ ! -f $HISTDIR ];then</div><div class="line">touch $HISTDIR</div><div class="line">chmod 666 $HISTDIR</div><div class="line">fi</div><div class="line">export HISTTIMEFORMAT=&quot;&#123;\&quot;TIME\&quot;:\&quot;%F %T\&quot;,\&quot;HOSTNAME\&quot;:\&quot;$HOSTNAME\&quot;,\&quot;LI\&quot;:\&quot;$(who -u am i 2&gt;/dev/null| awk &apos;&#123;print $NF&#125;&apos;|sed -e &apos;s/[()]//g&apos;)\&quot;,\&quot;LU\&quot;:\&quot;$(who am i|awk &apos;&#123;print $1&#125;&apos;)\&quot;,\&quot;NU\&quot;:\&quot;$&#123;USER&#125;\&quot;,\&quot;CMD\&quot;:\&quot;&quot;</div><div class="line">export PROMPT_COMMAND=&apos;history 1|tail -1|sed &quot;s/^[ ]\+[0-9]\+  //&quot;|sed &quot;s/$/\&quot;&#125;/&quot;&gt;&gt; /var/log/command.log&apos;</div><div class="line"></div><div class="line">#nginx日志格式为</div><div class="line">        log_format json &apos;&#123;&quot;@timestamp&quot;:&quot;$time_local&quot;,&apos;</div><div class="line">                &apos;&quot;source&quot;:&quot;nginx147&quot;,&apos;</div><div class="line">                &apos;&quot;serverAddr&quot;:&quot;$server_addr&quot;,&apos;</div><div class="line">                &apos;&quot;remoteAddr&quot;:&quot;$remote_addr&quot;,&apos;</div><div class="line">                &apos;&quot;remoteUser&quot;:&quot;$remote_user&quot;,&apos;</div><div class="line">                &apos;&quot;size&quot;:$body_bytes_sent,&apos;</div><div class="line">                &apos;&quot;status&quot;:$status,&apos;</div><div class="line">                &apos;&quot;time&quot;:$request_time,&apos;</div><div class="line">                &apos;&quot;method&quot;:&quot;$request_method&quot;,&apos;</div><div class="line">                &apos;&quot;protocol&quot;:&quot;$server_protocol&quot;,&apos;</div><div class="line">                &apos;&quot;url&quot;:&quot;$scheme://$host$request_uri&quot;,&apos;</div><div class="line">                &apos;&quot;host&quot;:&quot;$http_host&quot;,&apos;</div><div class="line">                &apos;&quot;uri&quot;:&quot;$uri&quot;,&apos;</div><div class="line">                &apos;&quot;referer&quot;:&quot;$http_referer&quot;,&apos;</div><div class="line">                &apos;&quot;xforwarded&quot;:&quot;$http_x_forwarded_for&quot;,&apos;</div><div class="line">                &apos;&quot;agent&quot;:&quot;$http_user_agent&quot;,&apos;</div><div class="line">                &apos;&quot;upsTime&quot;:&quot;$upstream_response_time&quot;,&apos;</div><div class="line">                &apos;&quot;sslPro&quot;:&quot;$ssl_protocol&quot;,&apos;</div><div class="line">                &apos;&quot;sslCip&quot;:&quot;$ssl_cipher&quot;,&apos;</div><div class="line">                &apos;&quot;upsStatus&quot;:&quot;$upstream_status&quot;&#125;&apos;;</div></pre></td></tr></table></figure>
<h5 id="logstash配置"><a href="#logstash配置" class="headerlink" title="logstash配置"></a><b>logstash配置</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div></pre></td><td class="code"><pre><div class="line">[root@localhost ~]# cat /etc/logstash/conf.d/history-logstash.conf </div><div class="line">input &#123;</div><div class="line">    redis &#123;</div><div class="line">        data_type =&gt; &quot;list&quot;  </div><div class="line">        host =&gt; &quot;192.168.90.147&quot;</div><div class="line">        port =&gt; &quot;6379&quot;</div><div class="line">        password =&gt; &quot;password&quot;</div><div class="line">        key  =&gt; &quot;command-log&quot;</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">output &#123;</div><div class="line">       if [fields][log_source] == &quot;command-log&quot; &#123; </div><div class="line">      elasticsearch &#123;</div><div class="line">          hosts   =&gt; [&quot;192.16.90.149:9200&quot;]</div><div class="line">          manage_template =&gt; false</div><div class="line">          action  =&gt; &quot;index&quot;</div><div class="line">          index   =&gt; &quot;command-log-%&#123;+YYYY.MM.dd&#125;&quot;</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">#nginx</div><div class="line">[root@localhost ~]# cat /etc/logstash/conf.d/nginx-logstash.conf </div><div class="line">input &#123;</div><div class="line">    redis &#123;</div><div class="line">        data_type =&gt; &quot;list&quot;  </div><div class="line">        host =&gt; &quot;192.168.90.147&quot;</div><div class="line">        port =&gt; &quot;6379&quot;</div><div class="line">        password =&gt; &quot;password&quot;</div><div class="line">        key  =&gt; &quot;nginx-log&quot;</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line">output &#123;</div><div class="line"></div><div class="line">	 if [fields][log_source] == &quot;nginx-log&quot; &#123;</div><div class="line">        file &#123;</div><div class="line">        path =&gt; &quot;/tmp/logs/nginx-%&#123;+YYYY-MM-dd&#125;.log&quot;</div><div class="line">        &#125;</div><div class="line">   &#125;</div><div class="line">   </div><div class="line">#     elasticsearch &#123;</div><div class="line">#        hosts   =&gt; [&quot;192.168.90.149:9200&quot;]</div><div class="line">#        action  =&gt; &quot;index&quot;</div><div class="line">#        index   =&gt; &quot;nginx-log-%&#123;+YYYY.MM.dd&#125;&quot;</div><div class="line">#    &#125;</div><div class="line"></div><div class="line">#        stdout &#123; codec =&gt; rubydebug &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>备注：<br>logstash安装完后需要执行以下命令，进行service服务安装<br>/usr/share/logstash/bin/system-install /etc/logstash/startup.options sysv</p>
<p>ref<br><a href="https://jkzhao.github.io/2017/10/24/Filebeat%E6%97%A5%E5%BF%97%E6%94%B6%E9%9B%86%E5%99%A8/" target="_blank" rel="external">Filebeat日志收集器</a><br><a href="https://blog.csdn.net/jianblog/article/details/54669203" target="_blank" rel="external">Elastic测试笔记</a><br><a href="https://www.elastic.co/guide/en/beats/filebeat/current/redis-output.html" target="_blank" rel="external">Configure the Redis output</a></p>
<hr>
<p>您的鼓励是我写作最大的动力</p>
<p>俗话说，投资效率是最好的投资。 如果您感觉我的文章质量不错，读后收获很大，预计能为您提高 10% 的工作效率，不妨小额捐助我一下，让我有动力继续写出更多好文章。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;环境介绍&quot;&gt;&lt;a href=&quot;#环境介绍&quot; class=&quot;headerlink&quot; title=&quot;环境介绍&quot;&gt;&lt;/a&gt;&lt;b&gt;环境介绍&lt;/b&gt;&lt;/h5&gt;&lt;p&gt;系统为Centos6.8,相关软件版本如下：&lt;br&gt;filebeat-6.2.3&lt;br&gt;redis-3.0.7
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>redis性能分析</title>
    <link href="https://t1ger.github.io/2018/04/10/redis%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90/"/>
    <id>https://t1ger.github.io/2018/04/10/redis性能分析/</id>
    <published>2018-04-10T10:11:17.000Z</published>
    <updated>2018-04-10T10:16:03.106Z</updated>
    
    <content type="html"><![CDATA[<h5 id="前言"><a href="#前言" class="headerlink" title="前言"></a><b>前言</b></h5><p>redis性能分析常见的有以下几个方面：</p>
<ul>
<li>redis slowlog分析</li>
<li>SCAN，SSCAN，HSCAN和ZSCAN命令的使用方法</li>
<li>redis是否受到系统使用swap</li>
<li>redis watchdog定位延时</li>
<li>关于redis的延时监控框架,可参考<a href="https://redis.io/topics/latency-monitor" target="_blank" rel="external">官网资料</a><br>下面我们分别从这几个方面来介绍</li>
</ul>
<h5 id="redis-slowlog分析"><a href="#redis-slowlog分析" class="headerlink" title="redis slowlog分析"></a><b>redis slowlog分析</b></h5><ol>
<li><p>慢查询设置<br>在Redis中有两种修改配置的方法,一种是修改配置文件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">slowlog-log-slower-than 10000  #查询时间超过10ms的会被记录  </div><div class="line">slowlog-max-len 128            # 最多记录128个慢查询</div></pre></td></tr></table></figure>
<p> 另一种是使用config set命令动态修改.例如下面使用config set命令将slowlog-log-slower-than设置为20000微妙.slowlog-max-len设置为1024</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">config set slowlog-log-slower-than 20000</div><div class="line">config set slowlog-max-len 1024</div><div class="line">config rewrite</div></pre></td></tr></table></figure>
<p> 如果需要将Redis将配置持久化到本地配置文件,要执行config rewrite命令,如果slowlog-log-slower-than=0会记录所有命令,slowlog-log-slower-than&lt;0对于任何命令都不会进行记录</p>
</li>
<li><p>获取慢查询日志<br>slowlog get [n]</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">127.0.0.1:6379&gt; slowlog get 15</div><div class="line"> 1) 1) (integer) 79674  #slowlog的唯一编号 </div><div class="line">    2) (integer) 1523350838 #此次slowlog事件的发生时间  </div><div class="line">    3) (integer) 2987577    #耗时,以微秒为单位</div><div class="line">    4) 1) &quot;KEYS&quot;</div><div class="line">       2) &quot;mid_cache_app_list_*&quot;</div></pre></td></tr></table></figure>
</li>
<li><p>获取慢查询日志列表当前长度</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">127.0.0.1:6379&gt; slowlog len</div><div class="line">(integer) 128</div></pre></td></tr></table></figure>
</li>
<li><p>慢查询日志重置</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">slowlog reset</div></pre></td></tr></table></figure>
</li>
<li><p>建议：<br>slowlog-max-len 建议线上设置为1000以上<br>slowlog-log-slower-than对高流量场景应该设置在1毫秒以上<br>慢查询只记录命令的执行时间,并不包括命令排队和网络传输时间.因此客户端执行命令的时间会大于命令的实际执行时间.因为命令执行排队机制,慢查询会导致其他命令级联阻塞,因此客户端出现请求超时时,需要检查该时间点是否有对应的慢查询,从而分析是否为慢查询导致的命令级联阻塞.</p>
</li>
</ol>
<h5 id="SCAN，SSCAN，HSCAN和ZSCAN命令的使用方法"><a href="#SCAN，SSCAN，HSCAN和ZSCAN命令的使用方法" class="headerlink" title="SCAN，SSCAN，HSCAN和ZSCAN命令的使用方法"></a><b>SCAN，SSCAN，HSCAN和ZSCAN命令的使用方法</b></h5><ol>
<li><p>SCAN是基于游标的迭代器。每次调用命令时，服务器返回一个更新的游标，用户需要在下一次调用中用作游标参数。当游标设置为0时，迭代开始，并且当服务器返回的游标为0时终止迭代<br>开始游标值为0的迭代，并调用SCAN，直到返回的游标再次为0，称为完全迭代</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">127.0.0.1:6379&gt; scan 0  </div><div class="line"></div><div class="line">127.0.0.1:6379&gt; scan 0 count 20 //指定输出的数量</div><div class="line">127.0.0.1:6379&gt; scan 0 match *mid_sent*   //类似于keys命令按模式匹配</div></pre></td></tr></table></figure>
</li>
<li><p>sscan查询sets集合的方法：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">redis 127.0.0.1:6379&gt; sadd setone 1 2 3 foo foobar feelsgood  </div><div class="line">(integer) 6  </div><div class="line">redis 127.0.0.1:6379&gt; sscan setone 0 match f*  </div><div class="line">1) &quot;0&quot;  </div><div class="line">2) 1) &quot;foo&quot;  </div><div class="line">   2) &quot;feelsgood&quot;  </div><div class="line">   3) &quot;foobar&quot;</div></pre></td></tr></table></figure>
</li>
<li><p>hscan查询hash集合的方法：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">redis 127.0.0.1:6379&gt; hmset hash name Tom age 35  </div><div class="line">OK  </div><div class="line">redis 127.0.0.1:6379&gt; hscan hash 0  </div><div class="line">1) &quot;0&quot;  </div><div class="line">2) 1) &quot;name&quot;  </div><div class="line">   2) &quot;Tom&quot;  </div><div class="line">   3) &quot;age&quot;  </div><div class="line">   4) &quot;35&quot;</div></pre></td></tr></table></figure>
</li>
<li><p>Linux内核启用了透明巨页功能时，Redis在使用fork调用之后会产生大的延迟代价，以便在磁盘进行数据持久化</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled</div></pre></td></tr></table></figure>
<p> 需重启redis才能生效</p>
</li>
</ol>
<h5 id="redis是否受到系统使用swap"><a href="#redis是否受到系统使用swap" class="headerlink" title="redis是否受到系统使用swap"></a><b>redis是否受到系统使用swap</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">查找redis进程id：  </div><div class="line">redis-cli -p 6319 info|grep process_id  </div><div class="line">process_id:9213  </div><div class="line">查看redis进程的内存使用信息：  </div><div class="line">cd /proc/9213 </div><div class="line">查看该进程使用swap分区的统计信息，以不使用或只有少量的4kB为佳：  </div><div class="line">cat smaps | grep &apos;Swap:&apos;  </div><div class="line">同时打印出内存映射和swap使用信息：查看那些较大的内存消耗是否引发了大的swap使用  </div><div class="line">cat smaps | egrep &apos;^(Swap:Size)&apos;</div></pre></td></tr></table></figure>
<h5 id="redis-watchdog定位延时"><a href="#redis-watchdog定位延时" class="headerlink" title="redis watchdog定位延时"></a><b>redis watchdog定位延时</b></h5><p>注意： 实验功能，请确保redis数据已备份,会对redis服务性能产生影响<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">Redis software watchdog  </div><div class="line">该功能只能动态启用，使用以下命令：  </div><div class="line">CONFIG SET watchdog-period 500  </div><div class="line">注：redis会开始频繁监控自身的延时问题，并把问题输出到日志文件中去。  </div><div class="line">  </div><div class="line">关闭watchdog：  </div><div class="line">CONFIG SET watchdog-period 0</div></pre></td></tr></table></figure></p>
<h5 id="Redis-latency-monitoring-framework"><a href="#Redis-latency-monitoring-framework" class="headerlink" title="Redis latency monitoring framework"></a><b>Redis latency monitoring framework</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">CONFIG SET latency-monitor-threshold 100</div></pre></td></tr></table></figure>
<p>默认情况下，阈值设置为0，即禁用redis监控。实际上启用该监控功能，对redis所增加的成本很少.</p>
<p>LATENCY命令的使用方法</p>
<ol>
<li><p>查看最新的延时事件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">127.0.0.1:6379&gt; latency latest  </div><div class="line">1) 1) &quot;command&quot;     #event name  </div><div class="line">   2) (integer) 1480865648     #发生时间  </div><div class="line">   3) (integer) 207     #耗时，毫秒  </div><div class="line">   4) (integer) 239     #从redis启动或上次latency reset以来，这种事件的最大延时记录</div></pre></td></tr></table></figure>
</li>
<li><p>查看延时事件的历史信息</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">127.0.0.1:6379&gt; latency history command  </div><div class="line">  1) 1) (integer) 1480865710  </div><div class="line">     2) (integer) 207  </div><div class="line">  2) 1) (integer) 1480865711  </div><div class="line">     2) (integer) 217</div></pre></td></tr></table></figure>
</li>
<li><p>LATENCY DOCTOR<br>延时事件统计信息的智能分析与建议</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">127.0.0.1:6379&gt; latency doctor  </div><div class="line">Dave, I have observed latency spikes in this Redis instance.  </div><div class="line">You don&apos;t mind talking about it, do you Dave?  </div><div class="line">1. command: 5 latency spikes (average 300ms, mean deviation 120ms,  </div><div class="line">  period 73.40 sec). Worst all time event 500ms.  </div><div class="line">I have a few advices for you:  </div><div class="line">- Your current Slow Log configuration only logs events that are  </div><div class="line">  slower than your configured latency monitor threshold. Please  </div><div class="line">  use &apos;CONFIG SET slowlog-log-slower-than 1000&apos;.  </div><div class="line">- Check your Slow Log to understand what are the commands you are  </div><div class="line">  running which are too slow to execute. Please check  </div><div class="line">  http://redis.io/commands/slowlog for more information.</div></pre></td></tr></table></figure>
</li>
</ol>
<hr>
<p>您的鼓励是我写作最大的动力</p>
<p>俗话说，投资效率是最好的投资。 如果您感觉我的文章质量不错，读后收获很大，预计能为您提高 10% 的工作效率，不妨小额捐助我一下，让我有动力继续写出更多好文章。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;&lt;b&gt;前言&lt;/b&gt;&lt;/h5&gt;&lt;p&gt;redis性能分析常见的有以下几个方面：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;redis slowlog分析&lt;/li&gt;
&lt;li&gt;SCAN
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>spark develop env on eclipse</title>
    <link href="https://t1ger.github.io/2018/03/27/spark-develop-env-on-eclipse/"/>
    <id>https://t1ger.github.io/2018/03/27/spark-develop-env-on-eclipse/</id>
    <published>2018-03-27T07:51:04.000Z</published>
    <updated>2018-05-10T07:31:57.728Z</updated>
    
    <content type="html"><![CDATA[<h5 id="开发准备"><a href="#开发准备" class="headerlink" title="开发准备"></a><b>开发准备</b></h5><p>　　jdk1.8.45<br>　　spark-2.1.1-bin-hadoop2.7<br>　　CentOS系统<br>　　spark安装环境<br>　　hadoop-2.7.2<br>　　Hadoop安装环境</p>
<h5 id="开发环境配置"><a href="#开发环境配置" class="headerlink" title="开发环境配置"></a><b>开发环境配置</b></h5><ol>
<li>这里下载<a href="http://downloads.typesafe.com/scalaide-pack/4.7.0-vfinal-oxygen-212-20170929/scala-SDK-4.7.0-vfinal-2.12-win32.win32.x86_64.zip" target="_blank" rel="external">ScaleIDE for Win64</a>,其他可选对应版本</li>
<li>解压后，直接运行里边的eclipse</li>
<li>建立scala project,并创建scala类 WordCount</li>
<li><p>右键工程属性，添加spark-2.1.1-bin-hadoop2.7下面所有的库，可自定义库放进来：Java Build Path =&gt; Libraries =&gt; Add library,这里有个小知识:<br>add external jars  = 增加工程外部的包<br>add jars = 增加工程内包<br>add library = 增加一个库 // 是一些已经定义好的jar的集合<br>add class folder = 添加类的目录，是指本Eclipse范围中的，在工程列表下选取接口<br>区别:通过“add jar” 和“add external jars”添加的jar包作为程序的一部分被打包到最终的程序中。通过“User Libraries”添加的jar包不是。</p>
</li>
<li><p>编辑代码如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">package cn.spark.study.core</div><div class="line">import org.apache.spark.SparkConf</div><div class="line">import org.apache.spark.SparkContext </div><div class="line"></div><div class="line">object WordCount &#123;</div><div class="line">  </div><div class="line">    def main(args: Array[String]) &#123;</div><div class="line">      val conf= new SparkConf()</div><div class="line">            .setAppName(&quot;WordCount&quot;)</div><div class="line">            .setMaster(&quot;local&quot;)</div><div class="line">            </div><div class="line">      val sc = new SparkContext(conf)</div><div class="line">      val lines = sc.textFile(&quot;C:\\Users\\Administrator\\Desktop\\spark.txt&quot;)</div><div class="line">      </div><div class="line">      val words = lines.flatMap&#123;line =&gt; line.split(&quot; &quot;)&#125;</div><div class="line">      val pairs = words.map&#123;word =&gt; (word,1)&#125;</div><div class="line">      </div><div class="line">      val wordCounts = pairs.reduceByKey(_+_)</div><div class="line">      wordCounts.foreach(wordNumberPair =&gt; println(wordNumberPair._1 + &quot; : &quot; + wordNumberPair._2 + &quot;times .&quot;))</div><div class="line">      sc.stop()  </div><div class="line">            </div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
</li>
<li><p>右键，导出jar文件</p>
</li>
<li><p>在spark部署路径执行</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">/usr/local/spark/bin/spark-submit \</div><div class="line">  --class cn.spark.study.core.WordCount \</div><div class="line">  --num-executors 1 \</div><div class="line">  --executor-memory 1g \</div><div class="line">  /spark/scala/spark.study.scala.jar \</div></pre></td></tr></table></figure>
</li>
<li><p>参数解析：<br>可以执行./spark-submit –help获得帮助</p>
</li>
</ol>
<hr>
<p>您的鼓励是我写作最大的动力</p>
<p>俗话说，投资效率是最好的投资。 如果您感觉我的文章质量不错，读后收获很大，预计能为您提高 10% 的工作效率，不妨小额捐助我一下，让我有动力继续写出更多好文章。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;开发准备&quot;&gt;&lt;a href=&quot;#开发准备&quot; class=&quot;headerlink&quot; title=&quot;开发准备&quot;&gt;&lt;/a&gt;&lt;b&gt;开发准备&lt;/b&gt;&lt;/h5&gt;&lt;p&gt;　　jdk1.8.45&lt;br&gt;　　spark-2.1.1-bin-hadoop2.7&lt;br&gt;　　CentOS系
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>How to run Etherenum on CentOS7</title>
    <link href="https://t1ger.github.io/2018/02/05/How-to-run-Etherenum-on-CentOS7/"/>
    <id>https://t1ger.github.io/2018/02/05/How-to-run-Etherenum-on-CentOS7/</id>
    <published>2018-02-05T02:24:40.000Z</published>
    <updated>2018-02-05T06:47:03.527Z</updated>
    
    <content type="html"><![CDATA[<h5 id="以太坊客户端"><a href="#以太坊客户端" class="headerlink" title="以太坊客户端"></a><b>以太坊客户端</b></h5><p>Etherenum主流的客户端实现有以下几种，分别是C++, Go, Python，分别对应cpp-ethereum, go-ethereum, pyethapp.<br>C++实现称为Eth,Go语言的实现被称为Geth,Python的实现被称为Pyethapp.客户端是指一种接入Ethereum网络的节点并且与其发生交互和更新blockchain状态</p>
<p>其中最常用的有 Go 语言实现的 go-ethereum 客户端 Geth，支持接入以太坊网络并成为一个完整节点，也可作为一个 HTTP-RPC 服务器对外提供 JSON-RPC 接口</p>
<h5 id="Geth安装和配置"><a href="#Geth安装和配置" class="headerlink" title="Geth安装和配置"></a><b>Geth安装和配置</b></h5><p>Go语言的安装和配置略，Geth可以编译安装，也可以直接下载二进制安装包,参考<a href="https://geth.ethereum.org/downloads/" target="_blank" rel="external">这里</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">git clone https://github.com/ethereum/go-ethereum</div><div class="line">cd go-ethereum</div><div class="line">make geth</div></pre></td></tr></table></figure></p>
<p>安装完成后，可以使用 geth version 命令查看是否安装成功。记得把生成的 geth 加入到系统的环境变量中</p>
<h5 id="安装-Solidity-编译器"><a href="#安装-Solidity-编译器" class="headerlink" title="安装 Solidity 编译器"></a><b>安装 Solidity 编译器</b></h5><p>Solidity 编译器也有多种方法安装，参照<a href="https://solidity.readthedocs.io/en/latest/installing-solidity.html" target="_blank" rel="external">这里</a><br>官方推荐使用基于浏览器的 IDE 环境:<a href="https://remix.ethereum.org" target="_blank" rel="external">Remix</a></p>
<h5 id="私有链搭建"><a href="#私有链搭建" class="headerlink" title="私有链搭建"></a><b>私有链搭建</b></h5><ul>
<li><p>配置初始状态<br>运行以太坊私有链，需要定义自己的创世区块，创世区块信息写在一个 JSON 格式的配置文件中</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">[root@localhost]#cat genesis.json </div><div class="line">&#123;</div><div class="line">        &quot;nonce&quot;: &quot;0x0000000000000042&quot;,</div><div class="line">        &quot;timestamp&quot;: &quot;0x00&quot;,</div><div class="line">        &quot;parentHash&quot;: &quot;0x0000000000000000000000000000000000000000000000000000000000000000&quot;,</div><div class="line">        &quot;extraData&quot;: &quot;0x00&quot;,</div><div class="line">        &quot;gasLimit&quot;: &quot;0x8000000&quot;,</div><div class="line">        &quot;difficulty&quot;: &quot;0x4000&quot;,</div><div class="line">        &quot;mixhash&quot;: &quot;0x0000000000000000000000000000000000000000000000000000000000000000&quot;,</div><div class="line">        &quot;coinbase&quot;: &quot;0x3333333333333333333333333333333333333333&quot;,</div><div class="line">        </div><div class="line">        &quot;config&quot;: &#123;</div><div class="line">                &quot;chainId&quot;: 1984,</div><div class="line">                &quot;homesteadBlock&quot;: 0,</div><div class="line">                &quot;eip155Block&quot;: 0,</div><div class="line">                &quot;eip158Block&quot;: 0</div><div class="line">        &#125;,</div><div class="line">        </div><div class="line">        &quot;alloc&quot;:&#123;&#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">#初始化创世区块</div><div class="line">[root@localhost]#geth --datadir &quot;/root/.ethereum&quot; init genesis.json </div><div class="line">I0131 18:57:50.858609 ethdb/database.go:83] Alloted 16MB cache and 16 file handles to /root/.ethereum/chaindata</div><div class="line">I0131 18:57:50.865414 cmd/geth/main.go:299] successfully wrote genesis block and/or chain rule set: f2ebfaadd3ae79075cc9485eb5ab634c9927504736c60dc88d571fb85d9f6493</div></pre></td></tr></table></figure>
<p>  chainID 指定了独立的区块链网络 ID。网络 ID 在连接到其他节点的时候会用到，以太坊公网的网络 ID 是 1，为了不与公有链网络冲突，运行私有链节点的时候要指定自己的网络 ID。不同 ID 网络的节点无法相互连接。配置文件还对当前挖矿难度 difficulty、区块 Gas 消耗限制 gasLimit 等参数进行了设置<br>打开此节点的命令控制台console，并新建账户</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">#启动私有链节点</div><div class="line">geth --datadir &quot;/root/.ethereum&quot; --port 30002 --nodiscover  console</div><div class="line">#新建用户</div><div class="line">&gt;personal.newAccount(&quot;passward&quot;)</div><div class="line">#开始挖矿</div><div class="line">&gt;miner.start(1)</div><div class="line">true</div><div class="line">#停止挖矿</div><div class="line">&gt;miner.stop()</div><div class="line">true</div><div class="line">#查看账户</div><div class="line">&gt; eth.accounts</div><div class="line">#查看账户余额</div><div class="line">&gt;eth.getBalance(eth.accounts[0])</div></pre></td></tr></table></figure>
<p>  发送交易</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">&gt;  eth.getBalance(eth.accounts[0])</div><div class="line">320000000000000300000</div><div class="line">&gt; eth.getBalance(eth.accounts[1])</div><div class="line">0</div><div class="line">&gt;  personal.unlockAccount(eth.accounts[0])</div><div class="line">Unlock account 0x30a42e0da52f20154ce2b966a53a81099f048e73</div><div class="line">Passphrase: </div><div class="line">true</div><div class="line">&gt;  amount = web3.toWei(5,&apos;ether&apos;)</div><div class="line">&quot;5000000000000000000&quot;</div><div class="line">&gt; eth.sendTransaction(&#123;from:eth.accounts[0],to:eth.accounts[1],value:amount&#125;)</div><div class="line">I0205 11:51:11.752411 eth/api.go:1185] Tx(0x318cf8d6f86f0f294eb927af3019cede420990beac1cf72046d03d454ff48b88) to: 0xae370f3b2af53f6ba282a76bbe6956d443bc8d79</div><div class="line">&quot;0x318cf8d6f86f0f294eb927af3019cede420990beac1cf72046d03d454ff48b88&quot;</div><div class="line"></div><div class="line">#此时如果没有挖矿，用 txpool.status 命令可以看到本地交易池中有一个待确认的交易，可以使用 eth.getBlock(&quot;pending&quot;, true).transactions 查看当前待确认交易</div><div class="line"></div><div class="line">&gt; miner.start(1);admin.sleepBlocks(1);miner.stop();</div><div class="line"></div><div class="line">#新区块挖出后，挖矿结束，查看账户 1 的余额，已经收到了账户 0 的以太币：</div><div class="line">&gt; web3.fromWei(eth.getBalance(eth.accounts[1]),&apos;ether&apos;)</div><div class="line">5</div></pre></td></tr></table></figure>
<p>  查看交易和区块</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line">#查看当前区块总数：</div><div class="line">&gt; eth.blockNumber</div><div class="line">66</div><div class="line">#通过交易 Hash 查看交易（Hash 值包含在上面交易返回值中）</div><div class="line">&gt; eth.getTransaction(&quot;0x318cf8d6f86f0f294eb927af3019cede420990beac1cf72046d03d454ff48b88&quot;)</div><div class="line">&#123;</div><div class="line">  blockHash: &quot;0x45c8732599e45791c22a4c1d2278387eff05e6f062f8f1871912bd4fbee4787f&quot;,</div><div class="line">  blockNumber: 65,</div><div class="line">  from: &quot;0x30a42e0da52f20154ce2b966a53a81099f048e73&quot;,</div><div class="line">  gas: 90000,</div><div class="line">  gasPrice: 20000000000,</div><div class="line">  hash: &quot;0x318cf8d6f86f0f294eb927af3019cede420990beac1cf72046d03d454ff48b88&quot;,</div><div class="line">  input: &quot;0x&quot;,</div><div class="line">  nonce: 0,</div><div class="line">  to: &quot;0xae370f3b2af53f6ba282a76bbe6956d443bc8d79&quot;,</div><div class="line">  transactionIndex: 0,</div><div class="line">  value: 5000000000000000000</div><div class="line">&#125;</div><div class="line"></div><div class="line">#通过区块号查看区块：</div><div class="line">&gt; eth.getBlock(66)</div><div class="line">&#123;</div><div class="line">  difficulty: 131072,</div><div class="line">  extraData: &quot;0xd783010412844765746887676f312e372e31856c696e7578&quot;,</div><div class="line">  gasLimit: 125836029,</div><div class="line">  gasUsed: 0,</div><div class="line">  hash: &quot;0x7398a4794f1a6c24fa193b3a45e96c9b8925047a4c4ffb5112ffee182e8e05ba&quot;,</div><div class="line">  logsBloom: &quot;0x00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000&quot;,</div><div class="line">  miner: &quot;0x30a42e0da52f20154ce2b966a53a81099f048e73&quot;,</div><div class="line">  nonce: &quot;0x2d4f36efb89667d8&quot;,</div><div class="line">  number: 66,</div><div class="line">  parentHash: &quot;0x45c8732599e45791c22a4c1d2278387eff05e6f062f8f1871912bd4fbee4787f&quot;,</div><div class="line">  receiptRoot: &quot;0x56e81f171bcc55a6ff8345e692c0f86e5b48e01b996cadc001622fb5e363b421&quot;,</div><div class="line">  sha3Uncles: &quot;0x1dcc4de8dec75d7aab85b567b6ccd41ad312451b948a7413f0a142fd40d49347&quot;,</div><div class="line">  size: 536,</div><div class="line">  stateRoot: &quot;0x50cd8603d98fe157cabc1d596e7a7926231e8dee792cabd199f263dc0c5691a5&quot;,</div><div class="line">  timestamp: 1517802759,</div><div class="line">  totalDifficulty: 8792593,</div><div class="line">  transactions: [],</div><div class="line">  transactionsRoot: &quot;0x56e81f171bcc55a6ff8345e692c0f86e5b48e01b996cadc001622fb5e363b421&quot;,</div><div class="line">  uncles: []</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>  连接到其他节点</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">#通过 admin.addPeer() 方法连接到其他节点，两个节点要要指定相同的 chainID</div><div class="line">查看其中一个 enode 信息：</div><div class="line">&gt; admin.nodeInfo.enode</div><div class="line">&quot;enode://34753757021f60aac9ad5402344ae353b9c431c3b60c8e671a753639dd5aef35bb180b8d056a5f3b1ac46fd5cda0163f88eff8e1856a1b3982cc576df81e295a@[::]:30002?discport=0&quot;</div><div class="line"></div><div class="line">然后在另外节点的 JavaScript console 中执行 admin.addPeer()，就可以连接：</div><div class="line"> admin.addPeer(&quot;enode://34753757021f60aac9ad5402344ae353b9c431c3b60c8e671a753639dd5aef35bb180b8d056a5f3b1ac46fd5cda0163f88eff8e1856a1b3982cc576df81e295a@[::]:30002?discport=0&quot;)</div><div class="line"></div><div class="line">#addPeer() 的参数就是节点二的 enode 信息，注意要把 enode 中的 [::] 替换成节点二的 IP 地址。</div><div class="line">连接成功后，节点就会开始同步另一个节点的区块，同步完成后，</div><div class="line">任意一个节点开始挖矿，另一个节点会自动同步区块，向任意一个节点发送交易，另一个节点也会收到该笔交易</div></pre></td></tr></table></figure>
<p>  除了上面的方法，也可以在启动节点的时候指定 –bootnodes 选项连接到其他节点</p>
</li>
</ul>
<h5 id="智能合约操作"><a href="#智能合约操作" class="headerlink" title="智能合约操作"></a><b>智能合约操作</b></h5><ul>
<li><p>创建和编译智能合约<br>新建一个 Solidity 智能合约文件，命名为 test.sol，该合约包含一个方法 multiply()，将输入的两个数相乘后输出：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">pragma solidity ^0.4.0;</div><div class="line">contract Test</div><div class="line">&#123;</div><div class="line">    function multiply(uint a, uint b) returns (uint)</div><div class="line">    &#123;</div><div class="line">        return a * b;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>  编译智能合约，获得编译后的 EVM 二进制码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">solc --bin test.sol</div></pre></td></tr></table></figure>
<p>  再用 solc 获取智能合约的 JSON ABI（Application Binary Interface），其中指定了合约接口，包括可调用的合约方法、变量、事件等</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">solc --abi test.sol</div></pre></td></tr></table></figure>
<p>  回到 Geth 的控制台，用变量 code 和 abi 记录上面两个值，注意在 code 前加上 0x 前缀：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt; code = &quot;0x6060604052341561000f57600080fd5b5b60b48061001e6000396000f30060606040526000357c0100000000000000000000000000000000000000000000000000000000900463ffffffff168063165c4a1614603d575b600080fd5b3415604757600080fd5b60646004808035906020019091908035906020019091905050607a565b6040518082815260200191505060405180910390f35b600081830290505b929150505600a165627a7a72305820b494a4b3879b3810accf64d4cc3e1be55f2f4a86f49590b8a9b8d7009090a5d30029&quot;</div><div class="line">&gt; abi = [&#123;&quot;constant&quot;:false,&quot;inputs&quot;:[&#123;&quot;name&quot;:&quot;a&quot;,&quot;type&quot;:&quot;uint256&quot;&#125;,&#123;&quot;name&quot;:&quot;b&quot;,&quot;type&quot;:&quot;uint256&quot;&#125;],&quot;name&quot;:&quot;multiply&quot;,&quot;outputs&quot;:[&#123;&quot;name&quot;:&quot;&quot;,&quot;type&quot;:&quot;uint256&quot;&#125;],&quot;payable&quot;:false,&quot;stateMutability&quot;:&quot;nonpayable&quot;,&quot;type&quot;:&quot;function&quot;&#125;]</div></pre></td></tr></table></figure>
</li>
<li><p>部署智能合约<br>这里使用账户 0 来部署合约，首先解锁账户：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&gt;  personal.unlockAccount(eth.accounts[0])</div><div class="line">Unlock account 0x30a42e0da52f20154ce2b966a53a81099f048e73</div><div class="line">Passphrase: </div><div class="line">true</div></pre></td></tr></table></figure>
</li>
<li><p>发送部署合约的交易</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">&gt; myContract = eth.contract(abi)</div><div class="line">...</div><div class="line">&gt; contract = myContract.new(&#123;from:eth.accounts[0],data:code,gas:1000000&#125;)</div><div class="line"></div><div class="line">INFO [09-12|08:05:19] Submitted contract creation              fullhash=0x0a7dfa9cac7ef836a72ed1d5bbfa65c0220347cde4efb067a0b03b15fb70bce1 contract=0x7cbe4019e993f9922b8233502d94890099ee59e6</div><div class="line">&#123;</div><div class="line">  abi: [&#123;</div><div class="line">      constant: false,</div><div class="line">      inputs: [&#123;...&#125;, &#123;...&#125;],</div><div class="line">      name: &quot;multiply&quot;,</div><div class="line">      outputs: [&#123;...&#125;],</div><div class="line">      payable: false,</div><div class="line">      stateMutability: &quot;nonpayable&quot;,</div><div class="line">      type: &quot;function&quot;</div><div class="line">  &#125;],</div><div class="line">  address: undefined,</div><div class="line">  transactionHash: &quot;0x0a7dfa9cac7ef836a72ed1d5bbfa65c0220347cde4efb067a0b03b15fb70bce1&quot;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>  此时如果没有挖矿，用 txpool.status 命令可以看到本地交易池中有一个待确认的交易。可以查看当前待确认的交易，使用 miner.start() 命令开始挖矿，一段时间后交易会被确认，随新区块进入区块链</p>
</li>
<li><p>调用智能合约</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&gt; contract.multiply.sendTransaction(2, 4, &#123;from:eth.accounts[0]&#125;)</div></pre></td></tr></table></figure>
<p>  如果只是本地运行该方法查看返回结果，可以采用如下方式：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt; contract.multiply.call(2，4)</div><div class="line">8</div></pre></td></tr></table></figure>
</li>
</ul>
<h5 id="安装ethereum钱包"><a href="#安装ethereum钱包" class="headerlink" title="安装ethereum钱包"></a><b>安装ethereum钱包</b></h5><p>钱包下载地址，参考<a href="https://github.com/ethereum/mist/releases" target="_blank" rel="external">这里</a><br>钱包在启动时默认在本机查找的IPC路径,钱包启动后能自动识别到你的私有链<br>OS的默认查找路径如下,具体可查看<a href="https://github.com/ethereum/mist/blob/master/modules/settings.js#L248" target="_blank" rel="external">这里</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">windows: .\pipe\geth.ipc</div><div class="line">linux: /.ethereum/geth.ipc</div><div class="line">freebsd: /.ethereum/geth.ipc</div><div class="line">sunos: /.ethereum/geth.ipc</div></pre></td></tr></table></figure></p>
<p>ref<br><a href="https://github.com/ethereum/go-ethereum/wiki/Private-network" target="_blank" rel="external">Private network</a><br><a href="https://github.com/ethereum/cpp-ethereum" target="_blank" rel="external">ethereum/cpp-ethereum</a><br><a href="https://github.com/ethereum/go-ethereum" target="_blank" rel="external">Go Ethereum</a><br><a href="http://www.ethdocs.org/en/latest/ethereum-clients/cpp-ethereum/installing-binaries/index.html" target="_blank" rel="external">Installing binaries</a><br><a href="https://g2ex.github.io/2017/09/12/ethereum-guidance/" target="_blank" rel="external">以太坊私有链搭建指南</a><br><a href="https://github.com/xiaoping378/blog/blob/master/posts/%E4%BB%A5%E5%A4%AA%E5%9D%8A-%E7%A7%81%E6%9C%89%E9%93%BE%E6%90%AD%E5%BB%BA%E5%88%9D%E6%AD%A5%E5%AE%9E%E8%B7%B5.md" target="_blank" rel="external">利用puppeth搭建POA共识的以太坊私链网络</a></p>
<hr>
<p>您的鼓励是我写作最大的动力</p>
<p>俗话说，投资效率是最好的投资。 如果您感觉我的文章质量不错，读后收获很大，预计能为您提高 10% 的工作效率，不妨小额捐助我一下，让我有动力继续写出更多好文章。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;以太坊客户端&quot;&gt;&lt;a href=&quot;#以太坊客户端&quot; class=&quot;headerlink&quot; title=&quot;以太坊客户端&quot;&gt;&lt;/a&gt;&lt;b&gt;以太坊客户端&lt;/b&gt;&lt;/h5&gt;&lt;p&gt;Etherenum主流的客户端实现有以下几种，分别是C++, Go, Python，分别对应c
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>高可用的MongoDB集群</title>
    <link href="https://t1ger.github.io/2018/01/23/%E9%AB%98%E5%8F%AF%E7%94%A8%E7%9A%84MongoDB%E9%9B%86%E7%BE%A4/"/>
    <id>https://t1ger.github.io/2018/01/23/高可用的MongoDB集群/</id>
    <published>2018-01-23T01:53:10.000Z</published>
    <updated>2018-01-25T04:06:31.446Z</updated>
    
    <content type="html"><![CDATA[<h5 id="前言"><a href="#前言" class="headerlink" title="前言"></a><b>前言</b></h5><p>MongoDB是一个介于关系数据库和非关系数据库之间的产品，适合存储对象及JSON形式的数据。支持丰富的查询方式，几乎可以实现类似关系数据库单表查询的绝大部分功能，而且还支持对数据建立索引。</p>
<p>mongo适用的场景：</p>
<ul>
<li>网站数据:Mongo 非常适合实时的插入,更新与查询,并具备网站实时数据存储所需的复制及高度伸缩性</li>
<li>缓存:由于性能很高,Mongo 也适合作为信息基础设施的缓存层</li>
<li>高伸缩性的场景:Mongo非常适合由数十或数百台服务器组成的数据库</li>
<li>用于对象及JSON数据的存储:Mongo的BSON数据格式非常适合文档格式化的存储及查询</li>
</ul>
<p>MongDB不适合的场景:</p>
<ul>
<li>高度事务性的系统:例如银行或会计系统。</li>
<li>传统的商业智能应用:针对特定问题的 BI 数据库会对产生高度优化的查询方式。对于此类应用,数据仓库可能时更适合的选择(如Hadoop套件中的Hive)</li>
</ul>
<h5 id="MongDB-概念"><a href="#MongDB-概念" class="headerlink" title="MongDB 概念"></a><b>MongDB 概念</b></h5><p>首先了解几个概念：路由，分片、副本集、配置服务器等<br>mongodb支持数据的分布式存储，将collection作数据分片，减少每个节点的数据负载。<br>每个节点可以位于不同的物理机器上，一个简单的sharding集群如下图所示（引用自mongodb官网）<br><img src="https://docs.mongodb.com/manual/_images/sharded-cluster-production-architecture.bakedsvg.svg" alt="mongdb"><br>从图中可以看到有四个组件：mongos、config server、shard、replica set。</p>
<p>mongos，数据库集群请求的入口，所有的请求都通过mongos进行协调，不需要在应用程序添加一个路由选择器，mongos自己就是一个请求分发中心，它负责把对应的数据请求请求转发到对应的shard服务器上。在生产环境通常有多mongos作为请求的入口，防止其中一个挂掉所有的mongodb请求都没有办法操作。</p>
<p>config server，顾名思义为配置服务器，存储所有数据库元信息（路由、分片）的配置。mongos本身没有物理存储分片服务器和数据路由信息，只是缓存在内存里，配置服务器则实际存储这些数据。mongos第一次启动或者关掉重启就会从 config server 加载配置信息，以后如果配置服务器信息变化会通知到所有的 mongos 更新自己的状态，这样 mongos 就能继续准确路由。在生产环境通常有多个 config server 配置服务器，因为它存储了分片路由的元数据，防止数据丢失！</p>
<p>shard，分片（sharding）是指将数据库拆分，将其分散在不同的机器上的过程。将数据分散到不同的机器上，不需要功能强大的服务器就可以存储更多的数据和处理更大的负载。基本思想就是将集合切成小块，这些块分散到若干片里，每个片只负责总数据的一部分，最后通过一个均衡器来对各个分片进行均衡（数据迁移）。</p>
<p>replica set，中文翻译副本集，其实就是shard的备份，防止shard挂掉之后数据丢失。复制提供了数据的冗余备份，并在多个服务器上存储数据副本，提高了数据的可用性， 并可以保证数据的安全性。</p>
<p>仲裁者（Arbiter），是复制集中的一个MongoDB实例，它并不保存数据。仲裁节点使用最小的资源并且不要求硬件设备，不能将Arbiter部署在同一个数据集节点中，可以部署在其他应用服务器或者监视服务器中，也可部署在单独的虚拟机中。为了确保复制集中有奇数的投票成员（包括primary），需要添加仲裁节点做为投票，否则primary不能运行时不会自动切换primary。</p>
<p>简单了解之后，我们可以这样总结一下，应用请求mongos来操作mongodb的增删改查，配置服务器存储数据库元信息，并且和mongos做同步，数据最终存入在shard（分片）上，为了防止数据丢失同步在副本集中存储了一份，仲裁在数据存储到分片的时候决定存储到哪个节点。</p>
<h5 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a><b>环境准备</b></h5><p>九台测试服务器，操作系统centos6.8， MongoDB 3.6</p>
<p>服务器角色分配<br>192.168.1.100 config server<br>192.168.1.101 config server<br>192.168.1.102 config server</p>
<p>192.168.1.103 shard server1<br>192.168.1.104 shard server1 副节点<br>192.168.1.105 shard server1 仲裁</p>
<p>192.168.1.107 shard server2<br>192.168.1.108 shard server2 副节点<br>192.168.1.109 shard server2 仲裁</p>
<p>192.168.1.110 mongos</p>
<h5 id="集群搭建"><a href="#集群搭建" class="headerlink" title="集群搭建"></a><b>集群搭建</b></h5><ul>
<li><p>Install MongoDB on All Nodes</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">cat &gt; /etc/yum.repos.d/mongodb-org-3.6.repo &lt;&lt; &apos;EOF&apos;</div><div class="line">[mongodb-org-3.6]</div><div class="line">name=MongoDB Repository</div><div class="line">baseurl=https://repo.mongodb.org/yum/redhat/$releasever/mongodb-org/3.6/x86_64/</div><div class="line">gpgcheck=1</div><div class="line">enabled=1</div><div class="line">gpgkey=https://www.mongodb.org/static/pgp/server-3.6.asc</div><div class="line">EOF</div><div class="line"></div><div class="line">Package Name		Description</div><div class="line">mongodb-org		A metapackage that will automatically install the four component packages listed below.</div><div class="line">mongodb-org-server	Contains the mongod daemon and associated configuration and init scripts.</div><div class="line">mongodb-org-mongos	Contains the mongos daemon.</div><div class="line">mongodb-org-shell	Contains the mongo shell.</div><div class="line">mongodb-org-tools	Contains the following MongoDB tools: mongoimport bsondump, mongodump, mongoexport, mongofiles, mongoperf, mongorestore, mongostat, and mongotop.</div><div class="line"></div><div class="line"></div><div class="line">yum -y install mongodb-org</div></pre></td></tr></table></figure>
</li>
<li><p>Configure Firewalld</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">#for centos6，add under to file  /etc/sysconfig/iptables</div><div class="line">cat /etc/sysconfig/iptables</div><div class="line">-A INPUT -p tcp -m state --state NEW -m tcp --dport 22 -j ACCEPT </div><div class="line">-A INPUT -p tcp -m state --state NEW -m tcp --dport 27017 -j ACCEPT </div><div class="line">service iptables restart</div><div class="line">#for centos7</div><div class="line">yum -y install firewalld</div><div class="line">systemctl start firewalld</div><div class="line">systemctl enable firewalld</div><div class="line"></div><div class="line">firewall-cmd --permanent --add-port=22/tcp</div><div class="line">firewall-cmd --permanent --add-port=27017/tcp</div><div class="line">firewall-cmd --reload</div></pre></td></tr></table></figure>
</li>
<li><p>Configure MongoDB config server </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">net:</div><div class="line">  port: 27017</div><div class="line">  bindIP: 127.0.0.1,192.168.1.101  </div><div class="line">  #declare this is a config db of a cluster;  </div><div class="line">sharding:</div><div class="line">   clusterRole: configsvr</div><div class="line">replication:</div><div class="line">   replSetName: configs  </div><div class="line"></div><div class="line">#登录任意一台配置服务器，初始化配置副本集</div><div class="line"></div><div class="line">#连接</div><div class="line">mongo --port 27017</div><div class="line">#config变量</div><div class="line">config = &#123;</div><div class="line">		_id : &quot;configs&quot;,</div><div class="line">		members : [</div><div class="line">		&#123;_id : 0, host : &quot;192.168.1.100:27017&quot; &#125;,</div><div class="line">		&#123;_id : 0, host : &quot;192.168.1.101:27017&quot; &#125;,</div><div class="line">		&#123;_id : 1, host : &quot;192.168.1.102:27017&quot; &#125;</div><div class="line">		]</div><div class="line">	&#125;</div><div class="line"></div><div class="line">#初始化副本集</div><div class="line">rs.initiate(config)</div></pre></td></tr></table></figure>
<p>  其中，”_id” : “configs”应与配置文件中配置的 replicaction.replSetName 一致，”members” 中的 “host” 为三个节点的 ip 和 port</p>
</li>
<li><p>Configure MongoDB Replica Set </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">vim /etc/mongod.conf</div><div class="line"></div><div class="line">net:</div><div class="line">  port: 27017</div><div class="line">  bindIP: 127.0.0.1,192.168.1.103</div><div class="line">  </div><div class="line">replication:</div><div class="line">  replSetName: &quot;shard1&quot;</div><div class="line">sharding:</div><div class="line">  clusterRole: shardsvr</div><div class="line">  </div><div class="line">mongo --port 27017</div><div class="line">#使用admin数据库</div><div class="line">use admin</div><div class="line">#定义副本集配置</div><div class="line">config = &#123;</div><div class="line"> 	 _id : &quot;shard1&quot;,</div><div class="line">	 members : [</div><div class="line">         &#123;_id : 0, host : &quot;192.168.1.103:27017&quot; &#125;,</div><div class="line">         &#123;_id : 1, host : &quot;192.168.1.104:27017&quot; &#125;,</div><div class="line">         &#123;_id : 2, host : &quot;192.168.1.105:27017&quot;, arbiterOnly: true &#125;</div><div class="line">     ]</div><div class="line">&#125;</div><div class="line">#初始化副本集配置</div><div class="line">rs.initiate(config);</div></pre></td></tr></table></figure>
<p>  第二个Replica Set也执行类似操作</p>
</li>
</ul>
<ul>
<li><p>Configure MongoDB mongos</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">[root@localhost ~]# cat mongos.conf </div><div class="line"># where to write logging data.</div><div class="line">systemLog:</div><div class="line">destination: file</div><div class="line">logAppend: true</div><div class="line">path: /var/log/mongodb/mongos.log</div><div class="line"></div><div class="line">#security:</div><div class="line">#keyFile: /opt/mongo/mongodb-keyfile</div><div class="line"></div><div class="line">port=27017</div><div class="line">bind_ip=127.0.0.1,172.16.56.233</div><div class="line">#监听的配置服务器,只能有1个或者3个 configs为配置服务器的副本集名字</div><div class="line">configdb=configs/192.1688.1.100:27017,192.1688.1.101:27017,192.168.1.102:27017</div><div class="line"></div><div class="line">mongos --config mongos.conf  --fork</div></pre></td></tr></table></figure>
</li>
<li><p>MongoDB Replica Set initiate</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">登陆任意一台mongos</div><div class="line"></div><div class="line">mongo --port 27017</div><div class="line">#使用admin数据库</div><div class="line">use  admin</div><div class="line">#串联路由服务器与分配副本集</div><div class="line">sh.addShard(&quot;shard1/192.168.1.103:27017,192.168.1.104:27017,192.168.1.105:27017&quot;)</div><div class="line">sh.addShard(&quot;shard2/192.168.1.106:27017,192.168.1.107:27017,192.168.1.108:27017&quot;)</div><div class="line">#查看集群状态</div><div class="line">sh.status()</div></pre></td></tr></table></figure>
</li>
<li><p>Test the Replication</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">use admin</div><div class="line">#指定testdb分片生效</div><div class="line">sh.enableSharding( &quot;testdb&quot; )</div><div class="line"></div><div class="line">#Before sharding a non-empty collection, create an index on the shard key.</div><div class="line">use testdb</div><div class="line">db.table1.createIndex( &#123; id : 1 &#125; )</div><div class="line"></div><div class="line">#指定数据库里需要分片的集合和片键</div><div class="line">use testdb</div><div class="line">sh.shardCollection( &quot;testdb.table1&quot;, &#123; id : 1 &#125; )</div><div class="line"></div><div class="line">#Confirm the shard is balancing</div><div class="line">use testdb</div><div class="line">db.stats()</div><div class="line">db.printShardingStatus()</div></pre></td></tr></table></figure>
<p>  我们设置testdb的 table1 表需要分片，根据 id 自动分片到 shard1 ，shard2 上面去。要这样设置是因为不是所有mongodb 的数据库和表 都需要分片！<br>测试分片配置结果</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div></pre></td><td class="code"><pre><div class="line">mongo  127.0.0.1:27017</div><div class="line">#使用testdb</div><div class="line">use  testdb;</div><div class="line">#插入测试数据</div><div class="line">for (var i = 1; i &lt;= 600000; i++)db.table1.save(&#123;id:i,&quot;test1&quot;:&quot;testval1&quot;&#125;);</div><div class="line">#查看分片情况如下，部分无关信息省掉了</div><div class="line">mongos&gt; db.printShardingStatus()</div><div class="line">--- Sharding Status --- </div><div class="line">  sharding version: &#123;</div><div class="line">        &quot;_id&quot; : 1,</div><div class="line">        &quot;minCompatibleVersion&quot; : 5,</div><div class="line">        &quot;currentVersion&quot; : 6,</div><div class="line">        &quot;clusterId&quot; : ObjectId(&quot;5a674706887e9c5d977acacc&quot;)</div><div class="line">  &#125;</div><div class="line">  shards:</div><div class="line">        &#123;  &quot;_id&quot; : &quot;shard1&quot;,  &quot;host&quot; : &quot;shard1/192.168.1.103:27017,192.168.1.104:27017&quot;,  &quot;state&quot; : 1 &#125;</div><div class="line">        &#123;  &quot;_id&quot; : &quot;shard2&quot;,  &quot;host&quot; : &quot;shard2/192.168.1.106:27017,192.168.1.107:27017&quot;,  &quot;state&quot; : 1 &#125;</div><div class="line">  active mongoses:</div><div class="line">        &quot;3.6.2&quot; : 1</div><div class="line">  autosplit:</div><div class="line">        Currently enabled: yes</div><div class="line">  balancer:</div><div class="line">        Currently enabled:  yes</div><div class="line">        Currently running:  no</div><div class="line">        Failed balancer rounds in last 5 attempts:  0</div><div class="line">        Migration Results for the last 24 hours: </div><div class="line">                No recent migrations</div><div class="line">  databases:</div><div class="line">        &#123;  &quot;_id&quot; : &quot;config&quot;,  &quot;primary&quot; : &quot;config&quot;,  &quot;partitioned&quot; : true &#125;</div><div class="line">                config.system.sessions</div><div class="line">                        shard key: &#123; &quot;_id&quot; : 1 &#125;</div><div class="line">                        unique: false</div><div class="line">                        balancing: true</div><div class="line">                        chunks:</div><div class="line">                                shard1  1</div><div class="line">                        &#123; &quot;_id&quot; : &#123; &quot;$minKey&quot; : 1 &#125; &#125; --&gt;&gt; &#123; &quot;_id&quot; : &#123; &quot;$maxKey&quot; : 1 &#125; &#125; on : shard1 Timestamp(1, 0) </div><div class="line"></div><div class="line">        &#123;  &quot;_id&quot; : &quot;testdb&quot;,  &quot;primary&quot; : &quot;shard1&quot;,  &quot;partitioned&quot; : true &#125;</div><div class="line">                testdb.table1</div><div class="line">                        shard key: &#123; &quot;id&quot; : 1 &#125;</div><div class="line">                        unique: false</div><div class="line">                        balancing: true</div><div class="line">                        chunks:</div><div class="line">                                shard1  1</div><div class="line">                        &#123; &quot;id&quot; : &#123; &quot;$minKey&quot; : 1 &#125; &#125; --&gt;&gt; &#123; &quot;id&quot; : &#123; &quot;$maxKey&quot; : 1 &#125; &#125; on : shard1 Timestamp(1, 0)</div></pre></td></tr></table></figure>
<p>  可以看到目前只有一个chunk在shard1整个shard上</p>
</li>
</ul>
<h5 id="遇到的错误"><a href="#遇到的错误" class="headerlink" title="遇到的错误"></a><b>遇到的错误</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"> rs.initiate(config);</div><div class="line">&#123;</div><div class="line">        &quot;ok&quot; : 0,</div><div class="line">        &quot;errmsg&quot; : &quot;Attempting to initiate a replica set with name shard2, but command line reports shard1; rejecting&quot;,</div><div class="line">        &quot;code&quot; : 93,</div><div class="line">        &quot;codeName&quot; : &quot;InvalidReplicaSetConfig&quot;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>解决方案： 由于配置replSetName错误引起</p>
<h5 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a><b>常用命令</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">#List Databases with Sharding Enabled</div><div class="line">use config</div><div class="line">db.databases.find( &#123; &quot;partitioned&quot;: true &#125; )</div><div class="line"></div><div class="line">#List Shards</div><div class="line">db.adminCommand( &#123; listShards: 1 &#125; )</div><div class="line">#remove shard</div><div class="line">db.adminCommand( &#123; removeShard: &quot;testdb&quot; &#125; )</div><div class="line"></div><div class="line">#View Cluster Details</div><div class="line">db.printShardingStatus() or sh.status()</div><div class="line"></div><div class="line">#drop database</div><div class="line">use newdb</div><div class="line">switched to db newdb</div><div class="line">db.dropDatabase()</div></pre></td></tr></table></figure>
<p>ref<br><a href="http://blog.sina.com.cn/s/blog_8ea8e9d50102wl8x.html" target="_blank" rel="external">mongoDB-3.x Sharding with Replica</a><br><a href="https://docs.mongodb.com/manual/tutorial/convert-replica-set-to-replicated-shard-cluster/" target="_blank" rel="external">From Replica Set to Sharding</a><br><a href="http://www.cnblogs.com/xybaby/p/6832296.html" target="_blank" rel="external">通过一步步创建sharded cluster来认识mongodb</a></p>
<hr>
<p>您的鼓励是我写作最大的动力</p>
<p>俗话说，投资效率是最好的投资。 如果您感觉我的文章质量不错，读后收获很大，预计能为您提高 10% 的工作效率，不妨小额捐助我一下，让我有动力继续写出更多好文章。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;&lt;b&gt;前言&lt;/b&gt;&lt;/h5&gt;&lt;p&gt;MongoDB是一个介于关系数据库和非关系数据库之间的产品，适合存储对象及JSON形式的数据。支持丰富的查询方式，几乎可以实现
    
    </summary>
    
    
  </entry>
  
</feed>
