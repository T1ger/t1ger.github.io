<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>t1ger的茶馆</title>
  <subtitle>头顶有光终是幻，足下生云未是仙</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://t1ger.github.io/"/>
  <updated>2018-11-05T09:59:45.781Z</updated>
  <id>https://t1ger.github.io/</id>
  
  <author>
    <name>t1ger</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>mysql audit plugin</title>
    <link href="https://t1ger.github.io/2018/11/05/mysql-audit-plugin/"/>
    <id>https://t1ger.github.io/2018/11/05/mysql-audit-plugin/</id>
    <published>2018-11-05T09:40:06.000Z</published>
    <updated>2018-11-05T09:59:45.781Z</updated>
    
    <content type="html"><![CDATA[<p>此文转载于<a href="https://blog.csdn.net/heizistudio/article/details/50954294" target="_blank" rel="external">ora600</a>,略作调整.</p>
<p>下载地址如下<br><a href="http://pan.baidu.com/s/1dFGFCrv" target="_blank" rel="external">http://pan.baidu.com/s/1dFGFCrv</a></p>
<p>mysql5.6.X.tar.gz到mysql-5.7.8-rc.tar.gz是一个版本—-audit5_6_21.so<br>mysql5.7.1.tar.gz—mysql5.7.9.tar.gz是一个版本—-audit5_7_9.so<br>mysql5.7.10–mysql5.7.22是一个版本</p>
<p>一、查找插件所在位置<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">mysql&gt; show variables like &apos;%plugin_dir%&apos;;</div><div class="line">+---------------+------------------------------+</div><div class="line">| Variable_name | Value                        |</div><div class="line">+---------------+------------------------------+</div><div class="line">| plugin_dir    | /usr/local/mysql/lib/plugin/ |</div><div class="line">+---------------+------------------------------+</div><div class="line">1 row in set (0.00 sec)</div><div class="line">---------------------</div></pre></td></tr></table></figure></p>
<p>二、将audit_版本号.so插件下载后放到plugin_dir位置<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">mv audit_版本号.so  audit.so</div></pre></td></tr></table></figure></p>
<p>三、加载插件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">mysql&gt; install plugin audit  SONAME &apos;audit.so&apos;;</div></pre></td></tr></table></figure></p>
<p>四、卸载插件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">mysql&gt; uninstall plugin audit;</div></pre></td></tr></table></figure></p>
<p>使用插件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">mysql&gt; show variables like &apos;%audit%&apos;;</div><div class="line">+----------------+----------------------+</div><div class="line">| Variable_name  | Value                |</div><div class="line">+----------------+----------------------+</div><div class="line">| audit_logfile  | /tmp/mysql_audit.log |</div><div class="line">| audit_myswitch | OFF                  |</div><div class="line">| audit_num      | 0                    |</div><div class="line">| audit_sql      | all_sql              |</div><div class="line">| audit_user     | all_user             |</div><div class="line">+----------------+----------------------+</div><div class="line">5 rows in set (0.01 sec)</div><div class="line"></div><div class="line">mysql&gt; set global audit_logfile=&apos;/tmp/mysql_audit_1.log&apos;;----只读变量，审计仅指定在/tmp/mysql_audit.log文件，保障权限可以写</div><div class="line">ERROR 1238 (HY000): Variable &apos;audit_logfile&apos; is a read only variable</div><div class="line"></div><div class="line">set global audit_sql=&apos;delete;select;drop&apos;;   -----这些审计关键字用;分开</div><div class="line">set global audit_user=&apos;user2;user3&apos;;         ----审计用户用;隔开</div><div class="line">set global audit_num =0;                          ----审计sql影响的最少行数，默认为0</div><div class="line">set global audit_myswitch=on|off|ON|OFF|1|0;       -----开启关闭审计</div></pre></td></tr></table></figure></p>
<p>查看日志linux下tailf /tmp/mysql_audit.log</p>
<p>ref<br><a href="https://blog.csdn.net/heizistudio/article/details/50954294" target="_blank" rel="external">mysql审计插件(运维不在背锅)</a><br><a href="https://dev.mysql.com/doc/refman/5.5/en/writing-audit-plugins.html" target="_blank" rel="external">Writing Audit Plugins</a><br><a href="https://github.com/mcafee/mysql-audit/wiki/Configuration" target="_blank" rel="external">mcafee/mysql-audit</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;此文转载于&lt;a href=&quot;https://blog.csdn.net/heizistudio/article/details/50954294&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;ora600&lt;/a&gt;,略作调整.&lt;/p&gt;
&lt;p&gt;下载地址如下&lt;b
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Spark On YARN</title>
    <link href="https://t1ger.github.io/2018/10/30/Spark-On-YARN/"/>
    <id>https://t1ger.github.io/2018/10/30/Spark-On-YARN/</id>
    <published>2018-10-30T07:27:47.000Z</published>
    <updated>2018-10-30T10:02:56.494Z</updated>
    
    <content type="html"><![CDATA[<h5 id="Deployment-Modes"><a href="#Deployment-Modes" class="headerlink" title="Deployment Modes"></a><b>Deployment Modes</b></h5><p>在YARN中，每个应用实例有一个ApplicationMaster进程，这是应用实例开启的第一个容器。 ResourceManager向ApplicationMaster申请资源，<br>在资源分配后，应用实例会通知NodeManagers去启动容器。ApplicationMasters 评估每个客户端的需要:进程启动的应用可以被中断，持续协调管理进程。</p>
<h5 id="Cluster-Deployment-Mode"><a href="#Cluster-Deployment-Mode" class="headerlink" title="Cluster Deployment Mode"></a><b>Cluster Deployment Mode</b></h5><p>在集群模式，Spark driver 运行在ApplicationMaster集群主机里边，在Yarn容器里的进程负责驱动应用和向YARN请求资源。<br>集群模式不适合交互<br><img src="https://www.cloudera.com/documentation/enterprise/latest/images/xspark-yarn-cluster.png.pagespeed.ic.f4CfMwda2i.webp" alt="cluster mode"></p>
<h5 id="Cluster-Deployment-Mode-1"><a href="#Cluster-Deployment-Mode-1" class="headerlink" title="Cluster Deployment Mode"></a><b>Cluster Deployment Mode</b></h5><p>在客户端模式，Spark driver运行在提交job的主机上,ApplicationMaster响应来自于Yarn容器的请求，在容器启动后，客户端和容器协调完成任务调度<br><img src="https://www.cloudera.com/documentation/enterprise/latest/images/xspark-yarn-client.png.pagespeed.ic.Nm0CUtnR01.webp" alt="client mode"></p>
<h5 id="Configuring-the-Environment"><a href="#Configuring-the-Environment" class="headerlink" title="Configuring the Environment"></a><b>Configuring the Environment</b></h5><p>Spark 需要配置 HADOOP_CONF_DIR or YARN_CONF_DIR 环境变量指向包含客户端目录的配置文件，这些配置文件用于写入HDFS和连接YARN ResourceManager.如果使用Cloudera Manager的部署客户端配置，这些变量会自动配置好.<br>否则在提交job会出现如下错误<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">Exception in thread &quot;main&quot; java.lang.Exception: When running with master &apos;yarn&apos; either HADOOP_CONF_DIR or YARN_CONF_DIR must be set in the environment.</div><div class="line">    at org.apache.spark.deploy.SparkSubmitArguments.validateSubmitArguments(SparkSubmitArguments.scala:251)</div><div class="line">    at org.apache.spark.deploy.SparkSubmitArguments.validateArguments(SparkSubmitArguments.scala:228)</div><div class="line">    at org.apache.spark.deploy.SparkSubmitArguments.&lt;init&gt;(SparkSubmitArguments.scala:109)</div><div class="line">    at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:114)</div></pre></td></tr></table></figure></p>
<h5 id="Running-a-Spark-Shell-Application-on-YARN"><a href="#Running-a-Spark-Shell-Application-on-YARN" class="headerlink" title="Running a Spark Shell Application on YARN"></a><b>Running a Spark Shell Application on YARN</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">#launch a Spark application in cluster mode</div><div class="line">[root@cdh2 admin]# cat job.sh </div><div class="line">sudo -uhdfs  spark-submit --class org.apache.spark.examples.SparkPi \</div><div class="line">    --verbose \</div><div class="line">    --master yarn \</div><div class="line">    --deploy-mode cluster\</div><div class="line">    --num-executors 3 \</div><div class="line">    --driver-memory 2g \</div><div class="line">    --executor-memory 512m \</div><div class="line">    --executor-cores 1 \</div><div class="line">    --queue thequeue \</div><div class="line">    /opt/cloudera/parcels/CDH-6.0.1-1.cdh6.0.1.p0.590678/lib/spark/examples/jars/spark-examples_2.11-2.2.0-cdh6.0.1.jar \</div><div class="line">    10 </div><div class="line"></div><div class="line">#run spark-shell in client mode:	</div><div class="line"> ./bin/spark-shell --master yarn --deploy-mode client</div></pre></td></tr></table></figure>
<h5 id="Spark-On-YARN相关的配置参数"><a href="#Spark-On-YARN相关的配置参数" class="headerlink" title="Spark On YARN相关的配置参数"></a><b>Spark On YARN相关的配置参数</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">spark.driver.memory ：默认值512m</div><div class="line">spark.executor.memory ：默认值512m</div><div class="line">spark.yarn.am.memory ：默认值512m</div><div class="line">spark.yarn.executor.memoryOverhead ：值为 executorMemory * 0.07, with minimum of 384</div><div class="line">spark.yarn.driver.memoryOverhead ：值为 driverMemory * 0.07, with minimum of 384</div><div class="line">spark.yarn.am.memoryOverhead ：值为 AM memory * 0.07, with minimum of 384</div><div class="line"></div><div class="line">#--executor-memory/spark.executor.memory 控制 executor 的堆的大小，但是 JVM 本身也会占用一定的堆空间，比如内部的 String 或者直接 byte buffer， spark.yarn.XXX.memoryOverhead 属性决定向 YARN 请求的每个 executor 或dirver或am 的额外堆内存大小，默认值为 max(384, 0.07 * spark.executor.memory )</div><div class="line">#在 executor 执行的时候配置过大的 memory 经常会导致过长的GC延时，64G是推荐的一个 executor 内存大小的上限。</div><div class="line">#HDFS client 在大量并发线程时存在性能问题。大概的估计是每个 executor 中最多5个并行的 task 就可以占满写入带宽</div></pre></td></tr></table></figure>
<p>YARN中有几个关键参数，参考YARN的内存和CPU配置<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">yarn.app.mapreduce.am.resource.mb ：AM能够申请的最大内存，默认值为1536MB</div><div class="line">yarn.nodemanager.resource.memory-mb ：nodemanager能够申请的最大内存，默认值为8192MB</div><div class="line">yarn.scheduler.minimum-allocation-mb ：调度时一个container能够申请的最小资源，默认值为1024MB</div><div class="line">yarn.scheduler.maximum-allocation-mb ：调度时一个container能够申请的最大资源，默认值为8192MB</div></pre></td></tr></table></figure></p>
<p>设置AM申请的内存值，要么使用cluster模式，要么在client模式中，是有 –conf 手动设置 spark.yarn.am.memory 属性，例如：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">sudo -uhdfs  spark-submit --class org.apache.spark.examples.SparkPi \</div><div class="line">    --verbose \</div><div class="line">    --master yarn \</div><div class="line">    --deploy-mode cluster\</div><div class="line">    --num-executors 3 \</div><div class="line">    --driver-memory 2g \</div><div class="line">    --executor-memory 512m \</div><div class="line">    --executor-cores 1 \</div><div class="line">	--conf spark.yarn.am.memory=1024m \rr</div><div class="line">    --queue thequeue \</div><div class="line">    /opt/cloudera/parcels/CDH-6.0.1-1.cdh6.0.1.p0.590678/lib/spark/examples/jars/spark-examples_2.11-2.2.0-cdh6.0.1.jar \</div><div class="line">    10</div></pre></td></tr></table></figure></p>
<p>ref<br><a href="https://www.cloudera.com/documentation/enterprise/latest/topics/cdh_ig_running_spark_on_yarn.html" target="_blank" rel="external">Running Spark Applications on YARN</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;Deployment-Modes&quot;&gt;&lt;a href=&quot;#Deployment-Modes&quot; class=&quot;headerlink&quot; title=&quot;Deployment Modes&quot;&gt;&lt;/a&gt;&lt;b&gt;Deployment Modes&lt;/b&gt;&lt;/h5&gt;&lt;p&gt;在YARN中，
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>how to cleanup keys without exipiration in Redis</title>
    <link href="https://t1ger.github.io/2018/10/24/how-to-cleanup-keys-without-exipiration-in-Redis/"/>
    <id>https://t1ger.github.io/2018/10/24/how-to-cleanup-keys-without-exipiration-in-Redis/</id>
    <published>2018-10-24T04:02:18.000Z</published>
    <updated>2018-10-24T03:29:42.633Z</updated>
    
    <content type="html"><![CDATA[<p>在排查redis内存一直增长问题的时候，我们首先想到的是如何找出有多少key没有设置失效时间</p>
<p>我们可以通过 info keyspace 命令查看：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">127.0.0.1:6379&gt; info keyspace</div><div class="line"># Keyspace</div><div class="line">db0:keys=473688,expires=1645,avg_ttl=1625894</div></pre></td></tr></table></figure></p>
<p>keys是总共的key, expires是要失效的key,ave_ttl是这些key的平均失效时间，单位是ms</p>
<p>注意，我们不能再生产环境使用keys命令，如果没有开启RDB,需要通过执行bgsave 来获得rdb文件，之后拷贝到测试环境</p>
<p>我们可以使用docker来加载<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">docker run --name redis_dump -d -v `pwd`:/data -p 6379 redis:3.2</div><div class="line"></div><div class="line">docker exec -ti redis_dump bash</div></pre></td></tr></table></figure></p>
<p>现在我们来统计未设置失效时间的key吧<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">redis-cli keys &quot;*&quot; &gt; keys</div><div class="line">cat keys | xargs -n 1 -L 1 redis-cli ttl &gt; ttl</div><div class="line">paste -d &quot; &quot; keys ttl | grep .*-1$ | cut -d &quot; &quot; -f 1 &gt; without_ttl</div><div class="line"></div><div class="line"># We can create a script for deleting the keys </div><div class="line">cat without_ttl | awk &apos;&#123;print &quot;redis-cli del &quot;$1&#125;&apos; &gt; redis.sh</div></pre></td></tr></table></figure></p>
<p>再来一个大红包吧，怎么找出最大的key呢<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line">[root@localhost ~]# redis-cli --bigkeys</div><div class="line"></div><div class="line"># Scanning the entire keyspace to find biggest keys as well as</div><div class="line"># average sizes per key type.  You can use -i 0.1 to sleep 0.1 sec</div><div class="line"># per 100 SCAN commands (not usually needed).</div><div class="line"></div><div class="line">[00.00%] Biggest string found so far &apos;courseinfo_201703827_2018-10-29&apos; with 14189 bytes</div><div class="line">[00.00%] Biggest string found so far &apos;courseinfo_3160205116_2017-09-04&apos; with 16717 bytes</div><div class="line">[00.01%] Biggest string found so far &apos;courseinfo_201707708_2018-12-03&apos; with 20835 bytes</div><div class="line">[00.02%] Biggest string found so far &apos;courseinfo_3170204119_2017-10-02&apos; with 23037 bytes</div><div class="line">[00.07%] Biggest string found so far &apos;courseinfo_3160204209_2017-09-18&apos; with 29201 bytes</div><div class="line">[00.38%] Biggest string found so far &apos;courseinfo_201504330342_2018-05-21&apos; with 138565 bytes</div><div class="line">[00.77%] Biggest string found so far &apos;courseinfo_201606060919_2018-03-26&apos; with 201381 bytes</div><div class="line">[00.98%] Biggest string found so far &apos;courseinfo_201501310518_2018-05-14&apos; with 215428 bytes</div><div class="line">[01.07%] Biggest string found so far &apos;courseinfo_201706061709_2018-06-11&apos; with 223421 bytes</div><div class="line">[04.43%] Biggest string found so far &apos;courseinfo_201706061633_2018-03-05&apos; with 249319 bytes</div><div class="line">[05.33%] Biggest string found so far &apos;courseinfo_201706062917_2018-03-05&apos; with 278392 bytes</div><div class="line">[06.03%] Biggest string found so far &apos;courseinfo_201707030224_2018-03-19&apos; with 281098 bytes</div><div class="line">[19.89%] Biggest string found so far &apos;courseinfo_201706062917_2018-03-19&apos; with 296014 bytes</div><div class="line"></div><div class="line">-------- summary -------</div><div class="line"></div><div class="line">Sampled 472908 keys in the keyspace!</div><div class="line">Total key length in bytes is 17874174 (avg len 37.80)</div><div class="line"></div><div class="line">Biggest string found &apos;courseinfo_201706062917_2018-03-19&apos; has 296014 bytes</div><div class="line"></div><div class="line">472908 strings with 2259279653 bytes (100.00% of keys, avg size 4777.42)</div><div class="line">0 lists with 0 items (00.00% of keys, avg size 0.00)</div><div class="line">0 sets with 0 members (00.00% of keys, avg size 0.00)</div><div class="line">0 hashs with 0 fields (00.00% of keys, avg size 0.00)</div><div class="line">0 zsets with 0 members (00.00% of keys, avg size 0.00)</div></pre></td></tr></table></figure></p>
<p>很快我们就会发现我们的问题了，courseinfo_201706062917_2018-03-19 就是我们要找的</p>
<p>ref<br><a href="https://jmaitrehenry.ca/2017/11/22/found-keys-without-expiration-in-redis/" target="_blank" rel="external">Found and cleanup keys without expiration in Redis</a></p>
<hr>
<p>您的鼓励是我写作最大的动力</p>
<p>俗话说，投资效率是最好的投资。 如果您感觉我的文章质量不错，读后收获很大，预计能为您提高 10% 的工作效率，不妨小额捐助我一下，让我有动力继续写出更多好文章。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在排查redis内存一直增长问题的时候，我们首先想到的是如何找出有多少key没有设置失效时间&lt;/p&gt;
&lt;p&gt;我们可以通过 info keyspace 命令查看：&lt;br&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>How to install CDH6.0 Cluster on Centos7.5</title>
    <link href="https://t1ger.github.io/2018/09/03/How-to-install-CDH6-0-Cluster-on-Centos7-5/"/>
    <id>https://t1ger.github.io/2018/09/03/How-to-install-CDH6-0-Cluster-on-Centos7-5/</id>
    <published>2018-09-03T02:21:26.000Z</published>
    <updated>2018-09-03T10:22:33.977Z</updated>
    
    <content type="html"><![CDATA[<h5 id="Cloudera-简介"><a href="#Cloudera-简介" class="headerlink" title="Cloudera 简介"></a><b>Cloudera 简介</b></h5><ul>
<li>Cloudera 官网：<a href="https://www.cloudera.com" target="_blank" rel="external">https://www.cloudera.com</a></li>
<li>Cloudera 官方文档： <a href="https://www.cloudera.com/documentation/enterprise/latest.html" target="_blank" rel="external">https://www.cloudera.com/documentation/enterprise/latest.html</a></li>
</ul>
<h5 id="安装Cloudera-Manager和CDH"><a href="#安装Cloudera-Manager和CDH" class="headerlink" title="安装Cloudera Manager和CDH"></a><b>安装Cloudera Manager和CDH</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">系统环境：CentOS7.5 </div><div class="line">软件环境：Oracle JDK、Cloudera Manager Server 和 Agent 、数据库、CDH各组件</div></pre></td></tr></table></figure>
<ul>
<li><p>系统初始化<br>关闭防火墙 禁用selinux,服务器之间免密，时间保持同步</p>
</li>
<li><p>Cloudera安装,官方文档参考<a href="https://www.cloudera.com/documentation/enterprise/6/6.0/topics/install_cm_cdh.html" target="_blank" rel="external">这里</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line">#Configure a Repository</div><div class="line">wget https://archive.cloudera.com/cm6/6.0.0/redhat7/yum/cloudera-manager.repo -P /etc/yum.repos.d/</div><div class="line">sudo rpm --import https://archive.cloudera.com/cm6/6.0.0/redhat7/yum/RPM-GPG-KEY-cloudera</div><div class="line"></div><div class="line">#Installing the JDK</div><div class="line">sudo yum install oracle-j2sdk1.8</div><div class="line"></div><div class="line">#Install Cloudera Manager Packages</div><div class="line">sudo yum install cloudera-manager-daemons cloudera-manager-agent cloudera-manager-server</div><div class="line"></div><div class="line">#Install Databases</div><div class="line">wget http://repo.mysql.com/mysql-community-release-el7-5.noarch.rpm</div><div class="line">sudo rpm -ivh mysql-community-release-el7-5.noarch.rpm</div><div class="line">sudo yum update</div><div class="line">sudo yum install mysql-server</div><div class="line">sudo systemctl start mysqld</div><div class="line"></div><div class="line">#Set up the Cloudera Manager Database</div><div class="line">1. /opt/cloudera/cm/schema/scm_prepare_database.sh \</div><div class="line">[options] &lt;databaseType&gt; &lt;databaseName&gt; &lt;databaseUser&gt; &lt;password&gt;</div><div class="line">2. If it exists, remove the embedded PostgreSQL properties file:</div><div class="line">sudo rm /etc/cloudera-scm-server/db.mgmt.properties</div><div class="line"></div><div class="line">Example：</div><div class="line">sudo /opt/cloudera/cm/schema/scm_prepare_database.sh mysql scm scm</div><div class="line"></div><div class="line">#Install CDH and Other Software</div><div class="line">sudo systemctl start cloudera-scm-server</div><div class="line"></div><div class="line">we go to browser http://&lt;server_host&gt;:7180,login admin/admin</div></pre></td></tr></table></figure>
</li>
<li><p>创建必需的数据库，可以参考<a href="https://www.cloudera.com/documentation/enterprise/latest/topics/install_cm_mariadb.html" target="_blank" rel="external">这里</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">create database metastore DEFAULT CHARACTER SET utf8;</div><div class="line">grant all on metastore.* TO &apos;hive&apos;@&apos;%&apos; IDENTIFIED BY &apos;hive&apos;;</div><div class="line"></div><div class="line">create database amon DEFAULT CHARACTER SET utf8;</div><div class="line">grant all on amon.* TO &apos;amon&apos;@&apos;%&apos; IDENTIFIED BY &apos;amon&apos;;</div><div class="line"></div><div class="line">create database hue DEFAULT CHARACTER SET utf8;</div><div class="line">grant all on hue.* TO &apos;hue&apos;@&apos;%&apos; IDENTIFIED BY &apos;hue&apos;;</div><div class="line"></div><div class="line">create database rman DEFAULT CHARACTER SET utf8;</div><div class="line">grant all on rman.* TO &apos;rman&apos;@&apos;%&apos; IDENTIFIED BY &apos;rman&apos;;</div><div class="line"></div><div class="line">create database navms DEFAULT CHARACTER SET utf8;</div><div class="line">grant all on navms.* TO &apos;navms&apos;@&apos;%&apos; IDENTIFIED BY &apos;navms&apos;;</div><div class="line"></div><div class="line">create database nas DEFAULT CHARACTER SET utf8;</div><div class="line">grant all on nas.* TO &apos;nas&apos;@&apos;%&apos; IDENTIFIED BY &apos;nas&apos;;</div><div class="line"></div><div class="line">create database oos DEFAULT CHARACTER SET utf8;</div><div class="line">grant all on oos.* TO &apos;oos&apos;@&apos;%&apos; IDENTIFIED BY &apos;oos&apos;;</div></pre></td></tr></table></figure>
</li>
</ul>
<h5 id="卸载重装CM服务"><a href="#卸载重装CM服务" class="headerlink" title="卸载重装CM服务"></a><b>卸载重装CM服务</b></h5><p>如果，第一次没有安装成功，那这部分就对你有帮助了<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"># 安装CDH manager的服务器上面执行</div><div class="line">yum remove cloudera-manager-server -y</div><div class="line"></div><div class="line"># 在所有的服务器执行下面操作</div><div class="line">systemctl stop  cloudera-scm-agent</div><div class="line">yum remove cloudera-manager-agennt-y</div><div class="line">ps -ef | grep cmf | grep -v grep | awk &apos;&#123;print $2&#125;&apos; | xargs kill -9</div><div class="line">find / -name clouder* | xargs rm -rf </div><div class="line">find / -name cmf* | xargs rm -rf</div></pre></td></tr></table></figure></p>
<h5 id="Custom-Installation-Solutions"><a href="#Custom-Installation-Solutions" class="headerlink" title="Custom Installation Solutions"></a><b>Custom Installation Solutions</b></h5><p>如果在线安装很慢，我们可以通过以下方式来加速安装</p>
<ul>
<li><p>Creating a Permanent Internal Repository(Option)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">#Setting Up a Web Server</div><div class="line">sudo yum install httpd</div><div class="line">sudo systemctl start httpd</div><div class="line"></div><div class="line">#Edit the Apache HTTP Server configuration file (/etc/httpd/conf/httpd.conf by default) to add or edit the following line in the &lt;IfModule mime_module&gt; section:</div><div class="line">AddType application/x-gzip .gz .tgz .parcel</div><div class="line"></div><div class="line">#Downloading and Publishing the Package Repository</div><div class="line"></div><div class="line">#Cloudera Manager 6</div><div class="line">sudo mkdir -p /var/www/html/cloudera-repos</div><div class="line">sudo wget --recursive --no-parent --no-host-directories https://archive.cloudera.com/cm6/6.0.0/redhat7/ -P /var/www/html/cloudera-repos</div><div class="line">sudo chmod -R ugo+rX /var/www/html/cloudera-repos/cm6</div><div class="line"></div><div class="line">#CDH 6</div><div class="line">sudo mkdir -p /var/www/html/cloudera-repos</div><div class="line">sudo wget --recursive --no-parent --no-host-directories https://archive.cloudera.com/cdh6/6.0.0/redhat7/ -P /var/www/html/cloudera-repos</div><div class="line"></div><div class="line">sudo wget --recursive --no-parent --no-host-directories https://archive.cloudera.com/gplextras6/6.0.0/redhat7/ -P /var/www/html/cloudera-repos</div><div class="line"></div><div class="line">sudo chmod -R ugo+rX /var/www/html/cloudera-repos/cdh6</div><div class="line">sudo chmod -R ugo+rX /var/www/html/cloudera-repos/gplextras6</div></pre></td></tr></table></figure>
</li>
<li><p>Creating a Temporary Internal Repository(Option)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"># Download the repository you need following the instructions in Downloading and Publishing the Package Repository.</div><div class="line">cd /var/www/html</div><div class="line">python -m SimpleHTTPServer 8900</div><div class="line"></div><div class="line">#Visit the Repository URL http://&lt;web_server&gt;:8900/cloudera-repos/</div></pre></td></tr></table></figure>
</li>
<li><p>Configuring Hosts to Use the Internal Repository</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"># Create /etc/yum.repos.d/cloudera-repo.repo files on cluster hosts with the following content, </div><div class="line">where &lt;web_server&gt; is the hostname of the web server:</div><div class="line"></div><div class="line">[cloudera-repo]</div><div class="line">name=cloudera-repo</div><div class="line">baseurl=http://&lt;web_server&gt;/cm/5</div><div class="line">enabled=1</div><div class="line">gpgcheck=0</div></pre></td></tr></table></figure>
</li>
</ul>
<h5 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a><b>遇到的问题</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"># jdbc驱动未找到</div><div class="line">mkdir -p /usr/share/java</div><div class="line">rz mysql-connector-java-5.1.45-bin.jar</div><div class="line">ln -s mysql-connector-java-5.1.45-bin.jar mysql-connector-java.jar</div><div class="line">systemctl restart cloudera-scm-server.service</div><div class="line"></div><div class="line"># host命令为找到</div><div class="line">yum install bind-utils -y</div></pre></td></tr></table></figure>
<p>ref<br><a href="https://gist.github.com/lilongen/b179b3868d2c2839ca7303b7605ce16b" target="_blank" rel="external">deploy-cm-cdh-on-centos7</a><br><a href="https://www.cloudera.com/documentation/enterprise/6/6.0/topics/cm_ig_create_local_package_repo.html#download_publish_package_repo" target="_blank" rel="external">Custom Installation Solutions</a><br><a href="https://www.cloudera.com/documentation/enterprise/6/latest/topics/cm_ig_create_local_parcel_repo.html#" target="_blank" rel="external">Using an Internally Hosted Remote Parcel Repository</a></p>
<hr>
<p>您的鼓励是我写作最大的动力</p>
<p>俗话说，投资效率是最好的投资。 如果您感觉我的文章质量不错，读后收获很大，预计能为您提高 10% 的工作效率，不妨小额捐助我一下，让我有动力继续写出更多好文章。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;Cloudera-简介&quot;&gt;&lt;a href=&quot;#Cloudera-简介&quot; class=&quot;headerlink&quot; title=&quot;Cloudera 简介&quot;&gt;&lt;/a&gt;&lt;b&gt;Cloudera 简介&lt;/b&gt;&lt;/h5&gt;&lt;ul&gt;
&lt;li&gt;Cloudera 官网：&lt;a href=&quot;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Elasticsearch6.3的新特性</title>
    <link href="https://t1ger.github.io/2018/06/15/Elasticsearch6-3%E7%9A%84%E6%96%B0%E7%89%B9%E6%80%A7/"/>
    <id>https://t1ger.github.io/2018/06/15/Elasticsearch6-3的新特性/</id>
    <published>2018-06-15T03:02:17.000Z</published>
    <updated>2018-06-20T06:57:05.131Z</updated>
    
    <content type="html"><![CDATA[<p>Elasticsearch 官方推出了6.3.0，今天，让我们看一下 Elasticsearch6.3.0给我们带来的新特性吧，如果想看官网的同学可以参考<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/release-notes-6.3.0.html" target="_blank" rel="external">这里</a></p>
<ul>
<li>Elasticsearch6.3默认包含了X-Pack,X-Pack包括APM,Canvas</li>
<li>支持SQL </li>
<li>支持Java 10</li>
<li>汇总统计</li>
<li>安全更新</li>
</ul>
<h5 id="支持SQL"><a href="#支持SQL" class="headerlink" title="支持SQL"></a><b>支持SQL</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div></pre></td><td class="code"><pre><div class="line">#创建index并添加数据</div><div class="line">[root@localhost ~]# curl -X PUT &quot;localhost:9200/library/book/_bulk?refresh&quot; -H &apos;Content-Type: application/json&apos; -d&apos;</div><div class="line">&gt; &#123;&quot;index&quot;:&#123;&quot;_id&quot;: &quot;Leviathan Wakes&quot;&#125;&#125;</div><div class="line">&gt; &#123;&quot;name&quot;: &quot;Leviathan Wakes&quot;, &quot;author&quot;: &quot;James S.A. Corey&quot;, &quot;release_date&quot;: &quot;2011-06-02&quot;, &quot;page_count&quot;: 561&#125;</div><div class="line">&gt; &#123;&quot;index&quot;:&#123;&quot;_id&quot;: &quot;Hyperion&quot;&#125;&#125;</div><div class="line">&gt; &#123;&quot;name&quot;: &quot;Hyperion&quot;, &quot;author&quot;: &quot;Dan Simmons&quot;, &quot;release_date&quot;: &quot;1989-05-26&quot;, &quot;page_count&quot;: 482&#125;</div><div class="line">&gt; &#123;&quot;index&quot;:&#123;&quot;_id&quot;: &quot;Dune&quot;&#125;&#125;</div><div class="line">&gt; &#123;&quot;name&quot;: &quot;Dune&quot;, &quot;author&quot;: &quot;Frank Herbert&quot;, &quot;release_date&quot;: &quot;1965-06-01&quot;, &quot;page_count&quot;: 604&#125;</div><div class="line">&gt; &apos;</div><div class="line">&#123;&quot;took&quot;:426,&quot;errors&quot;:false,&quot;items&quot;:[&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;library&quot;,&quot;_type&quot;:&quot;book&quot;,&quot;_id&quot;:&quot;Leviathan Wakes&quot;,&quot;_version&quot;:1,&quot;result&quot;:&quot;created&quot;,&quot;forced_refresh&quot;:true,&quot;_shards&quot;:&#123;&quot;total&quot;:2,&quot;successful&quot;:1,&quot;failed&quot;:0&#125;,&quot;_seq_no&quot;:0,&quot;_primary_term&quot;:1,&quot;status&quot;:201&#125;&#125;,&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;library&quot;,&quot;_type&quot;:&quot;book&quot;,&quot;_id&quot;:&quot;Hyperion&quot;,&quot;_version&quot;:1,&quot;result&quot;:&quot;created&quot;,&quot;forced_refresh&quot;:true,&quot;_shards&quot;:&#123;&quot;total&quot;:2,&quot;successful&quot;:1,&quot;failed&quot;:0&#125;,&quot;_seq_no&quot;:0,&quot;_primary_term&quot;:1,&quot;status&quot;:201&#125;&#125;,&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;library&quot;,&quot;_type&quot;:&quot;book&quot;,&quot;_id&quot;:&quot;Dune&quot;,&quot;_version&quot;:1,&quot;result&quot;:&quot;created&quot;,&quot;forced_refresh&quot;:true,&quot;_shards&quot;:&#123;&quot;total&quot;:2,&quot;successful&quot;:1,&quot;failed&quot;:0&#125;,&quot;_seq_no&quot;:1,&quot;_primary_term&quot;:1,&quot;status&quot;:201&#125;&#125;]&#125;</div><div class="line"></div><div class="line">#通过SQL REST API执行SQL</div><div class="line">[root@localhost ~]# curl -X POST &quot;localhost:9200/_xpack/sql?format=txt&quot; -H &apos;Content-Type: application/json&apos; -d&apos;</div><div class="line">&gt; &#123;</div><div class="line">&gt;     &quot;query&quot;: &quot;SELECT * FROM library WHERE release_date &lt; \u00272000-01-01\u0027&quot;</div><div class="line">&gt; &#125;</div><div class="line">&gt; &apos;</div><div class="line">    author     |     name      |  page_count   |      release_date      </div><div class="line">---------------+---------------+---------------+------------------------</div><div class="line">Dan Simmons    |Hyperion       |482            |1989-05-26T00:00:00.000Z</div><div class="line">Frank Herbert  |Dune           |604            |1965-06-01T00:00:00.000Z</div><div class="line"></div><div class="line"></div><div class="line">#支持SQL REST API</div><div class="line">[root@localhost ~]# curl -X POST &quot;localhost:9200/_xpack/sql/translate&quot; -H &apos;Content-Type: application/json&apos; -d&apos;</div><div class="line">&gt; &#123;</div><div class="line">&gt;     &quot;query&quot;: &quot;SELECT * FROM library ORDER BY page_count DESC&quot;,</div><div class="line">&gt;     &quot;fetch_size&quot;: 10</div><div class="line">&gt; &#125;</div><div class="line">&gt; &apos;</div><div class="line">&#123;&quot;size&quot;:10,&quot;_source&quot;:&#123;&quot;includes&quot;:[&quot;author&quot;,&quot;name&quot;],&quot;excludes&quot;:[]&#125;,&quot;docvalue_fields&quot;:[&quot;page_count&quot;,&quot;release_date&quot;],&quot;sort&quot;:[&#123;&quot;page_count&quot;:&#123;&quot;order&quot;:&quot;desc&quot;&#125;&#125;]&#125;</div><div class="line"></div><div class="line">#支持SQL CLI</div><div class="line">[root@localhost ~]# /usr/share/elasticsearch/bin/elasticsearch-sql-cli</div><div class="line">     .sssssss.`                     .sssssss.</div><div class="line">  .:sXXXXXXXXXXo`                `ohXXXXXXXXXho.</div><div class="line"> .yXXXXXXXXXXXXXXo`            `oXXXXXXXXXXXXXXX-</div><div class="line">.XXXXXXXXXXXXXXXXXXo`        `oXXXXXXXXXXXXXXXXXX.</div><div class="line">.XXXXXXXXXXXXXXXXXXXXo.    .oXXXXXXXXXXXXXXXXXXXXh</div><div class="line">.XXXXXXXXXXXXXXXXXXXXXXo``oXXXXXXXXXXXXXXXXXXXXXXy</div><div class="line">`yXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX.</div><div class="line"> `oXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXo`</div><div class="line">   `oXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXo`</div><div class="line">     `oXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXo`</div><div class="line">       `oXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXo`</div><div class="line">         `oXXXXXXXXXXXXXXXXXXXXXXXXXXXXo`</div><div class="line">           .XXXXXXXXXXXXXXXXXXXXXXXXXo`</div><div class="line">         .oXXXXXXXXXXXXXXXXXXXXXXXXo`</div><div class="line">       `oXXXXXXXXXXXXXXXXXXXXXXXXo`   `odo`</div><div class="line">     `oXXXXXXXXXXXXXXXXXXXXXXXXo`   `oXXXXXo`</div><div class="line">   `oXXXXXXXXXXXXXXXXXXXXXXXXo`   `oXXXXXXXXXo`</div><div class="line"> `oXXXXXXXXXXXXXXXXXXXXXXXXo`   `oXXXXXXXXXXXXXo`</div><div class="line">`yXXXXXXXXXXXXXXXXXXXXXXXo`    oXXXXXXXXXXXXXXXXX.</div><div class="line">.XXXXXXXXXXXXXXXXXXXXXXo`   `oXXXXXXXXXXXXXXXXXXXy</div><div class="line">.XXXXXXXXXXXXXXXXXXXXo`     /XXXXXXXXXXXXXXXXXXXXX</div><div class="line">.XXXXXXXXXXXXXXXXXXo`        `oXXXXXXXXXXXXXXXXXX-</div><div class="line"> -XXXXXXXXXXXXXXXo`            `oXXXXXXXXXXXXXXXo`</div><div class="line">  .oXXXXXXXXXXXo`                `oXXXXXXXXXXXo.</div><div class="line">    `.sshXXyso`        SQL         `.sshXhss.`</div><div class="line"></div><div class="line">sql&gt; SELECT * FROM library WHERE release_date &lt; &apos;2000-01-01&apos;;</div><div class="line">    author     |     name      |  page_count   |      release_date      </div><div class="line">---------------+---------------+---------------+------------------------</div><div class="line">Dan Simmons    |Hyperion       |482            |1989-05-26T00:00:00.000Z</div><div class="line">Frank Herbert  |Dune           |604            |1965-06-01T00:00:00.000Z</div><div class="line"></div><div class="line">#支持SQL JDBC</div><div class="line"></div><div class="line">String address = &quot;jdbc:es://&quot; + elasticsearchAddress;     </div><div class="line">Properties connectionProperties = connectionProperties(); </div><div class="line">Connection connection = DriverManager.getConnection(address, connectionProperties);</div><div class="line">try (Statement statement = connection.createStatement();</div><div class="line">        ResultSet results = statement.executeQuery(</div><div class="line">            &quot;SELECT name, page_count FROM library ORDER BY page_count DESC LIMIT 1&quot;)) &#123;</div><div class="line">    assertTrue(results.next());</div><div class="line">    assertEquals(&quot;Don Quixote&quot;, results.getString(1));</div><div class="line">    assertEquals(1072, results.getInt(2));</div><div class="line">    SQLException e = expectThrows(SQLException.class, () -&gt; results.getInt(1));</div><div class="line">    assertTrue(e.getMessage(), e.getMessage().contains(&quot;unable to convert column 1 to an int&quot;));</div><div class="line">    assertFalse(results.next());</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h5 id="支持Java-10"><a href="#支持Java-10" class="headerlink" title="支持Java 10"></a><b>支持Java 10</b></h5><p>Elasticsearch 支持Java9 和Java10，保持和Java快速发布周期一致。但是官方建议大多数用户使用java8，有兴趣的同学可以看<a href="https://jaxenter.com/no-more-public-updates-java-8-143703.html" target="_blank" rel="external">这里</a></p>
<h5 id="汇总统计"><a href="#汇总统计" class="headerlink" title="汇总统计"></a><b>汇总统计</b></h5><p>用户可以建立汇总统计job，job会汇聚统计最近搜索的更新数据。这个功能和SQl功能一样，都是实验性质的功能。</p>
<h5 id="安全更新"><a href="#安全更新" class="headerlink" title="安全更新"></a><b>安全更新</b></h5><p>XPackExtension  扩展机制被移除了，引入SPI扩展机制。</p>
<h5 id="Bug-fixes"><a href="#Bug-fixes" class="headerlink" title="Bug fixes"></a><b>Bug fixes</b></h5><p>具体可以参考<a href="https://www.elastic.co/guide/en/logstash/6.3/logstash-6-3-0.html" target="_blank" rel="external">这里</a></p>
<p>说了这么多，也许有朋友问了，怎么才能升级到Elasticsearch6.3呢，官方建议是：、<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">1. 6.x到6.y - 可以通过一次升级一个节点来执行</div><div class="line">2. 5.x至6.x - 需要完全重启群集</div><div class="line">3. 2.x至6.x - 不支持</div><div class="line"></div><div class="line">[root@localhost ~]# curl -XGET &apos;http://localhost:9200/&apos;</div><div class="line">&#123;</div><div class="line">  &quot;name&quot; : &quot;99NPxaU&quot;,</div><div class="line">  &quot;cluster_name&quot; : &quot;elasticsearch&quot;,</div><div class="line">  &quot;cluster_uuid&quot; : &quot;gW7bp0I3RNKvZI50SAsjeg&quot;,</div><div class="line">  &quot;version&quot; : &#123;</div><div class="line">    &quot;number&quot; : &quot;6.3.0&quot;,</div><div class="line">    &quot;build_flavor&quot; : &quot;default&quot;,</div><div class="line">    &quot;build_type&quot; : &quot;rpm&quot;,</div><div class="line">    &quot;build_hash&quot; : &quot;424e937&quot;,</div><div class="line">    &quot;build_date&quot; : &quot;2018-06-11T23:38:03.357887Z&quot;,</div><div class="line">    &quot;build_snapshot&quot; : false,</div><div class="line">    &quot;lucene_version&quot; : &quot;7.3.1&quot;,</div><div class="line">    &quot;minimum_wire_compatibility_version&quot; : &quot;5.6.0&quot;,</div><div class="line">    &quot;minimum_index_compatibility_version&quot; : &quot;5.0.0&quot;</div><div class="line">  &#125;,</div><div class="line">  &quot;tagline&quot; : &quot;You Know, for Search&quot;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<hr>
<p>您的鼓励是我写作最大的动力</p>
<p>俗话说，投资效率是最好的投资。 如果您感觉我的文章质量不错，读后收获很大，预计能为您提高 10% 的工作效率，不妨小额捐助我一下，让我有动力继续写出更多好文章。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Elasticsearch 官方推出了6.3.0，今天，让我们看一下 Elasticsearch6.3.0给我们带来的新特性吧，如果想看官网的同学可以参考&lt;a href=&quot;https://www.elastic.co/guide/en/elasticsearch/refer
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>how to use sqoop in hadoop</title>
    <link href="https://t1ger.github.io/2018/06/11/how-to-use-sqoop-in-hadoop/"/>
    <id>https://t1ger.github.io/2018/06/11/how-to-use-sqoop-in-hadoop/</id>
    <published>2018-06-11T03:27:36.000Z</published>
    <updated>2018-06-12T10:12:39.676Z</updated>
    
    <content type="html"><![CDATA[<h5 id="install"><a href="#install" class="headerlink" title="install"></a><b>install</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">#download Sqoop 1.4.7 version</div><div class="line">[root@localhost ~]# wget https://mirrors.tuna.tsinghua.edu.cn/apache/sqoop/1.4.7/sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz</div><div class="line">[root@localhost ~]# tar zxf sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz</div><div class="line">[root@localhost ~]# wget https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-8.0.11.tar.gz</div><div class="line">#after untar file, move jar package to Sqoop lib directory </div><div class="line">[root@localhost ~]# cd /usr/local/sqoop/conf &amp;&amp; cp sqoop-env-template.sh sqoop-env.sh</div><div class="line">[root@localhost ~]# cat sqoop-env.sh</div><div class="line">#Set path to where bin/hadoop is available</div><div class="line">export HADOOP_COMMON_HOME=/usr/local/hadoop</div><div class="line">#Set path to where hadoop-*-core.jar is available</div><div class="line">export HADOOP_MAPRED_HOME=/usr/local/hadoop/share/hadoop/mapreduce</div><div class="line">#set the path to where bin/hbase is available</div><div class="line">#export HBASE_HOME=</div><div class="line">#Set the path to where bin/hive is available</div><div class="line">#export HIVE_HOME=</div><div class="line">#Set the path for where zookeper config dir is</div><div class="line">#export ZOOCFGDIR=</div><div class="line"></div><div class="line">#test </div><div class="line"> /usr/local/sqoop/bin/sqoop list-databases \</div><div class="line">  --connect jdbc:mysql://&lt;dburi&gt;/&lt;dbname&gt;  \</div><div class="line">  --username &lt;username&gt; --password &lt;password&gt; </div><div class="line"> /usr/local/sqoop/bin/sqoop list-tables  </div><div class="line"> --connect jdbc:mysql://&lt;dburi&gt;/&lt;dbname&gt; \</div><div class="line"> --username &lt;username&gt; --password &lt;password&gt;</div></pre></td></tr></table></figure>
<h5 id="application-scenarios"><a href="#application-scenarios" class="headerlink" title="application scenarios"></a><b>application scenarios</b></h5><p>tips: before you use command,make sure to su hadoop</p>
<ol>
<li>mysql -&gt; hdfs</li>
<li>hdfs  -&gt; mysql</li>
<li>mysql -&gt; hive</li>
<li>hive  -&gt; mysql</li>
<li>use sql as import condition</li>
</ol>
<ul>
<li><p>from mysql to hdfs<br>–check-column (col): Specifies the column to be examined when determining which rows to import. (the column should not be of type CHAR/NCHAR/VARCHAR/VARNCHAR/ LONGVARCHAR/LONGNVARCHAR)<br>–incremental (mode): Specifies how Sqoop determines which rows are new. Legal values for mode include append and lastmodified.<br>–last-value (value): Specifies the maximum value of the check column from the previous import</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div></pre></td><td class="code"><pre><div class="line">sqoop import --connect jdbc:mysql://&lt;dburi&gt;/&lt;dbname&gt; \</div><div class="line">--username &lt;username&gt; --password &lt;password&gt; \</div><div class="line">--table &lt;tablename&gt; --check-column &lt;col&gt; --incremental &lt;mode&gt; --last-value &lt;value&gt; --target-dir &lt;hdfs-dir&gt;</div><div class="line"></div><div class="line">/usr/local/sqoop/bin/sqoop import \</div><div class="line">--connect jdbc:mysql://192.168.100.112/datbase?zeroDateTimeBehavior=CONVERT_TO_NULL \</div><div class="line">--username username --password password \</div><div class="line">--table pv_daxue \</div><div class="line">--target-dir /usr/sqoop/daxue</div><div class="line"></div><div class="line">#save as parquet(textfile,orcfile,parquet)</div><div class="line">/usr/local/sqoop/bin/sqoop import \</div><div class="line">--connect jdbc:mysql://192.168.100.112/datbase?zeroDateTimeBehavior=CONVERT_TO_NULL \</div><div class="line">--username username --password password \</div><div class="line">--table pv_daxue  --target-dir /usr/sqoop/daxue \</div><div class="line">--as-parquetfile</div><div class="line"></div><div class="line">#save columns id,account </div><div class="line">/usr/local/sqoop/bin/sqoop import \</div><div class="line">--connect jdbc:mysql://192.168.100.112/datbase?zeroDateTimeBehavior=CONVERT_TO_NULL \</div><div class="line">--username username --password password \</div><div class="line">--table pv_daxue  --target-dir /usr/sqoop/daxue \</div><div class="line">--columns id,account \</div><div class="line">--as-textfile</div><div class="line"></div><div class="line">tips: Parameters --as-sequencefile --as-avrodatafile and --as-parquetfile are not supported with --direct params in MySQL case. </div><div class="line"># after insert one record ,append import again</div><div class="line"># Append mode</div><div class="line">/usr/local/sqoop/bin/sqoop import \</div><div class="line">--connect jdbc:mysql://192.168.100.112/datbase?zeroDateTimeBehavior=CONVERT_TO_NULL \</div><div class="line">--username username --password password \</div><div class="line">--table pv_daxue --check-column id \</div><div class="line">--incremental append --target-dir /usr/sqoop/daxue \</div><div class="line">-last-value 5</div><div class="line"></div><div class="line"># Lastmodified mode</div><div class="line">#first import</div><div class="line">/usr/local/sqoop/bin/sqoop import \</div><div class="line">--connect jdbc:mysql://192.168.100.112/datbase?zeroDateTimeBehavior=CONVERT_TO_NULL \</div><div class="line">--username username --password password \</div><div class="line">--table pv_daxue --target-dir /usr/sqoop/daxue -m1</div><div class="line"></div><div class="line">bash-4.1$ hadoop fs -cat /usr/sqoop/daxue/part-m-00000</div><div class="line">1,hello,2018-06-12 23:48:32.0</div><div class="line">2,word,2018-06-12 23:48:32.0</div><div class="line">3,marry,2018-06-12 23:48:32.0</div><div class="line">4,tony,2018-06-12 23:48:32.0</div><div class="line">5,jack,2018-06-12 23:48:33.0</div><div class="line">6,james,2018-06-12 23:52:03.0</div><div class="line">#after insert one record , Lastmodified  import again</div><div class="line">/usr/local/sqoop/bin/sqoop import \</div><div class="line">--connect jdbc:mysql://192.168.100.112/datbase?zeroDateTimeBehavior=CONVERT_TO_NULL \</div><div class="line">--username username --password password \</div><div class="line">--table pv_daxue --check-column last_mod  --incremental lastmodified --last-value &quot;2018-06-12 10:52:03&quot; \</div><div class="line">--target-dir /usr/sqoop/daxue -m 1 --append </div><div class="line"></div><div class="line">18/06/12 10:59:48 INFO mapreduce.ImportJobBase: Transferred 60 bytes in 4.2309 seconds (14.1813 bytes/sec)</div><div class="line">18/06/12 10:59:48 INFO mapreduce.ImportJobBase: Retrieved 2 records</div><div class="line">bash-4.1$ hadoop fs -cat /usr/sqoop/daxue/part-m-00001</div><div class="line">6,james,2018-06-12 23:52:03.0</div><div class="line">7,hello,2018-06-12 23:58:08.0</div><div class="line"></div><div class="line">#merage by mode, after execute sql &quot;update customertest set name = &apos;Hello&apos; where id = 1;&quot;</div><div class="line">/usr/local/sqoop/bin/sqoop import \</div><div class="line">--connect jdbc:mysql://192.168.100.112/datbase?zeroDateTimeBehavior=CONVERT_TO_NULL \</div><div class="line">--username username --password password \</div><div class="line">--table pv_daxue  --check-column last_mod  --incremental lastmodified --last-value &quot;2018-06-12 23:52:03&quot; \</div><div class="line">--target-dir /usr/sqoop/daxue -m 1 --merge-key id </div><div class="line"></div><div class="line">bash-4.1$ hadoop fs -cat /usr/sqoop/daxue/part-r-00000</div><div class="line">1,Hello,2018-06-13 00:07:41.0</div><div class="line">2,word,2018-06-12 23:48:32.0</div><div class="line">3,marry,2018-06-12 23:48:32.0</div><div class="line">4,tony,2018-06-12 23:48:32.0</div><div class="line">5,jack,2018-06-12 23:48:33.0</div><div class="line">6,james,2018-06-12 23:52:03.0</div><div class="line">7,me,2018-06-12 23:58:08.0</div></pre></td></tr></table></figure>
</li>
<li><p>from hdfs to mysql<br>According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn’t set</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">sqoop export --connect jdbc:mysql://&lt;dburi&gt;/&lt;dbname&gt; \</div><div class="line">--username &lt;username&gt; --password &lt;password&gt; \</div><div class="line">--table &lt;tablename&gt; --export-dir &lt;hdfs-dir&gt;</div><div class="line"></div><div class="line">/usr/local/sqoop/bin/sqoop export  \</div><div class="line">--connect &quot;jdbc:mysql://192.168.100.112/datbase?useSSL=false&amp;useUnicode=true&amp;characterEncoding=utf-8&quot; \</div><div class="line">--username username --password password \</div><div class="line">--table pv_daxue  --export-dir /usr/sqoop/daxue</div></pre></td></tr></table></figure>
</li>
<li><p>from mysql to hive</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line">sqoop import --connect jdbc:mysql://&lt;dburi&gt;/&lt;dbname&gt; \</div><div class="line">--username &lt;username&gt; --password &lt;password&gt; \</div><div class="line">--table &lt;tablename&gt; --check-column &lt;col&gt; --incremental &lt;mode&gt; --last-value &lt;value&gt; \</div><div class="line">--fields-terminated-by &quot;\t&quot; --lines-terminated-by &quot;\n&quot; \</div><div class="line">--hive-import --target-dir &lt;hdfs-dir&gt; --hive-table &lt;hive-tablename&gt;</div><div class="line"></div><div class="line">/usr/local/sqoop/bin/sqoop import \</div><div class="line">--connect &quot;jdbc:mysql://192.168.100.112/datbase?useSSL=false&amp;useUnicode=true&amp;characterEncoding=utf-8&quot; \</div><div class="line">--username username --password password \</div><div class="line">–-table hive_table  -–hive-import  --hive-database database –-hive-table hive_test or -–create-hive-table hive_test \  --delete-target-dir  --split-by id</div><div class="line"></div><div class="line"></div><div class="line"># --map-column-hive </div><div class="line">MySQL(bigint) --&gt; Hive(bigint) </div><div class="line">MySQL(tinyint) --&gt; Hive(tinyint) </div><div class="line">MySQL(int) --&gt; Hive(int) </div><div class="line">MySQL(double) --&gt; Hive(double) </div><div class="line">MySQL(bit) --&gt; Hive(boolean) </div><div class="line">MySQL(varchar) --&gt; Hive(string) </div><div class="line">MySQL(decimal) --&gt; Hive(double) </div><div class="line">MySQL(date/timestamp) --&gt; Hive(string)</div><div class="line"></div><div class="line"></div><div class="line">/usr/local/sqoop/bin/sqoop import \</div><div class="line">--connect &quot;jdbc:mysql://192.168.100.112/datbase?useSSL=false&amp;useUnicode=true&amp;characterEncoding=utf-8&quot; \</div><div class="line">--username username --password password \</div><div class="line">–-table hive_table  -–hive-import  \</div><div class="line">--map-column-hive cost=&quot;DECIMAL&quot;,date=&quot;DATE&quot; \ </div><div class="line">--hive-database database –-hive-table hive_test or -–create-hive-table hive_test \  </div><div class="line">--delete-target-dir  --split-by id</div></pre></td></tr></table></figure>
</li>
<li><p>from hive to mysql</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">#Refer above from hdfs to mysql,only need specify the HDFS path corresponding to the Hive table</div><div class="line">/usr/local/sqoop/bin/sqoop export  \</div><div class="line">--connect &quot;jdbc:mysql://192.168.100.112/datbase?useSSL=false&amp;useUnicode=true&amp;characterEncoding=utf-8&quot; \</div><div class="line">--username username --password password</div><div class="line">--table customer --export-dir /user/hive/warehouse/user.db/customer --fields-terminated-by &apos;\001&apos;</div></pre></td></tr></table></figure>
</li>
<li><p>use sql as import condition</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">sqoop import --connect jdbc:mysql://&lt;dburi&gt;/&lt;dbname&gt; --username &lt;username&gt; --password &lt;password&gt; --query &lt;query-sql&gt; --split-by &lt;sp-column&gt; --hive-import --hive-table &lt;hive-tablename&gt; --target-dir &lt;hdfs-dir&gt;</div><div class="line"></div><div class="line">/usr/local/sqoop/bin/sqoop import \</div><div class="line">--connect jdbc:mysql://192.168.100.112/datbase?zeroDateTimeBehavior=CONVERT_TO_NULL \</div><div class="line">--username username --password password --table pv_daxue  --target-dir /usr/sqoop/daxue --delete-target-dir \</div><div class="line">--query &apos;select id,account from version where account=&quot;ddd&quot; and $CONDITIONS &apos; \</div><div class="line">--as-parquetfile</div></pre></td></tr></table></figure>
</li>
<li><p>from mysql to hbase</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">/usr/local/sqoop/bin/sqoop import \</div><div class="line">--connect jdbc:mysql://192.168.100.112/datbase?zeroDateTimeBehavior=CONVERT_TO_NULL \</div><div class="line">--username username --password password </div><div class="line">--query &apos;select id,account from version where account=&quot;ddd&quot; and $CONDITIONS &apos; \</div><div class="line">--hbase-table pv_daxue  --hbase-create-table \ </div><div class="line">--hbase-row-key id --split-by date -m 7 \ </div><div class="line">--column-family tiger</div></pre></td></tr></table></figure>
</li>
</ul>
<h5 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a><b>遇到的问题</b></h5><ul>
<li><p>ERROR tool.ImportTool: Import failed: java.io.FileNotFoundException</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">18/06/11 15:42:43 ERROR tool.ImportTool: Import failed: java.io.FileNotFoundException: File does not exist: hdfs://172.16.56.143:8020/usr/local/sqoop-1.4.7.bin__hadoop-2.6.0/lib/parquet-jackson-1.6.0.jar</div><div class="line"></div><div class="line">[hadoop@node1 conf]$ /usr/local/hadoop/bin/hadoop fs  -mkdir -p /usr/local/sqoop-1.4.7.bin__hadoop-2.6.0/lib</div><div class="line">[hadoop@node1 conf]$ /usr/local/hadoop/bin/hadoop fs  -put  /usr/local/sqoop-1.4.7.bin__hadoop-2.6.0/lib/* hdfs://172.16.56.143:8020/usr/local/sqoop-1.4.7.bin__hadoop-2.6.0/lib</div></pre></td></tr></table></figure>
</li>
<li><p>Caused by: com.mysql.cj.exceptions.CJException: The connection property ‘zeroDateTimeBehavior’ acceptable values are: ‘CONVERT_TO_NULL’, ‘EXCEPTION’ or ‘ROUND’. The value ‘convertToNull’ is not acceptable.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">#using the following code below:</div><div class="line">jdbc:mysql://localhost:3306/database?zeroDateTimeBehavior=CONVERT_TO_NULL</div></pre></td></tr></table></figure>
<p>  if config lzo ,you  perhaps see ,use command “hadoop checknative” check</p>
</li>
<li><p>ERROR sqoop.Sqoop: Got exception running Sqoop: java.lang.IllegalArgumentException: Compression codec com.hadoop.compression.lzo.LzoCodec not found.<br>java.lang.IllegalArgumentException: Compression codec com.hadoop.compression.lzo.LzoCodec not found.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"># install lzo support</div></pre></td></tr></table></figure>
</li>
<li><p>No primary key could be found for tablescore</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">specify one with --split-by or perform a sequential import with&apos;-m 1&apos;</div></pre></td></tr></table></figure>
</li>
<li><p>ERROR hive.HiveConfig: Could not load org.apache.hadoop.hive.conf.HiveConf. Make sure HIVE_CONF_DIR is set correctly.<br>ERROR tool.ImportTool: Import failed: java.io.IOException: java.lang.ClassNotFoundException: org.apache.hadoop.hive.conf.HiveConf</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">#add this one in .bash_profile:</div><div class="line">export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:/usr/lib/hive/lib/*</div><div class="line">source ~/.bash_profile</div></pre></td></tr></table></figure>
</li>
<li><p>FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException(message:For direct MetaStore DB connections, we don’t support retries at the client level.)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">mysql&gt; alter database hive character set latin1;</div></pre></td></tr></table></figure>
</li>
<li><p>java.lang.RuntimeException: Can’t parse input data: ‘1Hello2018-06-13 00:07:41.0’</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">--fields-terminated-by &apos;\001&apos;</div></pre></td></tr></table></figure>
</li>
</ul>
<p>ref<br><a href="http://sqoop.apache.org/docs/1.4.7/SqoopUserGuide.html#_incremental_imports" target="_blank" rel="external">Incremental Imports</a><br><a href="https://www.cnblogs.com/ljy2013/p/4872126.html" target="_blank" rel="external">sqoop的增量导入（increment import）</a><br><a href="https://github.com/kevinweil/hadoop-lzo" target="_blank" rel="external">hadoop-lzo</a><br><a href="http://www.oberhumer.com/opensource/lzo/#download" target="_blank" rel="external">lzo</a><br><a href="https://www.zybuluo.com/aitanjupt/note/209968#%E4%BD%BF%E7%94%A8sqoop%E4%BB%8Emysql%E5%AF%BC%E5%85%A5%E6%95%B0%E6%8D%AE%E5%88%B0hive" target="_blank" rel="external">Sqoop从MySQL导入数据</a></p>
<hr>
<p>您的鼓励是我写作最大的动力</p>
<p>俗话说，投资效率是最好的投资。 如果您感觉我的文章质量不错，读后收获很大，预计能为您提高 10% 的工作效率，不妨小额捐助我一下，让我有动力继续写出更多好文章。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;install&quot;&gt;&lt;a href=&quot;#install&quot; class=&quot;headerlink&quot; title=&quot;install&quot;&gt;&lt;/a&gt;&lt;b&gt;install&lt;/b&gt;&lt;/h5&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Rsyslog 连接 Kafka 指北</title>
    <link href="https://t1ger.github.io/2018/05/22/Rsyslog-%E8%BF%9E%E6%8E%A5-Kafka-%E6%8C%87%E5%8C%97/"/>
    <id>https://t1ger.github.io/2018/05/22/Rsyslog-连接-Kafka-指北/</id>
    <published>2018-05-22T07:20:48.000Z</published>
    <updated>2018-05-22T06:51:56.400Z</updated>
    
    <content type="html"><![CDATA[<h5 id="运行环境"><a href="#运行环境" class="headerlink" title="运行环境"></a><b>运行环境</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">[root@bogon ~]# cat /etc/redhat-release </div><div class="line">CentOS Linux release 7.4.1708 (Core) </div><div class="line"></div><div class="line">[root@bogon ~]# rpm -qa|grep rsyslog</div><div class="line">rsyslog-kafka-8.28.0-1.el7.x86_64</div><div class="line">rsyslog-8.28.0-1.el7.x86_64</div></pre></td></tr></table></figure>
<h5 id="安装"><a href="#安装" class="headerlink" title="安装"></a><b>安装</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">wget -O /etc/yum.repos.d/rsyslog.repo http://rpms.adiscon.com/v8-stable/rsyslog.repo</div><div class="line">yum install rsyslog rsyslog-kafka.x86_64</div></pre></td></tr></table></figure>
<p>国内的同学可能无法安装，同学们也可以通过<a href="http://rpms.adiscon.com/v8-stable/epel-7/x86_64/RPMS/" target="_blank" rel="external">这里</a>下载安装</p>
<h5 id="配置"><a href="#配置" class="headerlink" title="配置"></a><b>配置</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">[root@bogon ~]# cat /etc/rsyslog.d/kafka.conf</div><div class="line">module(load=&quot;omkafka&quot;)</div><div class="line">action (</div><div class="line">        type=&quot;omkafka&quot;</div><div class="line">        topic=&quot;topicA&quot;</div><div class="line">        broker=&quot;cdh1:9092,cdh2:9092,cdh3:9092&quot;</div><div class="line">    )</div><div class="line"></div><div class="line"></div><div class="line">#如果保存到本地</div><div class="line">[root@bogon ~]# cat /etc/rsyslog.d/router.conf</div><div class="line">template (name=&quot;DynFile&quot; type=&quot;string&quot; string=&quot;/data/%fromhost-ip%.log&quot;)</div><div class="line">if $fromhost-ip startswith &apos;192.168.100.2&apos; and $programname != &apos;Type=SESSION;&apos; and $programname != &apos;Type=Login;&apos; and $programname != &apos;Type=AuthLog;&apos; and $programname != &apos;Type=Ftp&apos; then &#123;</div><div class="line">    action(type=&quot;omfile&quot; dynaFile=&quot;DynFile&quot;)</div><div class="line">    stop</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>低版本的rsyslog保存到本地配置如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">[root@localhost ~]# rpm -qa|grep rsyslog</div><div class="line">rsyslog-5.8.10-6.el6.x86_64</div><div class="line"></div><div class="line">添加到 /etc/rsyslog.conf </div><div class="line">#### GLOBAL DIRECTIVES ####</div><div class="line">$template RemoteLogs,&quot;/data/var/log/%HOSTNAME%/%fromhost-ip%_%$YEAR%-%$MONTH%-%$DAY%.log&quot; *</div><div class="line">*.* ?RemoteLogs</div><div class="line">&amp;~</div></pre></td></tr></table></figure></p>
<p>通过kafka查看消息<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">cd /usr/local/kafka&amp;&amp; bin/kafka-console-consumer.sh  --bootstrap-server cdh1:9092,cdh2:9092,cdh3:9092   --topic topicA</div></pre></td></tr></table></figure></p>
<p>ref<br><a href="https://doc.yonyoucloud.com/doc/logstash-best-practice-cn/ecosystem/rsyslog.html" target="_blank" rel="external">Rsyslog</a><br><a href="http://wdxtub.com/2016/08/17/rsyslog-kafka-guide/" target="_blank" rel="external">Rsyslog 连接 Kafka 指南</a><br><a href="https://serverfault.com/questions/807108/how-to-call-template-so-rsyslog-8-creates-one-log-file-per-client" target="_blank" rel="external">How to call template so rsyslog 8 creates one log file per client</a><br><a href="http://wiki.rsyslog.com/index.php/DailyLogRotation" target="_blank" rel="external">DailyLogRotation</a><br><a href="http://blog.kompaz.win/2018/01/11/20180111%20CentOS7%20rsyslog%20+loganalyzer%E9%85%8D%E7%BD%AE/" target="_blank" rel="external">CentOS7 rsyslog +loganalyzer配置</a></p>
<hr>
<p>您的鼓励是我写作最大的动力</p>
<p>俗话说，投资效率是最好的投资。 如果您感觉我的文章质量不错，读后收获很大，预计能为您提高 10% 的工作效率，不妨小额捐助我一下，让我有动力继续写出更多好文章。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;运行环境&quot;&gt;&lt;a href=&quot;#运行环境&quot; class=&quot;headerlink&quot; title=&quot;运行环境&quot;&gt;&lt;/a&gt;&lt;b&gt;运行环境&lt;/b&gt;&lt;/h5&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutt
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Spark-Streaming with Kafka Programming</title>
    <link href="https://t1ger.github.io/2018/05/08/Spark-Streaming-with-Kafka-Programming/"/>
    <id>https://t1ger.github.io/2018/05/08/Spark-Streaming-with-Kafka-Programming/</id>
    <published>2018-05-08T07:55:06.000Z</published>
    <updated>2018-05-10T10:31:07.231Z</updated>
    
    <content type="html"><![CDATA[<h5 id="运行环境"><a href="#运行环境" class="headerlink" title="运行环境"></a><b>运行环境</b></h5><ol>
<li><p>jdk环境</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[root@cdh1 kafka]# java -version</div><div class="line">java version &quot;1.8.0_112&quot;</div><div class="line">Java(TM) SE Runtime Environment (build 1.8.0_112-b15)</div><div class="line">Java HotSpot(TM) 64-Bit Server VM (build 25.112-b15, mixed mode)</div></pre></td></tr></table></figure>
</li>
<li><p>引入maven</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">&lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</div><div class="line">&lt;artifactId&gt;spark-streaming_2.11&lt;/artifactId&gt;</div><div class="line">&lt;version&gt;2.3.0&lt;/version&gt;</div><div class="line"></div><div class="line">&lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</div><div class="line">&lt;artifactId&gt;spark-streaming-kafka-0-10_2.11&lt;/artifactId&gt;</div><div class="line">&lt;version&gt;2.3.0&lt;/version&gt;</div></pre></td></tr></table></figure>
</li>
</ol>
<h5 id="示例WordCount"><a href="#示例WordCount" class="headerlink" title="示例WordCount"></a><b>示例WordCount</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div></pre></td><td class="code"><pre><div class="line">package cn.spark.streaming;</div><div class="line"></div><div class="line"></div><div class="line">import java.util.Arrays;</div><div class="line">import java.util.Collection;</div><div class="line">import java.util.HashMap;</div><div class="line">import java.util.HashSet;</div><div class="line">import java.util.Iterator;</div><div class="line">import java.util.Map;</div><div class="line">import org.apache.kafka.clients.consumer.ConsumerRecord;</div><div class="line">import org.apache.spark.SparkConf;</div><div class="line">import org.apache.spark.TaskContext;</div><div class="line">import org.apache.spark.api.java.function.FlatMapFunction;</div><div class="line">import org.apache.spark.api.java.function.Function2;</div><div class="line">import org.apache.spark.api.java.function.PairFunction;</div><div class="line">import org.apache.spark.streaming.Durations;</div><div class="line">import org.apache.spark.streaming.api.java.JavaDStream;</div><div class="line">import org.apache.spark.streaming.api.java.JavaInputDStream;</div><div class="line">import org.apache.spark.streaming.api.java.JavaPairDStream;</div><div class="line">import org.apache.spark.streaming.api.java.JavaStreamingContext;</div><div class="line">import org.apache.spark.streaming.kafka010.CanCommitOffsets;</div><div class="line">import org.apache.spark.streaming.kafka010.ConsumerStrategies;</div><div class="line">import org.apache.spark.streaming.kafka010.HasOffsetRanges;</div><div class="line">import org.apache.spark.streaming.kafka010.KafkaUtils;</div><div class="line">import org.apache.spark.streaming.kafka010.LocationStrategies;</div><div class="line">import org.apache.spark.streaming.kafka010.OffsetRange;</div><div class="line"></div><div class="line">import scala.Tuple2;</div><div class="line"></div><div class="line">public class KafkaDirectWordCount &#123;</div><div class="line"></div><div class="line">	public static void main(String[] args) throws InterruptedException &#123;</div><div class="line">		// TODO Auto-generated method stub</div><div class="line">		</div><div class="line">		SparkConf  conf = new SparkConf()</div><div class="line">				.setAppName(&quot;KafkaReceiveWordCount&quot;)</div><div class="line">				.setMaster(&quot;local[2]&quot;);</div><div class="line">		JavaStreamingContext jssc = new JavaStreamingContext(conf,Durations.seconds(5));</div><div class="line">		</div><div class="line">		String brokers = &quot;192.168.10.140:9092,192.168.10.141:9092,192.168.10.142:9092&quot;;</div><div class="line">		</div><div class="line">		Map&lt;String, Object&gt; kafkaparams = new HashMap&lt;&gt;();</div><div class="line">		kafkaparams.put(&quot;metadata.broker.list&quot;, brokers);</div><div class="line">		kafkaparams.put(&quot;bootstrap.servers&quot;, brokers);</div><div class="line">		kafkaparams.put(&quot;group.id&quot;, &quot;KafkaDirectWordCount&quot;);</div><div class="line">		kafkaparams.put(&quot;key.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);</div><div class="line">		kafkaparams.put(&quot;key.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;);</div><div class="line">		kafkaparams.put(&quot;value.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;);</div><div class="line">		kafkaparams.put(&quot;enable.auto.commit&quot;, false);</div><div class="line">		kafkaparams.put(&quot;auto.offset.reset&quot;, &quot;latest&quot;); // earliest latest none </div><div class="line">		kafkaparams.put(&quot;offsets.storage&quot;, &quot;kafka&quot;);</div><div class="line">		</div><div class="line">		Collection&lt;String&gt; topics = new HashSet&lt;String&gt;();</div><div class="line">		topics.add(&quot;topicA&quot;);		</div><div class="line">		</div><div class="line">//		Map&lt;TopicPartition,Long&gt; offsets = new HashMap&lt;&gt;();</div><div class="line">//		offsets.put(new TopicPartition(&quot;topicA&quot;,0),2L);</div><div class="line">		</div><div class="line">		JavaInputDStream&lt;ConsumerRecord&lt;String,String&gt;&gt; lines = KafkaUtils.createDirectStream(</div><div class="line">				jssc,</div><div class="line">				LocationStrategies.PreferConsistent(),</div><div class="line">				ConsumerStrategies.Subscribe(topics, kafkaparams)</div><div class="line">				);</div><div class="line">		</div><div class="line">			lines.foreachRDD(rdd -&gt; &#123;</div><div class="line">			  OffsetRange[] offsetRanges = ((HasOffsetRanges) rdd.rdd()).offsetRanges();</div><div class="line">			  rdd.foreachPartition(consumerRecords -&gt; &#123;</div><div class="line">			    OffsetRange o = offsetRanges[TaskContext.get().partitionId()];</div><div class="line">			    System.out.println(</div><div class="line">			      o.topic() + &quot; &quot; + o.partition()  + &quot; &quot; + o.fromOffset() + &quot; &quot; + o.untilOffset());</div><div class="line">			  &#125;);</div><div class="line">			&#125;);</div><div class="line">		</div><div class="line">	    JavaDStream&lt;String&gt; words = lines.flatMap(new FlatMapFunction&lt;ConsumerRecord&lt;String,String&gt;,String&gt;()&#123;</div><div class="line">			private static final long serialVersionUID = 1L;</div><div class="line"></div><div class="line">			@Override</div><div class="line">			public Iterator&lt;String&gt; call(ConsumerRecord&lt;String, String&gt; line) throws Exception &#123;</div><div class="line">				// TODO Auto-generated method stub</div><div class="line">				return Arrays.asList(line.value().split(&quot; &quot;)).iterator();</div><div class="line">			&#125;</div><div class="line">	    	</div><div class="line">	    &#125;);</div><div class="line"></div><div class="line">		</div><div class="line">		JavaPairDStream&lt;String,Integer&gt; paris = words.mapToPair(new PairFunction&lt;String,String,Integer&gt;()&#123;</div><div class="line">			private static final long serialVersionUID = 1L;</div><div class="line"></div><div class="line">			@Override</div><div class="line">			public Tuple2&lt;String, Integer&gt; call(String word) throws Exception &#123;</div><div class="line">				// TODO Auto-generated method stub</div><div class="line">				return new Tuple2&lt;String,Integer&gt;(word,1);</div><div class="line">			&#125;</div><div class="line">			</div><div class="line">		&#125;);</div><div class="line">		</div><div class="line">	 JavaPairDStream&lt;String,Integer&gt; wordcount= paris.reduceByKey(new Function2&lt;Integer,Integer,Integer&gt;()&#123;</div><div class="line">		private static final long serialVersionUID = 1L;</div><div class="line"></div><div class="line">		@Override</div><div class="line">		public Integer call(Integer v1, Integer v2) throws Exception &#123;</div><div class="line">			// TODO Auto-generated method stub</div><div class="line">			return v1 + v2;</div><div class="line">		&#125;</div><div class="line">		 </div><div class="line">	 &#125;);</div><div class="line">	 </div><div class="line">	 </div><div class="line">	 wordcount.print();</div><div class="line">	 jssc.start();</div><div class="line">	 jssc.awaitTermination();</div><div class="line">	 jssc.close();</div><div class="line">	 </div><div class="line">	&#125;</div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>运行之前开启生产者：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">cd /usr/local/kafka</div><div class="line">bin/kafka-console-producer.sh --broker-list 192.168.10.140:9092,192.168.10.141:9092,192.168.10.142:9092 --topic topicA</div><div class="line">&gt; hello word hello me</div></pre></td></tr></table></figure></p>
<p>ref<br><a href="https://github.com/jaceklaskowski/spark-streaming-notebook/blob/master/spark-streaming-kafka-DirectKafkaInputDStream.adoc" target="_blank" rel="external">DirectKafkaInputDStream — Direct Kafka DStream</a><br><a href="http://spark.apache.org/docs/latest/streaming-kafka-0-10-integration.html" target="_blank" rel="external">Creating a Direct Stream</a><br><a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html#input-dstreams-and-receivers" target="_blank" rel="external">Spark Streaming Programming Guide</a><br><a href="http://blog.cloudera.com/blog/2017/06/offset-management-for-apache-kafka-with-apache-spark-streaming/" target="_blank" rel="external">Offset Management For Apache Kafka With Apache Spark Streaming</a><br><a href="https://blog.csdn.net/xueba207/article/details/51135423" target="_blank" rel="external">Spark Streaming ‘numRecords must not be negative’问题解决</a><br><a href="https://blog.csdn.net/lishuangzhe7047/article/details/74530417" target="_blank" rel="external">Kafka auto.offset.reset值详解</a><br><a href="https://blog.csdn.net/Dax1n/article/details/61614379" target="_blank" rel="external">Spark整合kafka0.10.0新特性(一)</a><br><a href="https://blog.csdn.net/sinat_27545249/article/details/78090872" target="_blank" rel="external">kafka0.8版本和sparkstreaming整合的两种不同方式</a><br><a href="https://blog.csdn.net/qfwyp0714/article/details/73998293" target="_blank" rel="external">Spark streaming 跟踪kafka offset的问题研究</a></p>
<hr>
<p>您的鼓励是我写作最大的动力</p>
<p>俗话说，投资效率是最好的投资。 如果您感觉我的文章质量不错，读后收获很大，预计能为您提高 10% 的工作效率，不妨小额捐助我一下，让我有动力继续写出更多好文章。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;运行环境&quot;&gt;&lt;a href=&quot;#运行环境&quot; class=&quot;headerlink&quot; title=&quot;运行环境&quot;&gt;&lt;/a&gt;&lt;b&gt;运行环境&lt;/b&gt;&lt;/h5&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;jdk环境&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;tab
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>how to config Filebeat6 quickly</title>
    <link href="https://t1ger.github.io/2018/04/11/how-to-config-Filebeat6-quickly/"/>
    <id>https://t1ger.github.io/2018/04/11/how-to-config-Filebeat6-quickly/</id>
    <published>2018-04-11T11:39:00.000Z</published>
    <updated>2018-04-19T08:04:50.199Z</updated>
    
    <content type="html"><![CDATA[<h5 id="环境介绍"><a href="#环境介绍" class="headerlink" title="环境介绍"></a><b>环境介绍</b></h5><p>系统为Centos6.8,相关软件版本如下：<br>filebeat-6.2.3<br>redis-3.0.7<br>logstash-6.2.3<br>kibana-6.2.3</p>
<p>架构为前端filebeat 读取nginx日志或其他日志（json格式），输出到中间redis，后端logstash从redis读取并解析</p>
<h5 id="filebeat-yml配置"><a href="#filebeat-yml配置" class="headerlink" title="filebeat.yml配置"></a><b>filebeat.yml配置</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div></pre></td><td class="code"><pre><div class="line">#filebeat配置</div><div class="line">cat /etc/filebeat/filebeat.yml </div><div class="line">filebeat.config_dir: prospectors.d</div><div class="line">filebeat.config.prospectors:</div><div class="line">  enabled: true</div><div class="line">  path: prospectors.d/*.yml</div><div class="line">  reload.enabled: true</div><div class="line">  reload.period: 10s </div><div class="line">filebeat.prospectors:</div><div class="line">- type: log</div><div class="line">  enabled: false</div><div class="line">  paths:</div><div class="line">    - /var/log/message</div><div class="line">filebeat.config.modules:</div><div class="line">  path: $&#123;path.config&#125;/modules.d/*.yml</div><div class="line">  reload.enabled: true</div><div class="line">  reload.period: 10s</div><div class="line">setup.template.settings:</div><div class="line">  index.number_of_shards: 3</div><div class="line">setup.kibana:</div><div class="line"> </div><div class="line">output.file:   #主要用于调试</div><div class="line">   path: &quot;/tmp&quot;</div><div class="line">   filename: filebeat.out</div><div class="line">   number_of_files: 7</div><div class="line">   rotate_every_kb: 10000 </div><div class="line">   enabled: false   #关闭输出</div><div class="line">output.redis:</div><div class="line">   hosts: [&quot;192.168.90.147:6379&quot;]</div><div class="line">   password: &quot;password&quot;</div><div class="line">   key: &quot;filebeat&quot;</div><div class="line">   db: 0</div><div class="line">   timeout: 60</div><div class="line">   max_retires: 3</div><div class="line">   bulk_max_size: 4096</div><div class="line">   datatype: list</div><div class="line">   keys:</div><div class="line">     - key: &quot;%&#123;[fields.log_source]&#125;&quot;</div><div class="line">       mapping:</div><div class="line">         &quot;bash_history&quot;: &quot;command-log&quot;</div><div class="line">         &quot;nginx&quot;  : &quot;nginx-log&quot;</div><div class="line"></div><div class="line">#bash历史记录</div><div class="line">[root@localhost ~]# cat /etc/filebeat/prospectors.d/history.yml </div><div class="line">- type: log</div><div class="line">  enabled: true</div><div class="line">  paths:</div><div class="line">    - /var/log/command.log  </div><div class="line">  fields:</div><div class="line">    log_source: command-log</div><div class="line">#  tags: &quot;bash_history&quot;</div><div class="line">  json.keys_under_root: true</div><div class="line">  json.add_error_key: true</div><div class="line">  json.message_key: TIME</div><div class="line">  </div><div class="line">#nginx日志配置</div><div class="line">[root@localhost ~]# cat /etc/filebeat/prospectors.d/nginx.yml</div><div class="line">- type: log</div><div class="line">  enabled: true</div><div class="line">  paths:</div><div class="line">    - /usr/local/openresty/nginx/logs/cms_log.log</div><div class="line">  fields:</div><div class="line">    log_source: nginx-log</div><div class="line">  exclude_lines: [&quot;helo.html&quot;]</div></pre></td></tr></table></figure>
<h5 id="JSON文件格式"><a href="#JSON文件格式" class="headerlink" title="JSON文件格式"></a><b>JSON文件格式</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line">#bash_history为json格式，添加到/etc/profile文件</div><div class="line">HISTDIR=&apos;/var/log/command.log&apos;</div><div class="line">if [ ! -f $HISTDIR ];then</div><div class="line">touch $HISTDIR</div><div class="line">chmod 666 $HISTDIR</div><div class="line">fi</div><div class="line">export HISTTIMEFORMAT=&quot;&#123;\&quot;TIME\&quot;:\&quot;%F %T\&quot;,\&quot;HOSTNAME\&quot;:\&quot;$HOSTNAME\&quot;,\&quot;LI\&quot;:\&quot;$(who -u am i 2&gt;/dev/null| awk &apos;&#123;print $NF&#125;&apos;|sed -e &apos;s/[()]//g&apos;)\&quot;,\&quot;LU\&quot;:\&quot;$(who am i|awk &apos;&#123;print $1&#125;&apos;)\&quot;,\&quot;NU\&quot;:\&quot;$&#123;USER&#125;\&quot;,\&quot;CMD\&quot;:\&quot;&quot;</div><div class="line">export PROMPT_COMMAND=&apos;history 1|tail -1|sed &quot;s/^[ ]\+[0-9]\+  //&quot;|sed &quot;s/$/\&quot;&#125;/&quot;&gt;&gt; /var/log/command.log&apos;</div><div class="line"></div><div class="line">#nginx日志格式为</div><div class="line">        log_format json &apos;&#123;&quot;@timestamp&quot;:&quot;$time_local&quot;,&apos;</div><div class="line">                &apos;&quot;source&quot;:&quot;nginx147&quot;,&apos;</div><div class="line">                &apos;&quot;serverAddr&quot;:&quot;$server_addr&quot;,&apos;</div><div class="line">                &apos;&quot;remoteAddr&quot;:&quot;$remote_addr&quot;,&apos;</div><div class="line">                &apos;&quot;remoteUser&quot;:&quot;$remote_user&quot;,&apos;</div><div class="line">                &apos;&quot;size&quot;:$body_bytes_sent,&apos;</div><div class="line">                &apos;&quot;status&quot;:$status,&apos;</div><div class="line">                &apos;&quot;time&quot;:$request_time,&apos;</div><div class="line">                &apos;&quot;method&quot;:&quot;$request_method&quot;,&apos;</div><div class="line">                &apos;&quot;protocol&quot;:&quot;$server_protocol&quot;,&apos;</div><div class="line">                &apos;&quot;url&quot;:&quot;$scheme://$host$request_uri&quot;,&apos;</div><div class="line">                &apos;&quot;host&quot;:&quot;$http_host&quot;,&apos;</div><div class="line">                &apos;&quot;uri&quot;:&quot;$uri&quot;,&apos;</div><div class="line">                &apos;&quot;referer&quot;:&quot;$http_referer&quot;,&apos;</div><div class="line">                &apos;&quot;xforwarded&quot;:&quot;$http_x_forwarded_for&quot;,&apos;</div><div class="line">                &apos;&quot;agent&quot;:&quot;$http_user_agent&quot;,&apos;</div><div class="line">                &apos;&quot;upsTime&quot;:&quot;$upstream_response_time&quot;,&apos;</div><div class="line">                &apos;&quot;sslPro&quot;:&quot;$ssl_protocol&quot;,&apos;</div><div class="line">                &apos;&quot;sslCip&quot;:&quot;$ssl_cipher&quot;,&apos;</div><div class="line">                &apos;&quot;upsStatus&quot;:&quot;$upstream_status&quot;&#125;&apos;;</div></pre></td></tr></table></figure>
<h5 id="logstash配置"><a href="#logstash配置" class="headerlink" title="logstash配置"></a><b>logstash配置</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div></pre></td><td class="code"><pre><div class="line">[root@localhost ~]# cat /etc/logstash/conf.d/history-logstash.conf </div><div class="line">input &#123;</div><div class="line">    redis &#123;</div><div class="line">        data_type =&gt; &quot;list&quot;  </div><div class="line">        host =&gt; &quot;192.168.90.147&quot;</div><div class="line">        port =&gt; &quot;6379&quot;</div><div class="line">        password =&gt; &quot;password&quot;</div><div class="line">        key  =&gt; &quot;command-log&quot;</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">output &#123;</div><div class="line">       if [fields][log_source] == &quot;command-log&quot; &#123; </div><div class="line">      elasticsearch &#123;</div><div class="line">          hosts   =&gt; [&quot;192.16.90.149:9200&quot;]</div><div class="line">          manage_template =&gt; false</div><div class="line">          action  =&gt; &quot;index&quot;</div><div class="line">          index   =&gt; &quot;command-log-%&#123;+YYYY.MM.dd&#125;&quot;</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">#nginx</div><div class="line">[root@localhost ~]# cat /etc/logstash/conf.d/nginx-logstash.conf </div><div class="line">input &#123;</div><div class="line">    redis &#123;</div><div class="line">        data_type =&gt; &quot;list&quot;  </div><div class="line">        host =&gt; &quot;192.168.90.147&quot;</div><div class="line">        port =&gt; &quot;6379&quot;</div><div class="line">        password =&gt; &quot;password&quot;</div><div class="line">        key  =&gt; &quot;nginx-log&quot;</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line">output &#123;</div><div class="line"></div><div class="line">	 if [fields][log_source] == &quot;nginx-log&quot; &#123;</div><div class="line">        file &#123;</div><div class="line">        path =&gt; &quot;/tmp/logs/nginx-%&#123;+YYYY-MM-dd&#125;.log&quot;</div><div class="line">        &#125;</div><div class="line">   &#125;</div><div class="line">   </div><div class="line">#     elasticsearch &#123;</div><div class="line">#        hosts   =&gt; [&quot;192.168.90.149:9200&quot;]</div><div class="line">#        action  =&gt; &quot;index&quot;</div><div class="line">#        index   =&gt; &quot;nginx-log-%&#123;+YYYY.MM.dd&#125;&quot;</div><div class="line">#    &#125;</div><div class="line"></div><div class="line">#        stdout &#123; codec =&gt; rubydebug &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>备注：<br>logstash安装完后需要执行以下命令，进行service服务安装<br>/usr/share/logstash/bin/system-install /etc/logstash/startup.options sysv</p>
<p>ref<br><a href="https://jkzhao.github.io/2017/10/24/Filebeat%E6%97%A5%E5%BF%97%E6%94%B6%E9%9B%86%E5%99%A8/" target="_blank" rel="external">Filebeat日志收集器</a><br><a href="https://blog.csdn.net/jianblog/article/details/54669203" target="_blank" rel="external">Elastic测试笔记</a><br><a href="https://www.elastic.co/guide/en/beats/filebeat/current/redis-output.html" target="_blank" rel="external">Configure the Redis output</a></p>
<hr>
<p>您的鼓励是我写作最大的动力</p>
<p>俗话说，投资效率是最好的投资。 如果您感觉我的文章质量不错，读后收获很大，预计能为您提高 10% 的工作效率，不妨小额捐助我一下，让我有动力继续写出更多好文章。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;环境介绍&quot;&gt;&lt;a href=&quot;#环境介绍&quot; class=&quot;headerlink&quot; title=&quot;环境介绍&quot;&gt;&lt;/a&gt;&lt;b&gt;环境介绍&lt;/b&gt;&lt;/h5&gt;&lt;p&gt;系统为Centos6.8,相关软件版本如下：&lt;br&gt;filebeat-6.2.3&lt;br&gt;redis-3.0.7
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>redis性能分析</title>
    <link href="https://t1ger.github.io/2018/04/10/redis%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90/"/>
    <id>https://t1ger.github.io/2018/04/10/redis性能分析/</id>
    <published>2018-04-10T10:11:17.000Z</published>
    <updated>2018-04-10T10:16:03.106Z</updated>
    
    <content type="html"><![CDATA[<h5 id="前言"><a href="#前言" class="headerlink" title="前言"></a><b>前言</b></h5><p>redis性能分析常见的有以下几个方面：</p>
<ul>
<li>redis slowlog分析</li>
<li>SCAN，SSCAN，HSCAN和ZSCAN命令的使用方法</li>
<li>redis是否受到系统使用swap</li>
<li>redis watchdog定位延时</li>
<li>关于redis的延时监控框架,可参考<a href="https://redis.io/topics/latency-monitor" target="_blank" rel="external">官网资料</a><br>下面我们分别从这几个方面来介绍</li>
</ul>
<h5 id="redis-slowlog分析"><a href="#redis-slowlog分析" class="headerlink" title="redis slowlog分析"></a><b>redis slowlog分析</b></h5><ol>
<li><p>慢查询设置<br>在Redis中有两种修改配置的方法,一种是修改配置文件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">slowlog-log-slower-than 10000  #查询时间超过10ms的会被记录  </div><div class="line">slowlog-max-len 128            # 最多记录128个慢查询</div></pre></td></tr></table></figure>
<p> 另一种是使用config set命令动态修改.例如下面使用config set命令将slowlog-log-slower-than设置为20000微妙.slowlog-max-len设置为1024</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">config set slowlog-log-slower-than 20000</div><div class="line">config set slowlog-max-len 1024</div><div class="line">config rewrite</div></pre></td></tr></table></figure>
<p> 如果需要将Redis将配置持久化到本地配置文件,要执行config rewrite命令,如果slowlog-log-slower-than=0会记录所有命令,slowlog-log-slower-than&lt;0对于任何命令都不会进行记录</p>
</li>
<li><p>获取慢查询日志<br>slowlog get [n]</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">127.0.0.1:6379&gt; slowlog get 15</div><div class="line"> 1) 1) (integer) 79674  #slowlog的唯一编号 </div><div class="line">    2) (integer) 1523350838 #此次slowlog事件的发生时间  </div><div class="line">    3) (integer) 2987577    #耗时,以微秒为单位</div><div class="line">    4) 1) &quot;KEYS&quot;</div><div class="line">       2) &quot;mid_cache_app_list_*&quot;</div></pre></td></tr></table></figure>
</li>
<li><p>获取慢查询日志列表当前长度</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">127.0.0.1:6379&gt; slowlog len</div><div class="line">(integer) 128</div></pre></td></tr></table></figure>
</li>
<li><p>慢查询日志重置</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">slowlog reset</div></pre></td></tr></table></figure>
</li>
<li><p>建议：<br>slowlog-max-len 建议线上设置为1000以上<br>slowlog-log-slower-than对高流量场景应该设置在1毫秒以上<br>慢查询只记录命令的执行时间,并不包括命令排队和网络传输时间.因此客户端执行命令的时间会大于命令的实际执行时间.因为命令执行排队机制,慢查询会导致其他命令级联阻塞,因此客户端出现请求超时时,需要检查该时间点是否有对应的慢查询,从而分析是否为慢查询导致的命令级联阻塞.</p>
</li>
</ol>
<h5 id="SCAN，SSCAN，HSCAN和ZSCAN命令的使用方法"><a href="#SCAN，SSCAN，HSCAN和ZSCAN命令的使用方法" class="headerlink" title="SCAN，SSCAN，HSCAN和ZSCAN命令的使用方法"></a><b>SCAN，SSCAN，HSCAN和ZSCAN命令的使用方法</b></h5><ol>
<li><p>SCAN是基于游标的迭代器。每次调用命令时，服务器返回一个更新的游标，用户需要在下一次调用中用作游标参数。当游标设置为0时，迭代开始，并且当服务器返回的游标为0时终止迭代<br>开始游标值为0的迭代，并调用SCAN，直到返回的游标再次为0，称为完全迭代</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">127.0.0.1:6379&gt; scan 0  </div><div class="line"></div><div class="line">127.0.0.1:6379&gt; scan 0 count 20 //指定输出的数量</div><div class="line">127.0.0.1:6379&gt; scan 0 match *mid_sent*   //类似于keys命令按模式匹配</div></pre></td></tr></table></figure>
</li>
<li><p>sscan查询sets集合的方法：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">redis 127.0.0.1:6379&gt; sadd setone 1 2 3 foo foobar feelsgood  </div><div class="line">(integer) 6  </div><div class="line">redis 127.0.0.1:6379&gt; sscan setone 0 match f*  </div><div class="line">1) &quot;0&quot;  </div><div class="line">2) 1) &quot;foo&quot;  </div><div class="line">   2) &quot;feelsgood&quot;  </div><div class="line">   3) &quot;foobar&quot;</div></pre></td></tr></table></figure>
</li>
<li><p>hscan查询hash集合的方法：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">redis 127.0.0.1:6379&gt; hmset hash name Tom age 35  </div><div class="line">OK  </div><div class="line">redis 127.0.0.1:6379&gt; hscan hash 0  </div><div class="line">1) &quot;0&quot;  </div><div class="line">2) 1) &quot;name&quot;  </div><div class="line">   2) &quot;Tom&quot;  </div><div class="line">   3) &quot;age&quot;  </div><div class="line">   4) &quot;35&quot;</div></pre></td></tr></table></figure>
</li>
<li><p>Linux内核启用了透明巨页功能时，Redis在使用fork调用之后会产生大的延迟代价，以便在磁盘进行数据持久化</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled</div></pre></td></tr></table></figure>
<p> 需重启redis才能生效</p>
</li>
</ol>
<h5 id="redis是否受到系统使用swap"><a href="#redis是否受到系统使用swap" class="headerlink" title="redis是否受到系统使用swap"></a><b>redis是否受到系统使用swap</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">查找redis进程id：  </div><div class="line">redis-cli -p 6319 info|grep process_id  </div><div class="line">process_id:9213  </div><div class="line">查看redis进程的内存使用信息：  </div><div class="line">cd /proc/9213 </div><div class="line">查看该进程使用swap分区的统计信息，以不使用或只有少量的4kB为佳：  </div><div class="line">cat smaps | grep &apos;Swap:&apos;  </div><div class="line">同时打印出内存映射和swap使用信息：查看那些较大的内存消耗是否引发了大的swap使用  </div><div class="line">cat smaps | egrep &apos;^(Swap:Size)&apos;</div></pre></td></tr></table></figure>
<h5 id="redis-watchdog定位延时"><a href="#redis-watchdog定位延时" class="headerlink" title="redis watchdog定位延时"></a><b>redis watchdog定位延时</b></h5><p>注意： 实验功能，请确保redis数据已备份,会对redis服务性能产生影响<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">Redis software watchdog  </div><div class="line">该功能只能动态启用，使用以下命令：  </div><div class="line">CONFIG SET watchdog-period 500  </div><div class="line">注：redis会开始频繁监控自身的延时问题，并把问题输出到日志文件中去。  </div><div class="line">  </div><div class="line">关闭watchdog：  </div><div class="line">CONFIG SET watchdog-period 0</div></pre></td></tr></table></figure></p>
<h5 id="Redis-latency-monitoring-framework"><a href="#Redis-latency-monitoring-framework" class="headerlink" title="Redis latency monitoring framework"></a><b>Redis latency monitoring framework</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">CONFIG SET latency-monitor-threshold 100</div></pre></td></tr></table></figure>
<p>默认情况下，阈值设置为0，即禁用redis监控。实际上启用该监控功能，对redis所增加的成本很少.</p>
<p>LATENCY命令的使用方法</p>
<ol>
<li><p>查看最新的延时事件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">127.0.0.1:6379&gt; latency latest  </div><div class="line">1) 1) &quot;command&quot;     #event name  </div><div class="line">   2) (integer) 1480865648     #发生时间  </div><div class="line">   3) (integer) 207     #耗时，毫秒  </div><div class="line">   4) (integer) 239     #从redis启动或上次latency reset以来，这种事件的最大延时记录</div></pre></td></tr></table></figure>
</li>
<li><p>查看延时事件的历史信息</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">127.0.0.1:6379&gt; latency history command  </div><div class="line">  1) 1) (integer) 1480865710  </div><div class="line">     2) (integer) 207  </div><div class="line">  2) 1) (integer) 1480865711  </div><div class="line">     2) (integer) 217</div></pre></td></tr></table></figure>
</li>
<li><p>LATENCY DOCTOR<br>延时事件统计信息的智能分析与建议</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">127.0.0.1:6379&gt; latency doctor  </div><div class="line">Dave, I have observed latency spikes in this Redis instance.  </div><div class="line">You don&apos;t mind talking about it, do you Dave?  </div><div class="line">1. command: 5 latency spikes (average 300ms, mean deviation 120ms,  </div><div class="line">  period 73.40 sec). Worst all time event 500ms.  </div><div class="line">I have a few advices for you:  </div><div class="line">- Your current Slow Log configuration only logs events that are  </div><div class="line">  slower than your configured latency monitor threshold. Please  </div><div class="line">  use &apos;CONFIG SET slowlog-log-slower-than 1000&apos;.  </div><div class="line">- Check your Slow Log to understand what are the commands you are  </div><div class="line">  running which are too slow to execute. Please check  </div><div class="line">  http://redis.io/commands/slowlog for more information.</div></pre></td></tr></table></figure>
</li>
</ol>
<hr>
<p>您的鼓励是我写作最大的动力</p>
<p>俗话说，投资效率是最好的投资。 如果您感觉我的文章质量不错，读后收获很大，预计能为您提高 10% 的工作效率，不妨小额捐助我一下，让我有动力继续写出更多好文章。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;&lt;b&gt;前言&lt;/b&gt;&lt;/h5&gt;&lt;p&gt;redis性能分析常见的有以下几个方面：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;redis slowlog分析&lt;/li&gt;
&lt;li&gt;SCAN
    
    </summary>
    
    
  </entry>
  
</feed>
