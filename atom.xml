<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>t1ger的茶馆</title>
  <subtitle>头顶有光终是幻，足下生云未是仙</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://t1ger.github.io/"/>
  <updated>2018-05-22T06:51:56.400Z</updated>
  <id>https://t1ger.github.io/</id>
  
  <author>
    <name>t1ger</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Rsyslog 连接 Kafka 指北</title>
    <link href="https://t1ger.github.io/2018/05/22/Rsyslog-%E8%BF%9E%E6%8E%A5-Kafka-%E6%8C%87%E5%8C%97/"/>
    <id>https://t1ger.github.io/2018/05/22/Rsyslog-连接-Kafka-指北/</id>
    <published>2018-05-22T07:20:48.000Z</published>
    <updated>2018-05-22T06:51:56.400Z</updated>
    
    <content type="html"><![CDATA[<h5 id="运行环境"><a href="#运行环境" class="headerlink" title="运行环境"></a><b>运行环境</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">[root@bogon ~]# cat /etc/redhat-release </div><div class="line">CentOS Linux release 7.4.1708 (Core) </div><div class="line"></div><div class="line">[root@bogon ~]# rpm -qa|grep rsyslog</div><div class="line">rsyslog-kafka-8.28.0-1.el7.x86_64</div><div class="line">rsyslog-8.28.0-1.el7.x86_64</div></pre></td></tr></table></figure>
<h5 id="安装"><a href="#安装" class="headerlink" title="安装"></a><b>安装</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">wget -O /etc/yum.repos.d/rsyslog.repo http://rpms.adiscon.com/v8-stable/rsyslog.repo</div><div class="line">yum install rsyslog rsyslog-kafka.x86_64</div></pre></td></tr></table></figure>
<p>国内的同学可能无法安装，同学们也可以通过<a href="http://rpms.adiscon.com/v8-stable/epel-7/x86_64/RPMS/" target="_blank" rel="external">这里</a>下载安装</p>
<h5 id="配置"><a href="#配置" class="headerlink" title="配置"></a><b>配置</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">[root@bogon ~]# cat /etc/rsyslog.d/kafka.conf</div><div class="line">module(load=&quot;omkafka&quot;)</div><div class="line">action (</div><div class="line">        type=&quot;omkafka&quot;</div><div class="line">        topic=&quot;topicA&quot;</div><div class="line">        broker=&quot;cdh1:9092,cdh2:9092,cdh3:9092&quot;</div><div class="line">    )</div><div class="line"></div><div class="line"></div><div class="line">#如果保存到本地</div><div class="line">[root@bogon ~]# cat /etc/rsyslog.d/router.conf</div><div class="line">template (name=&quot;DynFile&quot; type=&quot;string&quot; string=&quot;/data/%fromhost-ip%.log&quot;)</div><div class="line">if $fromhost-ip startswith &apos;192.168.100.2&apos; and $programname != &apos;Type=SESSION;&apos; and $programname != &apos;Type=Login;&apos; and $programname != &apos;Type=AuthLog;&apos; and $programname != &apos;Type=Ftp&apos; then &#123;</div><div class="line">    action(type=&quot;omfile&quot; dynaFile=&quot;DynFile&quot;)</div><div class="line">    stop</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>低版本的rsyslog保存到本地配置如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">[root@localhost ~]# rpm -qa|grep rsyslog</div><div class="line">rsyslog-5.8.10-6.el6.x86_64</div><div class="line"></div><div class="line">添加到 /etc/rsyslog.conf </div><div class="line">#### GLOBAL DIRECTIVES ####</div><div class="line">$template RemoteLogs,&quot;/data/var/log/%HOSTNAME%/%fromhost-ip%_%$YEAR%-%$MONTH%-%$DAY%.log&quot; *</div><div class="line">*.* ?RemoteLogs</div><div class="line">&amp;~</div></pre></td></tr></table></figure></p>
<p>通过kafka查看消息<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">cd /usr/local/kafka&amp;&amp; bin/kafka-console-consumer.sh  --bootstrap-server cdh1:9092,cdh2:9092,cdh3:9092   --topic topicA</div></pre></td></tr></table></figure></p>
<p>ref<br><a href="https://doc.yonyoucloud.com/doc/logstash-best-practice-cn/ecosystem/rsyslog.html" target="_blank" rel="external">Rsyslog</a><br><a href="http://wdxtub.com/2016/08/17/rsyslog-kafka-guide/" target="_blank" rel="external">Rsyslog 连接 Kafka 指南</a><br><a href="https://serverfault.com/questions/807108/how-to-call-template-so-rsyslog-8-creates-one-log-file-per-client" target="_blank" rel="external">How to call template so rsyslog 8 creates one log file per client</a><br><a href="http://wiki.rsyslog.com/index.php/DailyLogRotation" target="_blank" rel="external">DailyLogRotation</a><br><a href="http://blog.kompaz.win/2018/01/11/20180111%20CentOS7%20rsyslog%20+loganalyzer%E9%85%8D%E7%BD%AE/" target="_blank" rel="external">CentOS7 rsyslog +loganalyzer配置</a></p>
<hr>
<p>您的鼓励是我写作最大的动力</p>
<p>俗话说，投资效率是最好的投资。 如果您感觉我的文章质量不错，读后收获很大，预计能为您提高 10% 的工作效率，不妨小额捐助我一下，让我有动力继续写出更多好文章。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;运行环境&quot;&gt;&lt;a href=&quot;#运行环境&quot; class=&quot;headerlink&quot; title=&quot;运行环境&quot;&gt;&lt;/a&gt;&lt;b&gt;运行环境&lt;/b&gt;&lt;/h5&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutt
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Spark-Streaming with Kafka Programming</title>
    <link href="https://t1ger.github.io/2018/05/08/Spark-Streaming-with-Kafka-Programming/"/>
    <id>https://t1ger.github.io/2018/05/08/Spark-Streaming-with-Kafka-Programming/</id>
    <published>2018-05-08T07:55:06.000Z</published>
    <updated>2018-05-10T10:31:07.231Z</updated>
    
    <content type="html"><![CDATA[<h5 id="运行环境"><a href="#运行环境" class="headerlink" title="运行环境"></a><b>运行环境</b></h5><ol>
<li><p>jdk环境</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[root@cdh1 kafka]# java -version</div><div class="line">java version &quot;1.8.0_112&quot;</div><div class="line">Java(TM) SE Runtime Environment (build 1.8.0_112-b15)</div><div class="line">Java HotSpot(TM) 64-Bit Server VM (build 25.112-b15, mixed mode)</div></pre></td></tr></table></figure>
</li>
<li><p>引入maven</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">&lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</div><div class="line">&lt;artifactId&gt;spark-streaming_2.11&lt;/artifactId&gt;</div><div class="line">&lt;version&gt;2.3.0&lt;/version&gt;</div><div class="line"></div><div class="line">&lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</div><div class="line">&lt;artifactId&gt;spark-streaming-kafka-0-10_2.11&lt;/artifactId&gt;</div><div class="line">&lt;version&gt;2.3.0&lt;/version&gt;</div></pre></td></tr></table></figure>
</li>
</ol>
<h5 id="示例WordCount"><a href="#示例WordCount" class="headerlink" title="示例WordCount"></a><b>示例WordCount</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div></pre></td><td class="code"><pre><div class="line">package cn.spark.streaming;</div><div class="line"></div><div class="line"></div><div class="line">import java.util.Arrays;</div><div class="line">import java.util.Collection;</div><div class="line">import java.util.HashMap;</div><div class="line">import java.util.HashSet;</div><div class="line">import java.util.Iterator;</div><div class="line">import java.util.Map;</div><div class="line">import org.apache.kafka.clients.consumer.ConsumerRecord;</div><div class="line">import org.apache.spark.SparkConf;</div><div class="line">import org.apache.spark.TaskContext;</div><div class="line">import org.apache.spark.api.java.function.FlatMapFunction;</div><div class="line">import org.apache.spark.api.java.function.Function2;</div><div class="line">import org.apache.spark.api.java.function.PairFunction;</div><div class="line">import org.apache.spark.streaming.Durations;</div><div class="line">import org.apache.spark.streaming.api.java.JavaDStream;</div><div class="line">import org.apache.spark.streaming.api.java.JavaInputDStream;</div><div class="line">import org.apache.spark.streaming.api.java.JavaPairDStream;</div><div class="line">import org.apache.spark.streaming.api.java.JavaStreamingContext;</div><div class="line">import org.apache.spark.streaming.kafka010.CanCommitOffsets;</div><div class="line">import org.apache.spark.streaming.kafka010.ConsumerStrategies;</div><div class="line">import org.apache.spark.streaming.kafka010.HasOffsetRanges;</div><div class="line">import org.apache.spark.streaming.kafka010.KafkaUtils;</div><div class="line">import org.apache.spark.streaming.kafka010.LocationStrategies;</div><div class="line">import org.apache.spark.streaming.kafka010.OffsetRange;</div><div class="line"></div><div class="line">import scala.Tuple2;</div><div class="line"></div><div class="line">public class KafkaDirectWordCount &#123;</div><div class="line"></div><div class="line">	public static void main(String[] args) throws InterruptedException &#123;</div><div class="line">		// TODO Auto-generated method stub</div><div class="line">		</div><div class="line">		SparkConf  conf = new SparkConf()</div><div class="line">				.setAppName(&quot;KafkaReceiveWordCount&quot;)</div><div class="line">				.setMaster(&quot;local[2]&quot;);</div><div class="line">		JavaStreamingContext jssc = new JavaStreamingContext(conf,Durations.seconds(5));</div><div class="line">		</div><div class="line">		String brokers = &quot;192.168.10.140:9092,192.168.10.141:9092,192.168.10.142:9092&quot;;</div><div class="line">		</div><div class="line">		Map&lt;String, Object&gt; kafkaparams = new HashMap&lt;&gt;();</div><div class="line">		kafkaparams.put(&quot;metadata.broker.list&quot;, brokers);</div><div class="line">		kafkaparams.put(&quot;bootstrap.servers&quot;, brokers);</div><div class="line">		kafkaparams.put(&quot;group.id&quot;, &quot;KafkaDirectWordCount&quot;);</div><div class="line">		kafkaparams.put(&quot;key.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);</div><div class="line">		kafkaparams.put(&quot;key.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;);</div><div class="line">		kafkaparams.put(&quot;value.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;);</div><div class="line">		kafkaparams.put(&quot;enable.auto.commit&quot;, false);</div><div class="line">		kafkaparams.put(&quot;auto.offset.reset&quot;, &quot;latest&quot;); // earliest latest none </div><div class="line">		kafkaparams.put(&quot;offsets.storage&quot;, &quot;kafka&quot;);</div><div class="line">		</div><div class="line">		Collection&lt;String&gt; topics = new HashSet&lt;String&gt;();</div><div class="line">		topics.add(&quot;topicA&quot;);		</div><div class="line">		</div><div class="line">//		Map&lt;TopicPartition,Long&gt; offsets = new HashMap&lt;&gt;();</div><div class="line">//		offsets.put(new TopicPartition(&quot;topicA&quot;,0),2L);</div><div class="line">		</div><div class="line">		JavaInputDStream&lt;ConsumerRecord&lt;String,String&gt;&gt; lines = KafkaUtils.createDirectStream(</div><div class="line">				jssc,</div><div class="line">				LocationStrategies.PreferConsistent(),</div><div class="line">				ConsumerStrategies.Subscribe(topics, kafkaparams)</div><div class="line">				);</div><div class="line">		</div><div class="line">			lines.foreachRDD(rdd -&gt; &#123;</div><div class="line">			  OffsetRange[] offsetRanges = ((HasOffsetRanges) rdd.rdd()).offsetRanges();</div><div class="line">			  rdd.foreachPartition(consumerRecords -&gt; &#123;</div><div class="line">			    OffsetRange o = offsetRanges[TaskContext.get().partitionId()];</div><div class="line">			    System.out.println(</div><div class="line">			      o.topic() + &quot; &quot; + o.partition()  + &quot; &quot; + o.fromOffset() + &quot; &quot; + o.untilOffset());</div><div class="line">			  &#125;);</div><div class="line">			&#125;);</div><div class="line">		</div><div class="line">	    JavaDStream&lt;String&gt; words = lines.flatMap(new FlatMapFunction&lt;ConsumerRecord&lt;String,String&gt;,String&gt;()&#123;</div><div class="line">			private static final long serialVersionUID = 1L;</div><div class="line"></div><div class="line">			@Override</div><div class="line">			public Iterator&lt;String&gt; call(ConsumerRecord&lt;String, String&gt; line) throws Exception &#123;</div><div class="line">				// TODO Auto-generated method stub</div><div class="line">				return Arrays.asList(line.value().split(&quot; &quot;)).iterator();</div><div class="line">			&#125;</div><div class="line">	    	</div><div class="line">	    &#125;);</div><div class="line"></div><div class="line">		</div><div class="line">		JavaPairDStream&lt;String,Integer&gt; paris = words.mapToPair(new PairFunction&lt;String,String,Integer&gt;()&#123;</div><div class="line">			private static final long serialVersionUID = 1L;</div><div class="line"></div><div class="line">			@Override</div><div class="line">			public Tuple2&lt;String, Integer&gt; call(String word) throws Exception &#123;</div><div class="line">				// TODO Auto-generated method stub</div><div class="line">				return new Tuple2&lt;String,Integer&gt;(word,1);</div><div class="line">			&#125;</div><div class="line">			</div><div class="line">		&#125;);</div><div class="line">		</div><div class="line">	 JavaPairDStream&lt;String,Integer&gt; wordcount= paris.reduceByKey(new Function2&lt;Integer,Integer,Integer&gt;()&#123;</div><div class="line">		private static final long serialVersionUID = 1L;</div><div class="line"></div><div class="line">		@Override</div><div class="line">		public Integer call(Integer v1, Integer v2) throws Exception &#123;</div><div class="line">			// TODO Auto-generated method stub</div><div class="line">			return v1 + v2;</div><div class="line">		&#125;</div><div class="line">		 </div><div class="line">	 &#125;);</div><div class="line">	 </div><div class="line">	 </div><div class="line">	 wordcount.print();</div><div class="line">	 jssc.start();</div><div class="line">	 jssc.awaitTermination();</div><div class="line">	 jssc.close();</div><div class="line">	 </div><div class="line">	&#125;</div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>运行之前开启生产者：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">cd /usr/local/kafka</div><div class="line">bin/kafka-console-producer.sh --broker-list 192.168.10.140:9092,192.168.10.141:9092,192.168.10.142:9092 --topic topicA</div><div class="line">&gt; hello word hello me</div></pre></td></tr></table></figure></p>
<p>ref<br><a href="https://github.com/jaceklaskowski/spark-streaming-notebook/blob/master/spark-streaming-kafka-DirectKafkaInputDStream.adoc" target="_blank" rel="external">DirectKafkaInputDStream — Direct Kafka DStream</a><br><a href="http://spark.apache.org/docs/latest/streaming-kafka-0-10-integration.html" target="_blank" rel="external">Creating a Direct Stream</a><br><a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html#input-dstreams-and-receivers" target="_blank" rel="external">Spark Streaming Programming Guide</a><br><a href="http://blog.cloudera.com/blog/2017/06/offset-management-for-apache-kafka-with-apache-spark-streaming/" target="_blank" rel="external">Offset Management For Apache Kafka With Apache Spark Streaming</a><br><a href="https://blog.csdn.net/xueba207/article/details/51135423" target="_blank" rel="external">Spark Streaming ‘numRecords must not be negative’问题解决</a><br><a href="https://blog.csdn.net/lishuangzhe7047/article/details/74530417" target="_blank" rel="external">Kafka auto.offset.reset值详解</a><br><a href="https://blog.csdn.net/Dax1n/article/details/61614379" target="_blank" rel="external">Spark整合kafka0.10.0新特性(一)</a><br><a href="https://blog.csdn.net/sinat_27545249/article/details/78090872" target="_blank" rel="external">kafka0.8版本和sparkstreaming整合的两种不同方式</a><br><a href="https://blog.csdn.net/qfwyp0714/article/details/73998293" target="_blank" rel="external">Spark streaming 跟踪kafka offset的问题研究</a></p>
<hr>
<p>您的鼓励是我写作最大的动力</p>
<p>俗话说，投资效率是最好的投资。 如果您感觉我的文章质量不错，读后收获很大，预计能为您提高 10% 的工作效率，不妨小额捐助我一下，让我有动力继续写出更多好文章。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;运行环境&quot;&gt;&lt;a href=&quot;#运行环境&quot; class=&quot;headerlink&quot; title=&quot;运行环境&quot;&gt;&lt;/a&gt;&lt;b&gt;运行环境&lt;/b&gt;&lt;/h5&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;jdk环境&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;tab
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>how to config Filebeat6 quickly</title>
    <link href="https://t1ger.github.io/2018/04/11/how-to-config-Filebeat6-quickly/"/>
    <id>https://t1ger.github.io/2018/04/11/how-to-config-Filebeat6-quickly/</id>
    <published>2018-04-11T11:39:00.000Z</published>
    <updated>2018-04-19T08:04:50.199Z</updated>
    
    <content type="html"><![CDATA[<h5 id="环境介绍"><a href="#环境介绍" class="headerlink" title="环境介绍"></a><b>环境介绍</b></h5><p>系统为Centos6.8,相关软件版本如下：<br>filebeat-6.2.3<br>redis-3.0.7<br>logstash-6.2.3<br>kibana-6.2.3</p>
<p>架构为前端filebeat 读取nginx日志或其他日志（json格式），输出到中间redis，后端logstash从redis读取并解析</p>
<h5 id="filebeat-yml配置"><a href="#filebeat-yml配置" class="headerlink" title="filebeat.yml配置"></a><b>filebeat.yml配置</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div></pre></td><td class="code"><pre><div class="line">#filebeat配置</div><div class="line">cat /etc/filebeat/filebeat.yml </div><div class="line">filebeat.config_dir: prospectors.d</div><div class="line">filebeat.config.prospectors:</div><div class="line">  enabled: true</div><div class="line">  path: prospectors.d/*.yml</div><div class="line">  reload.enabled: true</div><div class="line">  reload.period: 10s </div><div class="line">filebeat.prospectors:</div><div class="line">- type: log</div><div class="line">  enabled: false</div><div class="line">  paths:</div><div class="line">    - /var/log/message</div><div class="line">filebeat.config.modules:</div><div class="line">  path: $&#123;path.config&#125;/modules.d/*.yml</div><div class="line">  reload.enabled: true</div><div class="line">  reload.period: 10s</div><div class="line">setup.template.settings:</div><div class="line">  index.number_of_shards: 3</div><div class="line">setup.kibana:</div><div class="line"> </div><div class="line">output.file:   #主要用于调试</div><div class="line">   path: &quot;/tmp&quot;</div><div class="line">   filename: filebeat.out</div><div class="line">   number_of_files: 7</div><div class="line">   rotate_every_kb: 10000 </div><div class="line">   enabled: false   #关闭输出</div><div class="line">output.redis:</div><div class="line">   hosts: [&quot;192.168.90.147:6379&quot;]</div><div class="line">   password: &quot;password&quot;</div><div class="line">   key: &quot;filebeat&quot;</div><div class="line">   db: 0</div><div class="line">   timeout: 60</div><div class="line">   max_retires: 3</div><div class="line">   bulk_max_size: 4096</div><div class="line">   datatype: list</div><div class="line">   keys:</div><div class="line">     - key: &quot;%&#123;[fields.log_source]&#125;&quot;</div><div class="line">       mapping:</div><div class="line">         &quot;bash_history&quot;: &quot;command-log&quot;</div><div class="line">         &quot;nginx&quot;  : &quot;nginx-log&quot;</div><div class="line"></div><div class="line">#bash历史记录</div><div class="line">[root@localhost ~]# cat /etc/filebeat/prospectors.d/history.yml </div><div class="line">- type: log</div><div class="line">  enabled: true</div><div class="line">  paths:</div><div class="line">    - /var/log/command.log  </div><div class="line">  fields:</div><div class="line">    log_source: command-log</div><div class="line">#  tags: &quot;bash_history&quot;</div><div class="line">  json.keys_under_root: true</div><div class="line">  json.add_error_key: true</div><div class="line">  json.message_key: TIME</div><div class="line">  </div><div class="line">#nginx日志配置</div><div class="line">[root@localhost ~]# cat /etc/filebeat/prospectors.d/nginx.yml</div><div class="line">- type: log</div><div class="line">  enabled: true</div><div class="line">  paths:</div><div class="line">    - /usr/local/openresty/nginx/logs/cms_log.log</div><div class="line">  fields:</div><div class="line">    log_source: nginx-log</div><div class="line">  exclude_lines: [&quot;helo.html&quot;]</div></pre></td></tr></table></figure>
<h5 id="JSON文件格式"><a href="#JSON文件格式" class="headerlink" title="JSON文件格式"></a><b>JSON文件格式</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line">#bash_history为json格式，添加到/etc/profile文件</div><div class="line">HISTDIR=&apos;/var/log/command.log&apos;</div><div class="line">if [ ! -f $HISTDIR ];then</div><div class="line">touch $HISTDIR</div><div class="line">chmod 666 $HISTDIR</div><div class="line">fi</div><div class="line">export HISTTIMEFORMAT=&quot;&#123;\&quot;TIME\&quot;:\&quot;%F %T\&quot;,\&quot;HOSTNAME\&quot;:\&quot;$HOSTNAME\&quot;,\&quot;LI\&quot;:\&quot;$(who -u am i 2&gt;/dev/null| awk &apos;&#123;print $NF&#125;&apos;|sed -e &apos;s/[()]//g&apos;)\&quot;,\&quot;LU\&quot;:\&quot;$(who am i|awk &apos;&#123;print $1&#125;&apos;)\&quot;,\&quot;NU\&quot;:\&quot;$&#123;USER&#125;\&quot;,\&quot;CMD\&quot;:\&quot;&quot;</div><div class="line">export PROMPT_COMMAND=&apos;history 1|tail -1|sed &quot;s/^[ ]\+[0-9]\+  //&quot;|sed &quot;s/$/\&quot;&#125;/&quot;&gt;&gt; /var/log/command.log&apos;</div><div class="line"></div><div class="line">#nginx日志格式为</div><div class="line">        log_format json &apos;&#123;&quot;@timestamp&quot;:&quot;$time_local&quot;,&apos;</div><div class="line">                &apos;&quot;source&quot;:&quot;nginx147&quot;,&apos;</div><div class="line">                &apos;&quot;serverAddr&quot;:&quot;$server_addr&quot;,&apos;</div><div class="line">                &apos;&quot;remoteAddr&quot;:&quot;$remote_addr&quot;,&apos;</div><div class="line">                &apos;&quot;remoteUser&quot;:&quot;$remote_user&quot;,&apos;</div><div class="line">                &apos;&quot;size&quot;:$body_bytes_sent,&apos;</div><div class="line">                &apos;&quot;status&quot;:$status,&apos;</div><div class="line">                &apos;&quot;time&quot;:$request_time,&apos;</div><div class="line">                &apos;&quot;method&quot;:&quot;$request_method&quot;,&apos;</div><div class="line">                &apos;&quot;protocol&quot;:&quot;$server_protocol&quot;,&apos;</div><div class="line">                &apos;&quot;url&quot;:&quot;$scheme://$host$request_uri&quot;,&apos;</div><div class="line">                &apos;&quot;host&quot;:&quot;$http_host&quot;,&apos;</div><div class="line">                &apos;&quot;uri&quot;:&quot;$uri&quot;,&apos;</div><div class="line">                &apos;&quot;referer&quot;:&quot;$http_referer&quot;,&apos;</div><div class="line">                &apos;&quot;xforwarded&quot;:&quot;$http_x_forwarded_for&quot;,&apos;</div><div class="line">                &apos;&quot;agent&quot;:&quot;$http_user_agent&quot;,&apos;</div><div class="line">                &apos;&quot;upsTime&quot;:&quot;$upstream_response_time&quot;,&apos;</div><div class="line">                &apos;&quot;sslPro&quot;:&quot;$ssl_protocol&quot;,&apos;</div><div class="line">                &apos;&quot;sslCip&quot;:&quot;$ssl_cipher&quot;,&apos;</div><div class="line">                &apos;&quot;upsStatus&quot;:&quot;$upstream_status&quot;&#125;&apos;;</div></pre></td></tr></table></figure>
<h5 id="logstash配置"><a href="#logstash配置" class="headerlink" title="logstash配置"></a><b>logstash配置</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div></pre></td><td class="code"><pre><div class="line">[root@localhost ~]# cat /etc/logstash/conf.d/history-logstash.conf </div><div class="line">input &#123;</div><div class="line">    redis &#123;</div><div class="line">        data_type =&gt; &quot;list&quot;  </div><div class="line">        host =&gt; &quot;192.168.90.147&quot;</div><div class="line">        port =&gt; &quot;6379&quot;</div><div class="line">        password =&gt; &quot;password&quot;</div><div class="line">        key  =&gt; &quot;command-log&quot;</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">output &#123;</div><div class="line">       if [fields][log_source] == &quot;command-log&quot; &#123; </div><div class="line">      elasticsearch &#123;</div><div class="line">          hosts   =&gt; [&quot;192.16.90.149:9200&quot;]</div><div class="line">          manage_template =&gt; false</div><div class="line">          action  =&gt; &quot;index&quot;</div><div class="line">          index   =&gt; &quot;command-log-%&#123;+YYYY.MM.dd&#125;&quot;</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">#nginx</div><div class="line">[root@localhost ~]# cat /etc/logstash/conf.d/nginx-logstash.conf </div><div class="line">input &#123;</div><div class="line">    redis &#123;</div><div class="line">        data_type =&gt; &quot;list&quot;  </div><div class="line">        host =&gt; &quot;192.168.90.147&quot;</div><div class="line">        port =&gt; &quot;6379&quot;</div><div class="line">        password =&gt; &quot;password&quot;</div><div class="line">        key  =&gt; &quot;nginx-log&quot;</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line">output &#123;</div><div class="line"></div><div class="line">	 if [fields][log_source] == &quot;nginx-log&quot; &#123;</div><div class="line">        file &#123;</div><div class="line">        path =&gt; &quot;/tmp/logs/nginx-%&#123;+YYYY-MM-dd&#125;.log&quot;</div><div class="line">        &#125;</div><div class="line">   &#125;</div><div class="line">   </div><div class="line">#     elasticsearch &#123;</div><div class="line">#        hosts   =&gt; [&quot;192.168.90.149:9200&quot;]</div><div class="line">#        action  =&gt; &quot;index&quot;</div><div class="line">#        index   =&gt; &quot;nginx-log-%&#123;+YYYY.MM.dd&#125;&quot;</div><div class="line">#    &#125;</div><div class="line"></div><div class="line">#        stdout &#123; codec =&gt; rubydebug &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>备注：<br>logstash安装完后需要执行以下命令，进行service服务安装<br>/usr/share/logstash/bin/system-install /etc/logstash/startup.options sysv</p>
<p>ref<br><a href="https://jkzhao.github.io/2017/10/24/Filebeat%E6%97%A5%E5%BF%97%E6%94%B6%E9%9B%86%E5%99%A8/" target="_blank" rel="external">Filebeat日志收集器</a><br><a href="https://blog.csdn.net/jianblog/article/details/54669203" target="_blank" rel="external">Elastic测试笔记</a><br><a href="https://www.elastic.co/guide/en/beats/filebeat/current/redis-output.html" target="_blank" rel="external">Configure the Redis output</a></p>
<hr>
<p>您的鼓励是我写作最大的动力</p>
<p>俗话说，投资效率是最好的投资。 如果您感觉我的文章质量不错，读后收获很大，预计能为您提高 10% 的工作效率，不妨小额捐助我一下，让我有动力继续写出更多好文章。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;环境介绍&quot;&gt;&lt;a href=&quot;#环境介绍&quot; class=&quot;headerlink&quot; title=&quot;环境介绍&quot;&gt;&lt;/a&gt;&lt;b&gt;环境介绍&lt;/b&gt;&lt;/h5&gt;&lt;p&gt;系统为Centos6.8,相关软件版本如下：&lt;br&gt;filebeat-6.2.3&lt;br&gt;redis-3.0.7
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>redis性能分析</title>
    <link href="https://t1ger.github.io/2018/04/10/redis%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90/"/>
    <id>https://t1ger.github.io/2018/04/10/redis性能分析/</id>
    <published>2018-04-10T10:11:17.000Z</published>
    <updated>2018-04-10T10:16:03.106Z</updated>
    
    <content type="html"><![CDATA[<h5 id="前言"><a href="#前言" class="headerlink" title="前言"></a><b>前言</b></h5><p>redis性能分析常见的有以下几个方面：</p>
<ul>
<li>redis slowlog分析</li>
<li>SCAN，SSCAN，HSCAN和ZSCAN命令的使用方法</li>
<li>redis是否受到系统使用swap</li>
<li>redis watchdog定位延时</li>
<li>关于redis的延时监控框架,可参考<a href="https://redis.io/topics/latency-monitor" target="_blank" rel="external">官网资料</a><br>下面我们分别从这几个方面来介绍</li>
</ul>
<h5 id="redis-slowlog分析"><a href="#redis-slowlog分析" class="headerlink" title="redis slowlog分析"></a><b>redis slowlog分析</b></h5><ol>
<li><p>慢查询设置<br>在Redis中有两种修改配置的方法,一种是修改配置文件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">slowlog-log-slower-than 10000  #查询时间超过10ms的会被记录  </div><div class="line">slowlog-max-len 128            # 最多记录128个慢查询</div></pre></td></tr></table></figure>
<p> 另一种是使用config set命令动态修改.例如下面使用config set命令将slowlog-log-slower-than设置为20000微妙.slowlog-max-len设置为1024</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">config set slowlog-log-slower-than 20000</div><div class="line">config set slowlog-max-len 1024</div><div class="line">config rewrite</div></pre></td></tr></table></figure>
<p> 如果需要将Redis将配置持久化到本地配置文件,要执行config rewrite命令,如果slowlog-log-slower-than=0会记录所有命令,slowlog-log-slower-than&lt;0对于任何命令都不会进行记录</p>
</li>
<li><p>获取慢查询日志<br>slowlog get [n]</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">127.0.0.1:6379&gt; slowlog get 15</div><div class="line"> 1) 1) (integer) 79674  #slowlog的唯一编号 </div><div class="line">    2) (integer) 1523350838 #此次slowlog事件的发生时间  </div><div class="line">    3) (integer) 2987577    #耗时,以微秒为单位</div><div class="line">    4) 1) &quot;KEYS&quot;</div><div class="line">       2) &quot;mid_cache_app_list_*&quot;</div></pre></td></tr></table></figure>
</li>
<li><p>获取慢查询日志列表当前长度</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">127.0.0.1:6379&gt; slowlog len</div><div class="line">(integer) 128</div></pre></td></tr></table></figure>
</li>
<li><p>慢查询日志重置</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">slowlog reset</div></pre></td></tr></table></figure>
</li>
<li><p>建议：<br>slowlog-max-len 建议线上设置为1000以上<br>slowlog-log-slower-than对高流量场景应该设置在1毫秒以上<br>慢查询只记录命令的执行时间,并不包括命令排队和网络传输时间.因此客户端执行命令的时间会大于命令的实际执行时间.因为命令执行排队机制,慢查询会导致其他命令级联阻塞,因此客户端出现请求超时时,需要检查该时间点是否有对应的慢查询,从而分析是否为慢查询导致的命令级联阻塞.</p>
</li>
</ol>
<h5 id="SCAN，SSCAN，HSCAN和ZSCAN命令的使用方法"><a href="#SCAN，SSCAN，HSCAN和ZSCAN命令的使用方法" class="headerlink" title="SCAN，SSCAN，HSCAN和ZSCAN命令的使用方法"></a><b>SCAN，SSCAN，HSCAN和ZSCAN命令的使用方法</b></h5><ol>
<li><p>SCAN是基于游标的迭代器。每次调用命令时，服务器返回一个更新的游标，用户需要在下一次调用中用作游标参数。当游标设置为0时，迭代开始，并且当服务器返回的游标为0时终止迭代<br>开始游标值为0的迭代，并调用SCAN，直到返回的游标再次为0，称为完全迭代</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">127.0.0.1:6379&gt; scan 0  </div><div class="line"></div><div class="line">127.0.0.1:6379&gt; scan 0 count 20 //指定输出的数量</div><div class="line">127.0.0.1:6379&gt; scan 0 match *mid_sent*   //类似于keys命令按模式匹配</div></pre></td></tr></table></figure>
</li>
<li><p>sscan查询sets集合的方法：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">redis 127.0.0.1:6379&gt; sadd setone 1 2 3 foo foobar feelsgood  </div><div class="line">(integer) 6  </div><div class="line">redis 127.0.0.1:6379&gt; sscan setone 0 match f*  </div><div class="line">1) &quot;0&quot;  </div><div class="line">2) 1) &quot;foo&quot;  </div><div class="line">   2) &quot;feelsgood&quot;  </div><div class="line">   3) &quot;foobar&quot;</div></pre></td></tr></table></figure>
</li>
<li><p>hscan查询hash集合的方法：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">redis 127.0.0.1:6379&gt; hmset hash name Tom age 35  </div><div class="line">OK  </div><div class="line">redis 127.0.0.1:6379&gt; hscan hash 0  </div><div class="line">1) &quot;0&quot;  </div><div class="line">2) 1) &quot;name&quot;  </div><div class="line">   2) &quot;Tom&quot;  </div><div class="line">   3) &quot;age&quot;  </div><div class="line">   4) &quot;35&quot;</div></pre></td></tr></table></figure>
</li>
<li><p>Linux内核启用了透明巨页功能时，Redis在使用fork调用之后会产生大的延迟代价，以便在磁盘进行数据持久化</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled</div></pre></td></tr></table></figure>
<p> 需重启redis才能生效</p>
</li>
</ol>
<h5 id="redis是否受到系统使用swap"><a href="#redis是否受到系统使用swap" class="headerlink" title="redis是否受到系统使用swap"></a><b>redis是否受到系统使用swap</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">查找redis进程id：  </div><div class="line">redis-cli -p 6319 info|grep process_id  </div><div class="line">process_id:9213  </div><div class="line">查看redis进程的内存使用信息：  </div><div class="line">cd /proc/9213 </div><div class="line">查看该进程使用swap分区的统计信息，以不使用或只有少量的4kB为佳：  </div><div class="line">cat smaps | grep &apos;Swap:&apos;  </div><div class="line">同时打印出内存映射和swap使用信息：查看那些较大的内存消耗是否引发了大的swap使用  </div><div class="line">cat smaps | egrep &apos;^(Swap:Size)&apos;</div></pre></td></tr></table></figure>
<h5 id="redis-watchdog定位延时"><a href="#redis-watchdog定位延时" class="headerlink" title="redis watchdog定位延时"></a><b>redis watchdog定位延时</b></h5><p>注意： 实验功能，请确保redis数据已备份,会对redis服务性能产生影响<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">Redis software watchdog  </div><div class="line">该功能只能动态启用，使用以下命令：  </div><div class="line">CONFIG SET watchdog-period 500  </div><div class="line">注：redis会开始频繁监控自身的延时问题，并把问题输出到日志文件中去。  </div><div class="line">  </div><div class="line">关闭watchdog：  </div><div class="line">CONFIG SET watchdog-period 0</div></pre></td></tr></table></figure></p>
<h5 id="Redis-latency-monitoring-framework"><a href="#Redis-latency-monitoring-framework" class="headerlink" title="Redis latency monitoring framework"></a><b>Redis latency monitoring framework</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">CONFIG SET latency-monitor-threshold 100</div></pre></td></tr></table></figure>
<p>默认情况下，阈值设置为0，即禁用redis监控。实际上启用该监控功能，对redis所增加的成本很少.</p>
<p>LATENCY命令的使用方法</p>
<ol>
<li><p>查看最新的延时事件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">127.0.0.1:6379&gt; latency latest  </div><div class="line">1) 1) &quot;command&quot;     #event name  </div><div class="line">   2) (integer) 1480865648     #发生时间  </div><div class="line">   3) (integer) 207     #耗时，毫秒  </div><div class="line">   4) (integer) 239     #从redis启动或上次latency reset以来，这种事件的最大延时记录</div></pre></td></tr></table></figure>
</li>
<li><p>查看延时事件的历史信息</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">127.0.0.1:6379&gt; latency history command  </div><div class="line">  1) 1) (integer) 1480865710  </div><div class="line">     2) (integer) 207  </div><div class="line">  2) 1) (integer) 1480865711  </div><div class="line">     2) (integer) 217</div></pre></td></tr></table></figure>
</li>
<li><p>LATENCY DOCTOR<br>延时事件统计信息的智能分析与建议</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">127.0.0.1:6379&gt; latency doctor  </div><div class="line">Dave, I have observed latency spikes in this Redis instance.  </div><div class="line">You don&apos;t mind talking about it, do you Dave?  </div><div class="line">1. command: 5 latency spikes (average 300ms, mean deviation 120ms,  </div><div class="line">  period 73.40 sec). Worst all time event 500ms.  </div><div class="line">I have a few advices for you:  </div><div class="line">- Your current Slow Log configuration only logs events that are  </div><div class="line">  slower than your configured latency monitor threshold. Please  </div><div class="line">  use &apos;CONFIG SET slowlog-log-slower-than 1000&apos;.  </div><div class="line">- Check your Slow Log to understand what are the commands you are  </div><div class="line">  running which are too slow to execute. Please check  </div><div class="line">  http://redis.io/commands/slowlog for more information.</div></pre></td></tr></table></figure>
</li>
</ol>
<hr>
<p>您的鼓励是我写作最大的动力</p>
<p>俗话说，投资效率是最好的投资。 如果您感觉我的文章质量不错，读后收获很大，预计能为您提高 10% 的工作效率，不妨小额捐助我一下，让我有动力继续写出更多好文章。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;&lt;b&gt;前言&lt;/b&gt;&lt;/h5&gt;&lt;p&gt;redis性能分析常见的有以下几个方面：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;redis slowlog分析&lt;/li&gt;
&lt;li&gt;SCAN
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>spark develop env on eclipse</title>
    <link href="https://t1ger.github.io/2018/03/27/spark-develop-env-on-eclipse/"/>
    <id>https://t1ger.github.io/2018/03/27/spark-develop-env-on-eclipse/</id>
    <published>2018-03-27T07:51:04.000Z</published>
    <updated>2018-05-10T07:31:57.728Z</updated>
    
    <content type="html"><![CDATA[<h5 id="开发准备"><a href="#开发准备" class="headerlink" title="开发准备"></a><b>开发准备</b></h5><p>　　jdk1.8.45<br>　　spark-2.1.1-bin-hadoop2.7<br>　　CentOS系统<br>　　spark安装环境<br>　　hadoop-2.7.2<br>　　Hadoop安装环境</p>
<h5 id="开发环境配置"><a href="#开发环境配置" class="headerlink" title="开发环境配置"></a><b>开发环境配置</b></h5><ol>
<li>这里下载<a href="http://downloads.typesafe.com/scalaide-pack/4.7.0-vfinal-oxygen-212-20170929/scala-SDK-4.7.0-vfinal-2.12-win32.win32.x86_64.zip" target="_blank" rel="external">ScaleIDE for Win64</a>,其他可选对应版本</li>
<li>解压后，直接运行里边的eclipse</li>
<li>建立scala project,并创建scala类 WordCount</li>
<li><p>右键工程属性，添加spark-2.1.1-bin-hadoop2.7下面所有的库，可自定义库放进来：Java Build Path =&gt; Libraries =&gt; Add library,这里有个小知识:<br>add external jars  = 增加工程外部的包<br>add jars = 增加工程内包<br>add library = 增加一个库 // 是一些已经定义好的jar的集合<br>add class folder = 添加类的目录，是指本Eclipse范围中的，在工程列表下选取接口<br>区别:通过“add jar” 和“add external jars”添加的jar包作为程序的一部分被打包到最终的程序中。通过“User Libraries”添加的jar包不是。</p>
</li>
<li><p>编辑代码如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">package cn.spark.study.core</div><div class="line">import org.apache.spark.SparkConf</div><div class="line">import org.apache.spark.SparkContext </div><div class="line"></div><div class="line">object WordCount &#123;</div><div class="line">  </div><div class="line">    def main(args: Array[String]) &#123;</div><div class="line">      val conf= new SparkConf()</div><div class="line">            .setAppName(&quot;WordCount&quot;)</div><div class="line">            .setMaster(&quot;local&quot;)</div><div class="line">            </div><div class="line">      val sc = new SparkContext(conf)</div><div class="line">      val lines = sc.textFile(&quot;C:\\Users\\Administrator\\Desktop\\spark.txt&quot;)</div><div class="line">      </div><div class="line">      val words = lines.flatMap&#123;line =&gt; line.split(&quot; &quot;)&#125;</div><div class="line">      val pairs = words.map&#123;word =&gt; (word,1)&#125;</div><div class="line">      </div><div class="line">      val wordCounts = pairs.reduceByKey(_+_)</div><div class="line">      wordCounts.foreach(wordNumberPair =&gt; println(wordNumberPair._1 + &quot; : &quot; + wordNumberPair._2 + &quot;times .&quot;))</div><div class="line">      sc.stop()  </div><div class="line">            </div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
</li>
<li><p>右键，导出jar文件</p>
</li>
<li><p>在spark部署路径执行</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">/usr/local/spark/bin/spark-submit \</div><div class="line">  --class cn.spark.study.core.WordCount \</div><div class="line">  --num-executors 1 \</div><div class="line">  --executor-memory 1g \</div><div class="line">  /spark/scala/spark.study.scala.jar \</div></pre></td></tr></table></figure>
</li>
<li><p>参数解析：<br>可以执行./spark-submit –help获得帮助</p>
</li>
</ol>
<hr>
<p>您的鼓励是我写作最大的动力</p>
<p>俗话说，投资效率是最好的投资。 如果您感觉我的文章质量不错，读后收获很大，预计能为您提高 10% 的工作效率，不妨小额捐助我一下，让我有动力继续写出更多好文章。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;开发准备&quot;&gt;&lt;a href=&quot;#开发准备&quot; class=&quot;headerlink&quot; title=&quot;开发准备&quot;&gt;&lt;/a&gt;&lt;b&gt;开发准备&lt;/b&gt;&lt;/h5&gt;&lt;p&gt;　　jdk1.8.45&lt;br&gt;　　spark-2.1.1-bin-hadoop2.7&lt;br&gt;　　CentOS系
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>How to run Etherenum on CentOS7</title>
    <link href="https://t1ger.github.io/2018/02/05/How-to-run-Etherenum-on-CentOS7/"/>
    <id>https://t1ger.github.io/2018/02/05/How-to-run-Etherenum-on-CentOS7/</id>
    <published>2018-02-05T02:24:40.000Z</published>
    <updated>2018-02-05T06:47:03.527Z</updated>
    
    <content type="html"><![CDATA[<h5 id="以太坊客户端"><a href="#以太坊客户端" class="headerlink" title="以太坊客户端"></a><b>以太坊客户端</b></h5><p>Etherenum主流的客户端实现有以下几种，分别是C++, Go, Python，分别对应cpp-ethereum, go-ethereum, pyethapp.<br>C++实现称为Eth,Go语言的实现被称为Geth,Python的实现被称为Pyethapp.客户端是指一种接入Ethereum网络的节点并且与其发生交互和更新blockchain状态</p>
<p>其中最常用的有 Go 语言实现的 go-ethereum 客户端 Geth，支持接入以太坊网络并成为一个完整节点，也可作为一个 HTTP-RPC 服务器对外提供 JSON-RPC 接口</p>
<h5 id="Geth安装和配置"><a href="#Geth安装和配置" class="headerlink" title="Geth安装和配置"></a><b>Geth安装和配置</b></h5><p>Go语言的安装和配置略，Geth可以编译安装，也可以直接下载二进制安装包,参考<a href="https://geth.ethereum.org/downloads/" target="_blank" rel="external">这里</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">git clone https://github.com/ethereum/go-ethereum</div><div class="line">cd go-ethereum</div><div class="line">make geth</div></pre></td></tr></table></figure></p>
<p>安装完成后，可以使用 geth version 命令查看是否安装成功。记得把生成的 geth 加入到系统的环境变量中</p>
<h5 id="安装-Solidity-编译器"><a href="#安装-Solidity-编译器" class="headerlink" title="安装 Solidity 编译器"></a><b>安装 Solidity 编译器</b></h5><p>Solidity 编译器也有多种方法安装，参照<a href="https://solidity.readthedocs.io/en/latest/installing-solidity.html" target="_blank" rel="external">这里</a><br>官方推荐使用基于浏览器的 IDE 环境:<a href="https://remix.ethereum.org" target="_blank" rel="external">Remix</a></p>
<h5 id="私有链搭建"><a href="#私有链搭建" class="headerlink" title="私有链搭建"></a><b>私有链搭建</b></h5><ul>
<li><p>配置初始状态<br>运行以太坊私有链，需要定义自己的创世区块，创世区块信息写在一个 JSON 格式的配置文件中</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">[root@localhost]#cat genesis.json </div><div class="line">&#123;</div><div class="line">        &quot;nonce&quot;: &quot;0x0000000000000042&quot;,</div><div class="line">        &quot;timestamp&quot;: &quot;0x00&quot;,</div><div class="line">        &quot;parentHash&quot;: &quot;0x0000000000000000000000000000000000000000000000000000000000000000&quot;,</div><div class="line">        &quot;extraData&quot;: &quot;0x00&quot;,</div><div class="line">        &quot;gasLimit&quot;: &quot;0x8000000&quot;,</div><div class="line">        &quot;difficulty&quot;: &quot;0x4000&quot;,</div><div class="line">        &quot;mixhash&quot;: &quot;0x0000000000000000000000000000000000000000000000000000000000000000&quot;,</div><div class="line">        &quot;coinbase&quot;: &quot;0x3333333333333333333333333333333333333333&quot;,</div><div class="line">        </div><div class="line">        &quot;config&quot;: &#123;</div><div class="line">                &quot;chainId&quot;: 1984,</div><div class="line">                &quot;homesteadBlock&quot;: 0,</div><div class="line">                &quot;eip155Block&quot;: 0,</div><div class="line">                &quot;eip158Block&quot;: 0</div><div class="line">        &#125;,</div><div class="line">        </div><div class="line">        &quot;alloc&quot;:&#123;&#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">#初始化创世区块</div><div class="line">[root@localhost]#geth --datadir &quot;/root/.ethereum&quot; init genesis.json </div><div class="line">I0131 18:57:50.858609 ethdb/database.go:83] Alloted 16MB cache and 16 file handles to /root/.ethereum/chaindata</div><div class="line">I0131 18:57:50.865414 cmd/geth/main.go:299] successfully wrote genesis block and/or chain rule set: f2ebfaadd3ae79075cc9485eb5ab634c9927504736c60dc88d571fb85d9f6493</div></pre></td></tr></table></figure>
<p>  chainID 指定了独立的区块链网络 ID。网络 ID 在连接到其他节点的时候会用到，以太坊公网的网络 ID 是 1，为了不与公有链网络冲突，运行私有链节点的时候要指定自己的网络 ID。不同 ID 网络的节点无法相互连接。配置文件还对当前挖矿难度 difficulty、区块 Gas 消耗限制 gasLimit 等参数进行了设置<br>打开此节点的命令控制台console，并新建账户</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">#启动私有链节点</div><div class="line">geth --datadir &quot;/root/.ethereum&quot; --port 30002 --nodiscover  console</div><div class="line">#新建用户</div><div class="line">&gt;personal.newAccount(&quot;passward&quot;)</div><div class="line">#开始挖矿</div><div class="line">&gt;miner.start(1)</div><div class="line">true</div><div class="line">#停止挖矿</div><div class="line">&gt;miner.stop()</div><div class="line">true</div><div class="line">#查看账户</div><div class="line">&gt; eth.accounts</div><div class="line">#查看账户余额</div><div class="line">&gt;eth.getBalance(eth.accounts[0])</div></pre></td></tr></table></figure>
<p>  发送交易</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">&gt;  eth.getBalance(eth.accounts[0])</div><div class="line">320000000000000300000</div><div class="line">&gt; eth.getBalance(eth.accounts[1])</div><div class="line">0</div><div class="line">&gt;  personal.unlockAccount(eth.accounts[0])</div><div class="line">Unlock account 0x30a42e0da52f20154ce2b966a53a81099f048e73</div><div class="line">Passphrase: </div><div class="line">true</div><div class="line">&gt;  amount = web3.toWei(5,&apos;ether&apos;)</div><div class="line">&quot;5000000000000000000&quot;</div><div class="line">&gt; eth.sendTransaction(&#123;from:eth.accounts[0],to:eth.accounts[1],value:amount&#125;)</div><div class="line">I0205 11:51:11.752411 eth/api.go:1185] Tx(0x318cf8d6f86f0f294eb927af3019cede420990beac1cf72046d03d454ff48b88) to: 0xae370f3b2af53f6ba282a76bbe6956d443bc8d79</div><div class="line">&quot;0x318cf8d6f86f0f294eb927af3019cede420990beac1cf72046d03d454ff48b88&quot;</div><div class="line"></div><div class="line">#此时如果没有挖矿，用 txpool.status 命令可以看到本地交易池中有一个待确认的交易，可以使用 eth.getBlock(&quot;pending&quot;, true).transactions 查看当前待确认交易</div><div class="line"></div><div class="line">&gt; miner.start(1);admin.sleepBlocks(1);miner.stop();</div><div class="line"></div><div class="line">#新区块挖出后，挖矿结束，查看账户 1 的余额，已经收到了账户 0 的以太币：</div><div class="line">&gt; web3.fromWei(eth.getBalance(eth.accounts[1]),&apos;ether&apos;)</div><div class="line">5</div></pre></td></tr></table></figure>
<p>  查看交易和区块</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line">#查看当前区块总数：</div><div class="line">&gt; eth.blockNumber</div><div class="line">66</div><div class="line">#通过交易 Hash 查看交易（Hash 值包含在上面交易返回值中）</div><div class="line">&gt; eth.getTransaction(&quot;0x318cf8d6f86f0f294eb927af3019cede420990beac1cf72046d03d454ff48b88&quot;)</div><div class="line">&#123;</div><div class="line">  blockHash: &quot;0x45c8732599e45791c22a4c1d2278387eff05e6f062f8f1871912bd4fbee4787f&quot;,</div><div class="line">  blockNumber: 65,</div><div class="line">  from: &quot;0x30a42e0da52f20154ce2b966a53a81099f048e73&quot;,</div><div class="line">  gas: 90000,</div><div class="line">  gasPrice: 20000000000,</div><div class="line">  hash: &quot;0x318cf8d6f86f0f294eb927af3019cede420990beac1cf72046d03d454ff48b88&quot;,</div><div class="line">  input: &quot;0x&quot;,</div><div class="line">  nonce: 0,</div><div class="line">  to: &quot;0xae370f3b2af53f6ba282a76bbe6956d443bc8d79&quot;,</div><div class="line">  transactionIndex: 0,</div><div class="line">  value: 5000000000000000000</div><div class="line">&#125;</div><div class="line"></div><div class="line">#通过区块号查看区块：</div><div class="line">&gt; eth.getBlock(66)</div><div class="line">&#123;</div><div class="line">  difficulty: 131072,</div><div class="line">  extraData: &quot;0xd783010412844765746887676f312e372e31856c696e7578&quot;,</div><div class="line">  gasLimit: 125836029,</div><div class="line">  gasUsed: 0,</div><div class="line">  hash: &quot;0x7398a4794f1a6c24fa193b3a45e96c9b8925047a4c4ffb5112ffee182e8e05ba&quot;,</div><div class="line">  logsBloom: &quot;0x00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000&quot;,</div><div class="line">  miner: &quot;0x30a42e0da52f20154ce2b966a53a81099f048e73&quot;,</div><div class="line">  nonce: &quot;0x2d4f36efb89667d8&quot;,</div><div class="line">  number: 66,</div><div class="line">  parentHash: &quot;0x45c8732599e45791c22a4c1d2278387eff05e6f062f8f1871912bd4fbee4787f&quot;,</div><div class="line">  receiptRoot: &quot;0x56e81f171bcc55a6ff8345e692c0f86e5b48e01b996cadc001622fb5e363b421&quot;,</div><div class="line">  sha3Uncles: &quot;0x1dcc4de8dec75d7aab85b567b6ccd41ad312451b948a7413f0a142fd40d49347&quot;,</div><div class="line">  size: 536,</div><div class="line">  stateRoot: &quot;0x50cd8603d98fe157cabc1d596e7a7926231e8dee792cabd199f263dc0c5691a5&quot;,</div><div class="line">  timestamp: 1517802759,</div><div class="line">  totalDifficulty: 8792593,</div><div class="line">  transactions: [],</div><div class="line">  transactionsRoot: &quot;0x56e81f171bcc55a6ff8345e692c0f86e5b48e01b996cadc001622fb5e363b421&quot;,</div><div class="line">  uncles: []</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>  连接到其他节点</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">#通过 admin.addPeer() 方法连接到其他节点，两个节点要要指定相同的 chainID</div><div class="line">查看其中一个 enode 信息：</div><div class="line">&gt; admin.nodeInfo.enode</div><div class="line">&quot;enode://34753757021f60aac9ad5402344ae353b9c431c3b60c8e671a753639dd5aef35bb180b8d056a5f3b1ac46fd5cda0163f88eff8e1856a1b3982cc576df81e295a@[::]:30002?discport=0&quot;</div><div class="line"></div><div class="line">然后在另外节点的 JavaScript console 中执行 admin.addPeer()，就可以连接：</div><div class="line"> admin.addPeer(&quot;enode://34753757021f60aac9ad5402344ae353b9c431c3b60c8e671a753639dd5aef35bb180b8d056a5f3b1ac46fd5cda0163f88eff8e1856a1b3982cc576df81e295a@[::]:30002?discport=0&quot;)</div><div class="line"></div><div class="line">#addPeer() 的参数就是节点二的 enode 信息，注意要把 enode 中的 [::] 替换成节点二的 IP 地址。</div><div class="line">连接成功后，节点就会开始同步另一个节点的区块，同步完成后，</div><div class="line">任意一个节点开始挖矿，另一个节点会自动同步区块，向任意一个节点发送交易，另一个节点也会收到该笔交易</div></pre></td></tr></table></figure>
<p>  除了上面的方法，也可以在启动节点的时候指定 –bootnodes 选项连接到其他节点</p>
</li>
</ul>
<h5 id="智能合约操作"><a href="#智能合约操作" class="headerlink" title="智能合约操作"></a><b>智能合约操作</b></h5><ul>
<li><p>创建和编译智能合约<br>新建一个 Solidity 智能合约文件，命名为 test.sol，该合约包含一个方法 multiply()，将输入的两个数相乘后输出：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">pragma solidity ^0.4.0;</div><div class="line">contract Test</div><div class="line">&#123;</div><div class="line">    function multiply(uint a, uint b) returns (uint)</div><div class="line">    &#123;</div><div class="line">        return a * b;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>  编译智能合约，获得编译后的 EVM 二进制码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">solc --bin test.sol</div></pre></td></tr></table></figure>
<p>  再用 solc 获取智能合约的 JSON ABI（Application Binary Interface），其中指定了合约接口，包括可调用的合约方法、变量、事件等</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">solc --abi test.sol</div></pre></td></tr></table></figure>
<p>  回到 Geth 的控制台，用变量 code 和 abi 记录上面两个值，注意在 code 前加上 0x 前缀：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt; code = &quot;0x6060604052341561000f57600080fd5b5b60b48061001e6000396000f30060606040526000357c0100000000000000000000000000000000000000000000000000000000900463ffffffff168063165c4a1614603d575b600080fd5b3415604757600080fd5b60646004808035906020019091908035906020019091905050607a565b6040518082815260200191505060405180910390f35b600081830290505b929150505600a165627a7a72305820b494a4b3879b3810accf64d4cc3e1be55f2f4a86f49590b8a9b8d7009090a5d30029&quot;</div><div class="line">&gt; abi = [&#123;&quot;constant&quot;:false,&quot;inputs&quot;:[&#123;&quot;name&quot;:&quot;a&quot;,&quot;type&quot;:&quot;uint256&quot;&#125;,&#123;&quot;name&quot;:&quot;b&quot;,&quot;type&quot;:&quot;uint256&quot;&#125;],&quot;name&quot;:&quot;multiply&quot;,&quot;outputs&quot;:[&#123;&quot;name&quot;:&quot;&quot;,&quot;type&quot;:&quot;uint256&quot;&#125;],&quot;payable&quot;:false,&quot;stateMutability&quot;:&quot;nonpayable&quot;,&quot;type&quot;:&quot;function&quot;&#125;]</div></pre></td></tr></table></figure>
</li>
<li><p>部署智能合约<br>这里使用账户 0 来部署合约，首先解锁账户：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&gt;  personal.unlockAccount(eth.accounts[0])</div><div class="line">Unlock account 0x30a42e0da52f20154ce2b966a53a81099f048e73</div><div class="line">Passphrase: </div><div class="line">true</div></pre></td></tr></table></figure>
</li>
<li><p>发送部署合约的交易</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">&gt; myContract = eth.contract(abi)</div><div class="line">...</div><div class="line">&gt; contract = myContract.new(&#123;from:eth.accounts[0],data:code,gas:1000000&#125;)</div><div class="line"></div><div class="line">INFO [09-12|08:05:19] Submitted contract creation              fullhash=0x0a7dfa9cac7ef836a72ed1d5bbfa65c0220347cde4efb067a0b03b15fb70bce1 contract=0x7cbe4019e993f9922b8233502d94890099ee59e6</div><div class="line">&#123;</div><div class="line">  abi: [&#123;</div><div class="line">      constant: false,</div><div class="line">      inputs: [&#123;...&#125;, &#123;...&#125;],</div><div class="line">      name: &quot;multiply&quot;,</div><div class="line">      outputs: [&#123;...&#125;],</div><div class="line">      payable: false,</div><div class="line">      stateMutability: &quot;nonpayable&quot;,</div><div class="line">      type: &quot;function&quot;</div><div class="line">  &#125;],</div><div class="line">  address: undefined,</div><div class="line">  transactionHash: &quot;0x0a7dfa9cac7ef836a72ed1d5bbfa65c0220347cde4efb067a0b03b15fb70bce1&quot;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>  此时如果没有挖矿，用 txpool.status 命令可以看到本地交易池中有一个待确认的交易。可以查看当前待确认的交易，使用 miner.start() 命令开始挖矿，一段时间后交易会被确认，随新区块进入区块链</p>
</li>
<li><p>调用智能合约</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&gt; contract.multiply.sendTransaction(2, 4, &#123;from:eth.accounts[0]&#125;)</div></pre></td></tr></table></figure>
<p>  如果只是本地运行该方法查看返回结果，可以采用如下方式：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt; contract.multiply.call(2，4)</div><div class="line">8</div></pre></td></tr></table></figure>
</li>
</ul>
<h5 id="安装ethereum钱包"><a href="#安装ethereum钱包" class="headerlink" title="安装ethereum钱包"></a><b>安装ethereum钱包</b></h5><p>钱包下载地址，参考<a href="https://github.com/ethereum/mist/releases" target="_blank" rel="external">这里</a><br>钱包在启动时默认在本机查找的IPC路径,钱包启动后能自动识别到你的私有链<br>OS的默认查找路径如下,具体可查看<a href="https://github.com/ethereum/mist/blob/master/modules/settings.js#L248" target="_blank" rel="external">这里</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">windows: .\pipe\geth.ipc</div><div class="line">linux: /.ethereum/geth.ipc</div><div class="line">freebsd: /.ethereum/geth.ipc</div><div class="line">sunos: /.ethereum/geth.ipc</div></pre></td></tr></table></figure></p>
<p>ref<br><a href="https://github.com/ethereum/go-ethereum/wiki/Private-network" target="_blank" rel="external">Private network</a><br><a href="https://github.com/ethereum/cpp-ethereum" target="_blank" rel="external">ethereum/cpp-ethereum</a><br><a href="https://github.com/ethereum/go-ethereum" target="_blank" rel="external">Go Ethereum</a><br><a href="http://www.ethdocs.org/en/latest/ethereum-clients/cpp-ethereum/installing-binaries/index.html" target="_blank" rel="external">Installing binaries</a><br><a href="https://g2ex.github.io/2017/09/12/ethereum-guidance/" target="_blank" rel="external">以太坊私有链搭建指南</a><br><a href="https://github.com/xiaoping378/blog/blob/master/posts/%E4%BB%A5%E5%A4%AA%E5%9D%8A-%E7%A7%81%E6%9C%89%E9%93%BE%E6%90%AD%E5%BB%BA%E5%88%9D%E6%AD%A5%E5%AE%9E%E8%B7%B5.md" target="_blank" rel="external">利用puppeth搭建POA共识的以太坊私链网络</a></p>
<hr>
<p>您的鼓励是我写作最大的动力</p>
<p>俗话说，投资效率是最好的投资。 如果您感觉我的文章质量不错，读后收获很大，预计能为您提高 10% 的工作效率，不妨小额捐助我一下，让我有动力继续写出更多好文章。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;以太坊客户端&quot;&gt;&lt;a href=&quot;#以太坊客户端&quot; class=&quot;headerlink&quot; title=&quot;以太坊客户端&quot;&gt;&lt;/a&gt;&lt;b&gt;以太坊客户端&lt;/b&gt;&lt;/h5&gt;&lt;p&gt;Etherenum主流的客户端实现有以下几种，分别是C++, Go, Python，分别对应c
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>高可用的MongoDB集群</title>
    <link href="https://t1ger.github.io/2018/01/23/%E9%AB%98%E5%8F%AF%E7%94%A8%E7%9A%84MongoDB%E9%9B%86%E7%BE%A4/"/>
    <id>https://t1ger.github.io/2018/01/23/高可用的MongoDB集群/</id>
    <published>2018-01-23T01:53:10.000Z</published>
    <updated>2018-01-25T04:06:31.446Z</updated>
    
    <content type="html"><![CDATA[<h5 id="前言"><a href="#前言" class="headerlink" title="前言"></a><b>前言</b></h5><p>MongoDB是一个介于关系数据库和非关系数据库之间的产品，适合存储对象及JSON形式的数据。支持丰富的查询方式，几乎可以实现类似关系数据库单表查询的绝大部分功能，而且还支持对数据建立索引。</p>
<p>mongo适用的场景：</p>
<ul>
<li>网站数据:Mongo 非常适合实时的插入,更新与查询,并具备网站实时数据存储所需的复制及高度伸缩性</li>
<li>缓存:由于性能很高,Mongo 也适合作为信息基础设施的缓存层</li>
<li>高伸缩性的场景:Mongo非常适合由数十或数百台服务器组成的数据库</li>
<li>用于对象及JSON数据的存储:Mongo的BSON数据格式非常适合文档格式化的存储及查询</li>
</ul>
<p>MongDB不适合的场景:</p>
<ul>
<li>高度事务性的系统:例如银行或会计系统。</li>
<li>传统的商业智能应用:针对特定问题的 BI 数据库会对产生高度优化的查询方式。对于此类应用,数据仓库可能时更适合的选择(如Hadoop套件中的Hive)</li>
</ul>
<h5 id="MongDB-概念"><a href="#MongDB-概念" class="headerlink" title="MongDB 概念"></a><b>MongDB 概念</b></h5><p>首先了解几个概念：路由，分片、副本集、配置服务器等<br>mongodb支持数据的分布式存储，将collection作数据分片，减少每个节点的数据负载。<br>每个节点可以位于不同的物理机器上，一个简单的sharding集群如下图所示（引用自mongodb官网）<br><img src="https://docs.mongodb.com/manual/_images/sharded-cluster-production-architecture.bakedsvg.svg" alt="mongdb"><br>从图中可以看到有四个组件：mongos、config server、shard、replica set。</p>
<p>mongos，数据库集群请求的入口，所有的请求都通过mongos进行协调，不需要在应用程序添加一个路由选择器，mongos自己就是一个请求分发中心，它负责把对应的数据请求请求转发到对应的shard服务器上。在生产环境通常有多mongos作为请求的入口，防止其中一个挂掉所有的mongodb请求都没有办法操作。</p>
<p>config server，顾名思义为配置服务器，存储所有数据库元信息（路由、分片）的配置。mongos本身没有物理存储分片服务器和数据路由信息，只是缓存在内存里，配置服务器则实际存储这些数据。mongos第一次启动或者关掉重启就会从 config server 加载配置信息，以后如果配置服务器信息变化会通知到所有的 mongos 更新自己的状态，这样 mongos 就能继续准确路由。在生产环境通常有多个 config server 配置服务器，因为它存储了分片路由的元数据，防止数据丢失！</p>
<p>shard，分片（sharding）是指将数据库拆分，将其分散在不同的机器上的过程。将数据分散到不同的机器上，不需要功能强大的服务器就可以存储更多的数据和处理更大的负载。基本思想就是将集合切成小块，这些块分散到若干片里，每个片只负责总数据的一部分，最后通过一个均衡器来对各个分片进行均衡（数据迁移）。</p>
<p>replica set，中文翻译副本集，其实就是shard的备份，防止shard挂掉之后数据丢失。复制提供了数据的冗余备份，并在多个服务器上存储数据副本，提高了数据的可用性， 并可以保证数据的安全性。</p>
<p>仲裁者（Arbiter），是复制集中的一个MongoDB实例，它并不保存数据。仲裁节点使用最小的资源并且不要求硬件设备，不能将Arbiter部署在同一个数据集节点中，可以部署在其他应用服务器或者监视服务器中，也可部署在单独的虚拟机中。为了确保复制集中有奇数的投票成员（包括primary），需要添加仲裁节点做为投票，否则primary不能运行时不会自动切换primary。</p>
<p>简单了解之后，我们可以这样总结一下，应用请求mongos来操作mongodb的增删改查，配置服务器存储数据库元信息，并且和mongos做同步，数据最终存入在shard（分片）上，为了防止数据丢失同步在副本集中存储了一份，仲裁在数据存储到分片的时候决定存储到哪个节点。</p>
<h5 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a><b>环境准备</b></h5><p>九台测试服务器，操作系统centos6.8， MongoDB 3.6</p>
<p>服务器角色分配<br>192.168.1.100 config server<br>192.168.1.101 config server<br>192.168.1.102 config server</p>
<p>192.168.1.103 shard server1<br>192.168.1.104 shard server1 副节点<br>192.168.1.105 shard server1 仲裁</p>
<p>192.168.1.107 shard server2<br>192.168.1.108 shard server2 副节点<br>192.168.1.109 shard server2 仲裁</p>
<p>192.168.1.110 mongos</p>
<h5 id="集群搭建"><a href="#集群搭建" class="headerlink" title="集群搭建"></a><b>集群搭建</b></h5><ul>
<li><p>Install MongoDB on All Nodes</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">cat &gt; /etc/yum.repos.d/mongodb-org-3.6.repo &lt;&lt; &apos;EOF&apos;</div><div class="line">[mongodb-org-3.6]</div><div class="line">name=MongoDB Repository</div><div class="line">baseurl=https://repo.mongodb.org/yum/redhat/$releasever/mongodb-org/3.6/x86_64/</div><div class="line">gpgcheck=1</div><div class="line">enabled=1</div><div class="line">gpgkey=https://www.mongodb.org/static/pgp/server-3.6.asc</div><div class="line">EOF</div><div class="line"></div><div class="line">Package Name		Description</div><div class="line">mongodb-org		A metapackage that will automatically install the four component packages listed below.</div><div class="line">mongodb-org-server	Contains the mongod daemon and associated configuration and init scripts.</div><div class="line">mongodb-org-mongos	Contains the mongos daemon.</div><div class="line">mongodb-org-shell	Contains the mongo shell.</div><div class="line">mongodb-org-tools	Contains the following MongoDB tools: mongoimport bsondump, mongodump, mongoexport, mongofiles, mongoperf, mongorestore, mongostat, and mongotop.</div><div class="line"></div><div class="line"></div><div class="line">yum -y install mongodb-org</div></pre></td></tr></table></figure>
</li>
<li><p>Configure Firewalld</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">#for centos6，add under to file  /etc/sysconfig/iptables</div><div class="line">cat /etc/sysconfig/iptables</div><div class="line">-A INPUT -p tcp -m state --state NEW -m tcp --dport 22 -j ACCEPT </div><div class="line">-A INPUT -p tcp -m state --state NEW -m tcp --dport 27017 -j ACCEPT </div><div class="line">service iptables restart</div><div class="line">#for centos7</div><div class="line">yum -y install firewalld</div><div class="line">systemctl start firewalld</div><div class="line">systemctl enable firewalld</div><div class="line"></div><div class="line">firewall-cmd --permanent --add-port=22/tcp</div><div class="line">firewall-cmd --permanent --add-port=27017/tcp</div><div class="line">firewall-cmd --reload</div></pre></td></tr></table></figure>
</li>
<li><p>Configure MongoDB config server </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">net:</div><div class="line">  port: 27017</div><div class="line">  bindIP: 127.0.0.1,192.168.1.101  </div><div class="line">  #declare this is a config db of a cluster;  </div><div class="line">sharding:</div><div class="line">   clusterRole: configsvr</div><div class="line">replication:</div><div class="line">   replSetName: configs  </div><div class="line"></div><div class="line">#登录任意一台配置服务器，初始化配置副本集</div><div class="line"></div><div class="line">#连接</div><div class="line">mongo --port 27017</div><div class="line">#config变量</div><div class="line">config = &#123;</div><div class="line">		_id : &quot;configs&quot;,</div><div class="line">		members : [</div><div class="line">		&#123;_id : 0, host : &quot;192.168.1.100:27017&quot; &#125;,</div><div class="line">		&#123;_id : 0, host : &quot;192.168.1.101:27017&quot; &#125;,</div><div class="line">		&#123;_id : 1, host : &quot;192.168.1.102:27017&quot; &#125;</div><div class="line">		]</div><div class="line">	&#125;</div><div class="line"></div><div class="line">#初始化副本集</div><div class="line">rs.initiate(config)</div></pre></td></tr></table></figure>
<p>  其中，”_id” : “configs”应与配置文件中配置的 replicaction.replSetName 一致，”members” 中的 “host” 为三个节点的 ip 和 port</p>
</li>
<li><p>Configure MongoDB Replica Set </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">vim /etc/mongod.conf</div><div class="line"></div><div class="line">net:</div><div class="line">  port: 27017</div><div class="line">  bindIP: 127.0.0.1,192.168.1.103</div><div class="line">  </div><div class="line">replication:</div><div class="line">  replSetName: &quot;shard1&quot;</div><div class="line">sharding:</div><div class="line">  clusterRole: shardsvr</div><div class="line">  </div><div class="line">mongo --port 27017</div><div class="line">#使用admin数据库</div><div class="line">use admin</div><div class="line">#定义副本集配置</div><div class="line">config = &#123;</div><div class="line"> 	 _id : &quot;shard1&quot;,</div><div class="line">	 members : [</div><div class="line">         &#123;_id : 0, host : &quot;192.168.1.103:27017&quot; &#125;,</div><div class="line">         &#123;_id : 1, host : &quot;192.168.1.104:27017&quot; &#125;,</div><div class="line">         &#123;_id : 2, host : &quot;192.168.1.105:27017&quot;, arbiterOnly: true &#125;</div><div class="line">     ]</div><div class="line">&#125;</div><div class="line">#初始化副本集配置</div><div class="line">rs.initiate(config);</div></pre></td></tr></table></figure>
<p>  第二个Replica Set也执行类似操作</p>
</li>
</ul>
<ul>
<li><p>Configure MongoDB mongos</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">[root@localhost ~]# cat mongos.conf </div><div class="line"># where to write logging data.</div><div class="line">systemLog:</div><div class="line">destination: file</div><div class="line">logAppend: true</div><div class="line">path: /var/log/mongodb/mongos.log</div><div class="line"></div><div class="line">#security:</div><div class="line">#keyFile: /opt/mongo/mongodb-keyfile</div><div class="line"></div><div class="line">port=27017</div><div class="line">bind_ip=127.0.0.1,172.16.56.233</div><div class="line">#监听的配置服务器,只能有1个或者3个 configs为配置服务器的副本集名字</div><div class="line">configdb=configs/192.1688.1.100:27017,192.1688.1.101:27017,192.168.1.102:27017</div><div class="line"></div><div class="line">mongos --config mongos.conf  --fork</div></pre></td></tr></table></figure>
</li>
<li><p>MongoDB Replica Set initiate</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">登陆任意一台mongos</div><div class="line"></div><div class="line">mongo --port 27017</div><div class="line">#使用admin数据库</div><div class="line">use  admin</div><div class="line">#串联路由服务器与分配副本集</div><div class="line">sh.addShard(&quot;shard1/192.168.1.103:27017,192.168.1.104:27017,192.168.1.105:27017&quot;)</div><div class="line">sh.addShard(&quot;shard2/192.168.1.106:27017,192.168.1.107:27017,192.168.1.108:27017&quot;)</div><div class="line">#查看集群状态</div><div class="line">sh.status()</div></pre></td></tr></table></figure>
</li>
<li><p>Test the Replication</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">use admin</div><div class="line">#指定testdb分片生效</div><div class="line">sh.enableSharding( &quot;testdb&quot; )</div><div class="line"></div><div class="line">#Before sharding a non-empty collection, create an index on the shard key.</div><div class="line">use testdb</div><div class="line">db.table1.createIndex( &#123; id : 1 &#125; )</div><div class="line"></div><div class="line">#指定数据库里需要分片的集合和片键</div><div class="line">use testdb</div><div class="line">sh.shardCollection( &quot;testdb.table1&quot;, &#123; id : 1 &#125; )</div><div class="line"></div><div class="line">#Confirm the shard is balancing</div><div class="line">use testdb</div><div class="line">db.stats()</div><div class="line">db.printShardingStatus()</div></pre></td></tr></table></figure>
<p>  我们设置testdb的 table1 表需要分片，根据 id 自动分片到 shard1 ，shard2 上面去。要这样设置是因为不是所有mongodb 的数据库和表 都需要分片！<br>测试分片配置结果</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div></pre></td><td class="code"><pre><div class="line">mongo  127.0.0.1:27017</div><div class="line">#使用testdb</div><div class="line">use  testdb;</div><div class="line">#插入测试数据</div><div class="line">for (var i = 1; i &lt;= 600000; i++)db.table1.save(&#123;id:i,&quot;test1&quot;:&quot;testval1&quot;&#125;);</div><div class="line">#查看分片情况如下，部分无关信息省掉了</div><div class="line">mongos&gt; db.printShardingStatus()</div><div class="line">--- Sharding Status --- </div><div class="line">  sharding version: &#123;</div><div class="line">        &quot;_id&quot; : 1,</div><div class="line">        &quot;minCompatibleVersion&quot; : 5,</div><div class="line">        &quot;currentVersion&quot; : 6,</div><div class="line">        &quot;clusterId&quot; : ObjectId(&quot;5a674706887e9c5d977acacc&quot;)</div><div class="line">  &#125;</div><div class="line">  shards:</div><div class="line">        &#123;  &quot;_id&quot; : &quot;shard1&quot;,  &quot;host&quot; : &quot;shard1/192.168.1.103:27017,192.168.1.104:27017&quot;,  &quot;state&quot; : 1 &#125;</div><div class="line">        &#123;  &quot;_id&quot; : &quot;shard2&quot;,  &quot;host&quot; : &quot;shard2/192.168.1.106:27017,192.168.1.107:27017&quot;,  &quot;state&quot; : 1 &#125;</div><div class="line">  active mongoses:</div><div class="line">        &quot;3.6.2&quot; : 1</div><div class="line">  autosplit:</div><div class="line">        Currently enabled: yes</div><div class="line">  balancer:</div><div class="line">        Currently enabled:  yes</div><div class="line">        Currently running:  no</div><div class="line">        Failed balancer rounds in last 5 attempts:  0</div><div class="line">        Migration Results for the last 24 hours: </div><div class="line">                No recent migrations</div><div class="line">  databases:</div><div class="line">        &#123;  &quot;_id&quot; : &quot;config&quot;,  &quot;primary&quot; : &quot;config&quot;,  &quot;partitioned&quot; : true &#125;</div><div class="line">                config.system.sessions</div><div class="line">                        shard key: &#123; &quot;_id&quot; : 1 &#125;</div><div class="line">                        unique: false</div><div class="line">                        balancing: true</div><div class="line">                        chunks:</div><div class="line">                                shard1  1</div><div class="line">                        &#123; &quot;_id&quot; : &#123; &quot;$minKey&quot; : 1 &#125; &#125; --&gt;&gt; &#123; &quot;_id&quot; : &#123; &quot;$maxKey&quot; : 1 &#125; &#125; on : shard1 Timestamp(1, 0) </div><div class="line"></div><div class="line">        &#123;  &quot;_id&quot; : &quot;testdb&quot;,  &quot;primary&quot; : &quot;shard1&quot;,  &quot;partitioned&quot; : true &#125;</div><div class="line">                testdb.table1</div><div class="line">                        shard key: &#123; &quot;id&quot; : 1 &#125;</div><div class="line">                        unique: false</div><div class="line">                        balancing: true</div><div class="line">                        chunks:</div><div class="line">                                shard1  1</div><div class="line">                        &#123; &quot;id&quot; : &#123; &quot;$minKey&quot; : 1 &#125; &#125; --&gt;&gt; &#123; &quot;id&quot; : &#123; &quot;$maxKey&quot; : 1 &#125; &#125; on : shard1 Timestamp(1, 0)</div></pre></td></tr></table></figure>
<p>  可以看到目前只有一个chunk在shard1整个shard上</p>
</li>
</ul>
<h5 id="遇到的错误"><a href="#遇到的错误" class="headerlink" title="遇到的错误"></a><b>遇到的错误</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"> rs.initiate(config);</div><div class="line">&#123;</div><div class="line">        &quot;ok&quot; : 0,</div><div class="line">        &quot;errmsg&quot; : &quot;Attempting to initiate a replica set with name shard2, but command line reports shard1; rejecting&quot;,</div><div class="line">        &quot;code&quot; : 93,</div><div class="line">        &quot;codeName&quot; : &quot;InvalidReplicaSetConfig&quot;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>解决方案： 由于配置replSetName错误引起</p>
<h5 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a><b>常用命令</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">#List Databases with Sharding Enabled</div><div class="line">use config</div><div class="line">db.databases.find( &#123; &quot;partitioned&quot;: true &#125; )</div><div class="line"></div><div class="line">#List Shards</div><div class="line">db.adminCommand( &#123; listShards: 1 &#125; )</div><div class="line">#remove shard</div><div class="line">db.adminCommand( &#123; removeShard: &quot;testdb&quot; &#125; )</div><div class="line"></div><div class="line">#View Cluster Details</div><div class="line">db.printShardingStatus() or sh.status()</div><div class="line"></div><div class="line">#drop database</div><div class="line">use newdb</div><div class="line">switched to db newdb</div><div class="line">db.dropDatabase()</div></pre></td></tr></table></figure>
<p>ref<br><a href="http://blog.sina.com.cn/s/blog_8ea8e9d50102wl8x.html" target="_blank" rel="external">mongoDB-3.x Sharding with Replica</a><br><a href="https://docs.mongodb.com/manual/tutorial/convert-replica-set-to-replicated-shard-cluster/" target="_blank" rel="external">From Replica Set to Sharding</a><br><a href="http://www.cnblogs.com/xybaby/p/6832296.html" target="_blank" rel="external">通过一步步创建sharded cluster来认识mongodb</a></p>
<hr>
<p>您的鼓励是我写作最大的动力</p>
<p>俗话说，投资效率是最好的投资。 如果您感觉我的文章质量不错，读后收获很大，预计能为您提高 10% 的工作效率，不妨小额捐助我一下，让我有动力继续写出更多好文章。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;&lt;b&gt;前言&lt;/b&gt;&lt;/h5&gt;&lt;p&gt;MongoDB是一个介于关系数据库和非关系数据库之间的产品，适合存储对象及JSON形式的数据。支持丰富的查询方式，几乎可以实现
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>浅谈Redis Cluster</title>
    <link href="https://t1ger.github.io/2018/01/19/%E6%B5%85%E8%B0%88Redis-Cluster/"/>
    <id>https://t1ger.github.io/2018/01/19/浅谈Redis-Cluster/</id>
    <published>2018-01-19T07:10:04.000Z</published>
    <updated>2018-01-19T09:20:11.288Z</updated>
    
    <content type="html"><![CDATA[<h5 id="redis-集群方案概述"><a href="#redis-集群方案概述" class="headerlink" title="redis 集群方案概述"></a><b>redis 集群方案概述</b></h5><p>自从redis3.0起，redis官方就推出了redis cluster,是一个可以在多个 Redis 节点之间进行数据共享的设施.Redis 集群不支持那些需要同时处理多个键的 Redis 命令， 因为执行这些命令需要在多个 Redis 节点之间移动数据， 并且在高负载的情况下， 这些命令将降低 Redis 集群的性能， 并导致不可预测的行为</p>
<p>当然，也有一些开源的解决方案来提供redis的高可用性，主要有</p>
<ul>
<li><p>Twitter的Redis/Memcached代理服务Twemproxy<br>Twemproxy是一个轻量级的Redis代理服务器，它通过引入一个代理层，将应用程序后端的多台Redis实例进行统一管理，使应用程序只需要在Twemproxy上进行操作，而不用关心后面具体有多少个真实的Redis或Memcached实例，从而实现了基于Redis和Memcached的集群服务。当某个节点宕掉时，Twemproxy可以自动将它从集群中剔除，而当它恢复服务时，Twemproxy也会自动连接。由于是代理，所以Twemproxy会有微小的性能损失。根据 Redis作者的测试结果，在大多数情况下，Twemproxy的性能相当不错，同直接操作Redis相比，最多只有20%的性能损失</p>
</li>
<li><p>豌豆荚的 Redis 集群解决方案Codis<br>它由 codis-server、codis-proxy、codis Dashboard、codis Admin、Codis FE、Storage组成,<br>基于proxy的codis，客户端对路由表变化无感知。客户端需要从codis dashhoard调用list proxy命令获取所有proxy列表，并根据自身的轮询策略决定访问哪个proxy节点以实现负载均衡，codis release 版本为 codis-3.2，codis-server 基于 redis-3.2.8,同时实现 select 命令,支持多db, 详细介绍参考<a href="https://github.com/CodisLabs/codis/blob/release3.2/doc/tutorial_zh.md" target="_blank" rel="external">这里</a></p>
</li>
<li><p>redis_sentinel<br>Redis-Sentinel是Redis官方推荐的高可用性(HA)解决方案，当用Redis做Master-slave的高可用方案时，假如master宕机了，Redis本身(包括它的很多客户端)都没有实现自动进行主备切换，而Redis-sentinel本身也是一个独立运行的进程，它能监控多个master-slave集群，发现master宕机后能进行自动切换</p>
</li>
<li><p>redis+keepalive<br>通过keepalived实现redis的高可用。keepalived利用shell脚本，定期检测redis服务是否正常。当redis服务异常时，利用虚拟IP的漂移实现故障切换</p>
</li>
</ul>
<p>如果你使用的是云平台，会提供类似的产品，有兴趣的请参考<a href="https://yq.aliyun.com/articles/68593" target="_blank" rel="external">这里</a></p>
<h5 id="redis-集群方案对比"><a href="#redis-集群方案对比" class="headerlink" title="redis 集群方案对比"></a><b>redis 集群方案对比</b></h5><p>redis_sentinel 和 redis+keepalive  两个方案都是基于redis主从实现，都不是基于分布式的解决方案，前者可以一主多从，后者基本是一主一从。<br>如果redis写入压力比较大，在碰到网络问题导致闪断切换，redis_sentinel切换需要1-2s的时间，则客户端有可能因此约10w记录无法写入，需要客户端在编码中有异常处理机制，在返回失败是尝试延时重写. keepalive则切换较快，但由于只有一主一从，在读写压力比较大时依然无法满足性能要求，并且在master出现故障，需要人工干预才能重新形成新的主从关系。</p>
<p>codis和twemproxy都是比较稳定的生产集群解决方案，随着官方redis4.x的推出，官方的redis cluster集群也是一个不错的选择方案</p>
<h5 id="redis-集群方案安装"><a href="#redis-集群方案安装" class="headerlink" title="redis 集群方案安装"></a><b>redis 集群方案安装</b></h5><p>这样主要讲解redis cluster集群的部署，基于redis4.0.6版本。</p>
<ul>
<li><p>redis_sentinel<br>redis_sentinel 的部署配置参考<a href="https://github.com/alisaifee/limits/tree/master/tests/redis-configurations/sentinel" target="_blank" rel="external">这里</a></p>
</li>
<li><p>redis cluster<br>集群部署官方推荐3主3从,这里用两台机器模拟 172.168.10.44 建立3个master，172.168.10.45建立3个slave</p>
</li>
</ul>
<ol>
<li><p>install</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">cd /usr/local/src</div><div class="line">wget http://download.redis.io/releases/redis-4.0.6.tar.gz</div><div class="line">tar zxf redis-4.0.6.tar.gz</div><div class="line">cd redis-4.0.6</div><div class="line">make</div></pre></td></tr></table></figure>
</li>
<li><p>config<br>redis cluster 的部署配置参考<a href="https://github.com/alisaifee/limits/tree/master/tests/redis-configurations/cluster" target="_blank" rel="external">这里</a></p>
</li>
<li><p>create cluster</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"># yum install ruby ruby-devel rubygems  -y</div><div class="line"># gem install redis</div><div class="line">#注意master和slave的顺序是一一对应的</div><div class="line"># redis-trib.rb create  --replicas  1 192.168.10.44:7000 192.168.10.44:7001  192.168.10.44:7002   192.168.10.45:7003  192.168.10.45:7004  192.168.10.45:7005</div><div class="line">yes</div><div class="line"></div><div class="line"># redis-cli -h 192.168.10.45 -c -p 7005</div><div class="line"></div><div class="line"># 查看集群节点情况</div><div class="line">192.168.10.45:7005&gt; cluster info</div></pre></td></tr></table></figure>
</li>
<li><p>新添加redis 主节点</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"># 查看集群节点槽占用情况</div><div class="line">192.168.10.45:7005&gt;  cluster nodes</div><div class="line"></div><div class="line"># 添加7006节点作为新节点</div><div class="line"># ./redis-trib.rb add-node 192.168.10.44:7006 192.168.10.44:7000</div><div class="line">add-node是加入指令，192.168.10.44:7006 表示新加入的节点，</div><div class="line">192.168.10.45:7000 表示加入的集群的一个节点，用来辨识是哪个集群，理论上哪个都可以</div><div class="line"></div><div class="line"># 给刚添加的7006结点分配槽</div><div class="line">./redis-trib.rb reshard 192.168.10.44:7006</div><div class="line">## 第二步：输入要分配的槽数量（输入：500，表示要分配500个槽）</div><div class="line">How many slots do you want to move (from 1 to 16384)? 500</div><div class="line"></div><div class="line">## 第三步：输入接收槽的结点id 7006的ID是：065c9c2223949e77f407d20aefa6408aa1bcd181</div><div class="line">What is the receiving node ID? 065c9c2223949e77f407d20aefa6408aa1bcd181</div><div class="line">## 第四步：输入源结点id（槽点会从源节点中拿，输入all从所有源节点中获取槽，输入done取消分配）</div><div class="line">Please enter all the source node IDs.</div><div class="line">  Type &apos;all&apos; to use all the nodes as source nodes for the hash slots.</div><div class="line">  Type &apos;done&apos; once you entered all the source nodes IDs.</div><div class="line">Source node #1:all</div><div class="line">## 输入yes开始移动槽到目标结点id</div><div class="line">Do you want to proceed with the proposed reshard plan (yes/no)? yes</div><div class="line"># 查看集群中槽占用情况</div><div class="line">192.168.10.44:7006&gt; cluster nodes</div></pre></td></tr></table></figure>
</li>
<li><p>添加从节点</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">./redis-trib.rb add-node --slave 192.168.10.45:7007 192.168.10.45:7000</div><div class="line">add-node的时候加上--slave表示是加入到从节点中，但是这样加，是随机的。</div><div class="line">这里的命令行完全像我们在添加一个新主服务器时使用的一样，</div><div class="line">所以我们没有指定要给哪个主服 务器添加副本。</div><div class="line">这种情况下，redis-trib 会将7006作为一个具有较少副本的随机的主服务器的副本</div><div class="line"></div><div class="line">我如果想指定一个主节点行不行？当然可以。我们再建一个7008节点。</div><div class="line">./redis-trib.rb add-node –slave –master-id 主节点id 新节点的ip和端口 旧节点ip和端口</div><div class="line"></div><div class="line">./redis-trib.rb add-node --slave --master-id 065c9c2223949e77f407d20aefa6408aa1bcd181 192.168.10.45:7008 192.168.10.44:7000</div></pre></td></tr></table></figure>
</li>
</ol>
<h5 id="后记"><a href="#后记" class="headerlink" title="后记"></a><b>后记</b></h5><p>这里推荐个redis集群的运维工具 CacheCloud，有兴趣的可以参考<a href="https://github.com/sohutv/cachecloud" target="_blank" rel="external">这里</a><br>一般影响cluster failover失败有：<br>(1)从节点超时过长,查看cluster-node-timeout和cluster-slave-validity-factor相关参数<br>(2)从节点的无法获取到集群中其他主节点的投票<br>(3)参与领导者选举的主节点不够一半以上<br>(3)从节点数达不到cluster-migration-barrier数量。<br>另外，需要注意的是没有负责任何槽的主节点没有投票权</p>
<p>ref<br><a href="https://redis.io/topics/cluster-tutorial" target="_blank" rel="external">Redis cluster tutorial</a><br><a href="http://magic_duck.oschina.io/2017/09/07/redis4.4.0_primary_05/" target="_blank" rel="external">Redis集群</a><br><a href="https://www.zybuluo.com/phper/note/195558" target="_blank" rel="external">Redis集群研究和实践（基于redis 3.0.5）</a></p>
<hr>
<p>您的鼓励是我写作最大的动力</p>
<p>俗话说，投资效率是最好的投资。 如果您感觉我的文章质量不错，读后收获很大，预计能为您提高 10% 的工作效率，不妨小额捐助我一下，让我有动力继续写出更多好文章。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;redis-集群方案概述&quot;&gt;&lt;a href=&quot;#redis-集群方案概述&quot; class=&quot;headerlink&quot; title=&quot;redis 集群方案概述&quot;&gt;&lt;/a&gt;&lt;b&gt;redis 集群方案概述&lt;/b&gt;&lt;/h5&gt;&lt;p&gt;自从redis3.0起，redis官方就推出了
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Why Datanode is Denied Communication With Namenode</title>
    <link href="https://t1ger.github.io/2017/12/20/Why-Datanode-is-Denied-Communication-With-Namenode/"/>
    <id>https://t1ger.github.io/2017/12/20/Why-Datanode-is-Denied-Communication-With-Namenode/</id>
    <published>2017-12-20T06:51:08.000Z</published>
    <updated>2017-12-20T07:54:06.282Z</updated>
    
    <content type="html"><![CDATA[<p>So for those trying to setup HDFS out there, and are struggling with this kind of error where it said datanode denied communication with namenode:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">2017-12-20 14:26:26,108 INFO org.apache.hadoop.ipc.Server: IPC Server handler 29 on 8020, call Call#9 Retry#0 org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.registerDatanode from 172.16.56.238:27481</div><div class="line">org.apache.hadoop.hdfs.server.protocol.DisallowedDatanodeException: Datanode denied communication with namenode because hostname cannot be resolved (ip=172.16.56.238, hostname=172.16.56.238): DatanodeRegistration(0.0.0.0:50010, datanodeUuid=95e30839-4e7b-4918-98bb-f125b0c932c7, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-57;cid=CID-d26bb3c5-ffa5-466c-a3be-fe16d8b84e73;nsid=1832233914;c=1513750982825)</div><div class="line">        at org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.registerDatanode(DatanodeManager.java:952)</div><div class="line">        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.registerDatanode(BlockManager.java:2014)</div><div class="line">        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.registerDatanode(FSNamesystem.java:3656)</div><div class="line">        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.registerDatanode(NameNodeRpcServer.java:1418)</div><div class="line">        at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolServerSideTranslatorPB.registerDatanode(DatanodeProtocolServerSideTranslatorPB.java:101)</div><div class="line">        at org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2.callBlockingMethod(DatanodeProtocolProtos.java:30583)</div><div class="line">        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)</div><div class="line">        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)</div><div class="line">        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:868)</div><div class="line">        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:814)</div><div class="line">        at java.security.AccessController.doPrivileged(Native Method)</div><div class="line">        at javax.security.auth.Subject.doAs(Subject.java:422)</div><div class="line">        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1886)</div><div class="line">        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2603)</div></pre></td></tr></table></figure></p>
<p>It could actually be because of many configurations problem, but in the end it boils to one thing and that’s what I am going to tell you. It costed me one day just to fix this problem when I was running test-kitchen to provision hadoop servers.</p>
<p>So we have a namenode up and running, now we boot up a datanode. We have the defaultFS correct, so the datanode knows where the namenode is. It tries to connect to namenode. The connection happens through IP, so namenode only see the datanode’s ip. The problem is, in this process. Namenode has a list of blacklisted hostnames, which should not connect to it. This list can be empty, but namenode will keep checking it anyway, so it will try to do a reverse dns lookup to see which hostname the ip has. If it fails, then namenode will throw the exception you saw, and this it the source of all problems.</p>
<p>So basically, we have a few options to fix this: </p>
<ol>
<li>Fix the reverse dns thing in namenode, so that it could do the reverse dns lookup properly. </li>
<li>Think that it doesn’t make sense to block anything in your case and turn off the checking.</li>
</ol>
<p>Well, if your cluster is in a private network (which usually is), what is the chance that the datanode which is trying to connect to your namenode is not your datanodes? Not big. So let’s just turn it off.</p>
<p>You will have to add this setting in namenode’s hdfs-site.xml<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&lt;property&gt;</div><div class="line">  &lt;name&gt;dfs.namenode.datanode.registration.ip-hostname-check&lt;/name&gt;</div><div class="line">  &lt;value&gt;false&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div></pre></td></tr></table></figure></p>
<p>It was really confusing for me at first, because it is not clear whether denied communication was a result of misconfiguration in datanode or namenode. There are some <a href="https://stackoverflow.com/questions/17252955/getting-the-following-error-datanode-denied-communication-with-namenode-while" target="_blank" rel="external">stackoverflow</a> entry about this, but they are single machine setup, and confused me even more because the solution is not working. I hope if you are experiencing same trouble, this post has helped you to understand a bit of the picture.</p>
<p>ref<br><a href="https://log.rowanto.com/why-datanode-is-denied-communication-with-namenode/" target="_blank" rel="external">Why Datanode is Denied Communication With Namenode</a><br><a href="https://stackoverflow.com/questions/17252955/getting-the-following-error-datanode-denied-communication-with-namenode-while" target="_blank" rel="external">Getting the following error “Datanode denied communication with namenode” while configuring hadoop 0.23.8</a><br><a href="https://ooon.me/2016/03/Hadoop-dns/" target="_blank" rel="external">Hadoop 使用 DNS 的问题</a></p>
<hr>
<p>您的鼓励是我写作最大的动力</p>
<p>俗话说，投资效率是最好的投资。 如果您感觉我的文章质量不错，读后收获很大，预计能为您提高 10% 的工作效率，不妨小额捐助我一下，让我有动力继续写出更多好文章。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;So for those trying to setup HDFS out there, and are struggling with this kind of error where it said datanode denied communication with 
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>HDFS Short-Circuit Local Reads</title>
    <link href="https://t1ger.github.io/2017/12/15/HDFS-Short-Circuit-Local-Reads/"/>
    <id>https://t1ger.github.io/2017/12/15/HDFS-Short-Circuit-Local-Reads/</id>
    <published>2017-12-15T02:14:40.000Z</published>
    <updated>2017-12-25T14:37:46.017Z</updated>
    
    <content type="html"><![CDATA[<h5 id="Background"><a href="#Background" class="headerlink" title="Background"></a><b>Background</b></h5><p>In HDFS, reads normally go through the DataNode. Thus, when the client asks the DataNode to read a file, the DataNode reads that file off of the disk and sends the data to the client over a TCP socket. So-called “short-circuit” reads bypass the DataNode, allowing the client to read the file directly. Obviously, this is only possible in cases where the client is co-located with the data. Short-circuit reads provide a substantial performance boost to many applications</p>
<h5 id="Setup"><a href="#Setup" class="headerlink" title="Setup"></a><b>Setup</b></h5><p>To configure short-circuit local reads, you will need to enable libhadoop.so. See <a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/NativeLibraries.html" target="_blank" rel="external">Native Libraries</a> for details on enabling this library. if you don’t compile yourself, may be use complile binary version ,see <a href="http://dl.bintray.com/sequenceiq/sequenceiq-bin/" target="_blank" rel="external">here</a><br>Short-circuit reads make use of a UNIX domain socket. This is a special path in the filesystem that allows the client and the DataNodes to communicate. You will need to set a path to this socket. The DataNode needs to be able to create this path. On the other hand, it should not be possible for any user except the HDFS user or root to create this path. For this reason, paths under /var/run or /var/lib are often used.</p>
<p>The client and the DataNode exchange information via a shared memory segment on /dev/shm.</p>
<p>Short-circuit local reads need to be configured on both the DataNode and the client.</p>
<p>Java can not use Unix Domain Socket directly，so you need install Hadoop native package libhadoop.so。if you use Pivotal HD，CDH and so on, native package will be install at you install hadoop package. you can use command to check native package like this</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">$ hadoop checknative</div><div class="line">hadoop: true /usr/lib/hadoop/lib/native/libhadoop.so.1.0.0</div><div class="line">zlib:   true /lib64/libz.so.1</div><div class="line">snappy: true /usr/lib64/libsnappy.so.1</div><div class="line">lz4:    true revision:99</div><div class="line">bzip2:  true /lib64/libbz2.so.1</div><div class="line"></div><div class="line"></div><div class="line">#如果要排查问题,可以更改日志级别，进行相应排查</div><div class="line">export HADOOP_ROOT_LOGGER=DEBUG,console</div><div class="line">$ hadoop checknative -a</div></pre></td></tr></table></figure>
<p>#打开short-circuit local reads 功能</p>
<p>dfs.client.read.shortcircuit:  false </p>
<p>#可选。该参数是一个指向UNIX域套接字的路径，用于DataNode和本地HDFS客户端通信。如果在该路径中出现了字符串”_PORT”，会被替换成DataNode的TCP端口。</p>
<p>dfs.domain.socket.path: </p>
<p>#设置了该参数，short-circuit local reads功能将跳过checksums校验。通常不推荐这么做，但是该参数对于特殊场合可能有用。如果你在HDFS之外自己做checksum校验，那么就该考虑设置该参数。</p>
<p>dfs.client.read.shortcircuit.skip.checksum: false</p>
<p>#DFSClient维护着一个用于保存最近已打开的文件描述符的缓存。该参数控制着此缓存的容量。增大该缓存的容量就可以使用更多文件描述符，但是，在涉及大量seek操作的负载上可能带来更好的性能</p>
<p>dfs.client.read.shortcircuit.streams.cache.size: 256</p>
<p>#该参数控制着文件描述符因为长期不活跃而被关闭之前需要在客户端缓存上下文中驻留的最小时间</p>
<p>dfs.client.read.shortcircuit.streams.cache.expiry.ms: 300000</p>
<h5 id="Example-Configuration"><a href="#Example-Configuration" class="headerlink" title="Example Configuration"></a><b>Example Configuration</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">&lt;configuration&gt;</div><div class="line">  &lt;property&gt;</div><div class="line">    &lt;name&gt;dfs.client.read.shortcircuit&lt;/name&gt;</div><div class="line">    &lt;value&gt;true&lt;/value&gt;</div><div class="line">  &lt;/property&gt;</div><div class="line">  &lt;property&gt;</div><div class="line">    &lt;name&gt;dfs.domain.socket.path&lt;/name&gt;</div><div class="line">    &lt;value&gt;/var/lib/hadoop-hdfs/dn_socket&lt;/value&gt;</div><div class="line">  &lt;/property&gt;</div><div class="line">&lt;/configuration&gt;</div></pre></td></tr></table></figure>
<h5 id="Legacy-HDFS-Short-Circuit-Local-Reads"><a href="#Legacy-HDFS-Short-Circuit-Local-Reads" class="headerlink" title="Legacy HDFS Short-Circuit Local Reads"></a><b>Legacy HDFS Short-Circuit Local Reads</b></h5><p>Legacy implementation of short-circuit local reads on which the clients directly open the HDFS block files is still available for platforms other than the Linux. Setting the value of dfs.client.use.legacy.blockreader.local in addition to dfs.client.read.shortcircuit to true enables this feature.</p>
<p>You also need to set the value of dfs.datanode.data.dir.perm to 750 instead of the default 700 and chmod/chown the directory tree under dfs.datanode.data.dir as readable to the client and the DataNode. You must take caution because this means that the client can read all of the block files bypassing HDFS permission.</p>
<p>Because Legacy short-circuit local reads is insecure, access to this feature is limited to the users listed in the value of dfs.block.local-path-access.user.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">&lt;configuration&gt;</div><div class="line">  &lt;property&gt;</div><div class="line">    &lt;name&gt;dfs.client.read.shortcircuit&lt;/name&gt;</div><div class="line">    &lt;value&gt;true&lt;/value&gt;</div><div class="line">  &lt;/property&gt;</div><div class="line">  &lt;property&gt;</div><div class="line">    &lt;name&gt;dfs.client.use.legacy.blockreader.local&lt;/name&gt;</div><div class="line">    &lt;value&gt;true&lt;/value&gt;</div><div class="line">  &lt;/property&gt;</div><div class="line">  &lt;property&gt;</div><div class="line">    &lt;name&gt;dfs.datanode.data.dir.perm&lt;/name&gt;</div><div class="line">    &lt;value&gt;750&lt;/value&gt;</div><div class="line">  &lt;/property&gt;</div><div class="line">  &lt;property&gt;</div><div class="line">    &lt;name&gt;dfs.block.local-path-access.user&lt;/name&gt;</div><div class="line">    &lt;value&gt;foo,bar&lt;/value&gt;</div><div class="line">  &lt;/property&gt;</div><div class="line">&lt;/configuration&gt;</div></pre></td></tr></table></figure>
<h5 id="碰到的问题-centos7-hadoop2-9"><a href="#碰到的问题-centos7-hadoop2-9" class="headerlink" title="碰到的问题(centos7 hadoop2.9)"></a><b>碰到的问题(centos7 hadoop2.9)</b></h5><p>1.openssl问题<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line">[root@namenode ~]# export HADOOP_OPTS=-Djava.library.path=/usr/local/hadoop-2.9.0/lib/native</div><div class="line">[root@namenode ~]# hadoop checknative -a</div><div class="line">17/12/25 15:46:06 DEBUG util.NativeCodeLoader: Trying to load the custom-built native-hadoop library...</div><div class="line">17/12/25 15:46:06 DEBUG util.NativeCodeLoader: Loaded the native-hadoop library</div><div class="line">17/12/25 15:46:07 DEBUG util.Shell: setsid exited with exit code 0</div><div class="line">17/12/25 15:46:07 INFO bzip2.Bzip2Factory: Successfully loaded &amp; initialized native-bzip2 library system-native</div><div class="line">17/12/25 15:46:07 INFO zlib.ZlibFactory: Successfully loaded &amp; initialized native-zlib library</div><div class="line">17/12/25 15:46:07 DEBUG crypto.OpensslCipher: Failed to load OpenSSL Cipher.</div><div class="line">java.lang.UnsatisfiedLinkError: Cannot load libcrypto.so (libcrypto.so: cannot open shared object file: No such file or directory)!</div><div class="line">        at org.apache.hadoop.crypto.OpensslCipher.initIDs(Native Method)</div><div class="line">        at org.apache.hadoop.crypto.OpensslCipher.&lt;clinit&gt;(OpensslCipher.java:87)</div><div class="line">        at org.apache.hadoop.util.NativeLibraryChecker.main(NativeLibraryChecker.java:101)</div><div class="line">Native library checking:</div><div class="line">hadoop:  true /usr/local/hadoop-2.9.0/lib/native/libhadoop.so.1.0.0</div><div class="line">zlib:    true /lib64/libz.so.1</div><div class="line">snappy:  true /lib64/libsnappy.so.1</div><div class="line">zstd  :  false </div><div class="line">lz4:     true revision:10301</div><div class="line">bzip2:   true /lib64/libbz2.so.1</div><div class="line">openssl: false Cannot load libcrypto.so (libcrypto.so: cannot open shared object file: No such file or directory)!</div><div class="line">17/12/25 15:46:07 DEBUG util.ExitUtil: Exiting with status 1: ExitException</div><div class="line">1: ExitException</div><div class="line">        at org.apache.hadoop.util.ExitUtil.terminate(ExitUtil.java:304)</div><div class="line">        at org.apache.hadoop.util.ExitUtil.terminate(ExitUtil.java:292)</div><div class="line">        at org.apache.hadoop.util.NativeLibraryChecker.main(NativeLibraryChecker.java:145)</div><div class="line">17/12/25 15:46:07 INFO util.ExitUtil: Exiting with status 1: ExitException</div></pre></td></tr></table></figure></p>
<p>解决方法：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ln -s /usr/lib64/libcrypto.so.1.0.1e /usr/lib64/libcrypto.so</div></pre></td></tr></table></figure></p>
<p>2.zstd问题<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">[root@namenode ~]# hadoop checknative -a</div><div class="line">17/12/25 17:24:43 DEBUG util.NativeCodeLoader: Trying to load the custom-built native-hadoop library...</div><div class="line">17/12/25 17:24:43 DEBUG util.NativeCodeLoader: Loaded the native-hadoop library</div><div class="line">17/12/25 17:24:43 DEBUG util.Shell: setsid exited with exit code 0</div><div class="line">17/12/25 17:24:43 INFO bzip2.Bzip2Factory: Successfully loaded &amp; initialized native-bzip2 library system-native</div><div class="line">17/12/25 17:24:43 INFO zlib.ZlibFactory: Successfully loaded &amp; initialized native-zlib library</div><div class="line">Native library checking:</div><div class="line">hadoop:  true /usr/local/hadoop-2.9.0/lib/native/libhadoop.so.1.0.0</div><div class="line">zlib:    true /lib64/libz.so.1</div><div class="line">snappy:  true /lib64/libsnappy.so.1</div><div class="line">zstd  :  false </div><div class="line">lz4:     true revision:10301</div><div class="line">bzip2:   true /lib64/libbz2.so.1</div><div class="line">openssl: true /lib64/libcrypto.so</div><div class="line">17/12/25 17:24:43 DEBUG util.ExitUtil: Exiting with status 1: ExitException</div><div class="line">1: ExitException</div><div class="line">        at org.apache.hadoop.util.ExitUtil.terminate(ExitUtil.java:304)</div><div class="line">        at org.apache.hadoop.util.ExitUtil.terminate(ExitUtil.java:292)</div><div class="line">        at org.apache.hadoop.util.NativeLibraryChecker.main(NativeLibraryChecker.java:145)</div><div class="line">17/12/25 17:24:43 INFO util.ExitUtil: Exiting with status 1: ExitException</div></pre></td></tr></table></figure></p>
<p>解决方法：无,做了以下尝试，无结果<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">yum install epel-release -y</div><div class="line">yum install libzstd -y</div></pre></td></tr></table></figure></p>
<p>后查看源代码，在源码包中查看BUILDING.txt文件，发现编译环境没有libzstd，猜测可能编译时没有安装，但代码里有检测，所以有提示<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div></pre></td><td class="code"><pre><div class="line">public class NativeCodeLoader &#123;</div><div class="line"></div><div class="line">  private static final Logger LOG =</div><div class="line">      LoggerFactory.getLogger(NativeCodeLoader.class);</div><div class="line">  </div><div class="line">  private static boolean nativeCodeLoaded = false;</div><div class="line">  </div><div class="line">  static &#123;</div><div class="line">    // Try to load native hadoop library and set fallback flag appropriately</div><div class="line">    if(LOG.isDebugEnabled()) &#123;</div><div class="line">      LOG.debug(&quot;Trying to load the custom-built native-hadoop library...&quot;);</div><div class="line">    &#125;</div><div class="line">    try &#123;</div><div class="line">      System.loadLibrary(&quot;hadoop&quot;);</div><div class="line">      LOG.debug(&quot;Loaded the native-hadoop library&quot;);</div><div class="line">      nativeCodeLoaded = true;</div><div class="line">    &#125; catch (Throwable t) &#123;</div><div class="line">      // Ignore failure to load</div><div class="line">      if(LOG.isDebugEnabled()) &#123;</div><div class="line">        LOG.debug(&quot;Failed to load native-hadoop with error: &quot; + t);</div><div class="line">        LOG.debug(&quot;java.library.path=&quot; +</div><div class="line">            System.getProperty(&quot;java.library.path&quot;));</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">    </div><div class="line">    if (!nativeCodeLoaded) &#123;</div><div class="line">      LOG.warn(&quot;Unable to load native-hadoop library for your platform... &quot; +</div><div class="line">               &quot;using builtin-java classes where applicable&quot;);</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  /**</div><div class="line">   * Check if native-hadoop code is loaded for this platform.</div><div class="line">   * </div><div class="line">   * @return &lt;code&gt;true&lt;/code&gt; if native-hadoop is loaded, </div><div class="line">   *         else &lt;code&gt;false&lt;/code&gt;</div><div class="line">   */</div><div class="line">  public static boolean isNativeCodeLoaded() &#123;</div><div class="line">    return nativeCodeLoaded;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  /**</div><div class="line">   * Returns true only if this build was compiled with support for snappy.</div><div class="line">   */</div><div class="line">  public static native boolean buildSupportsSnappy();</div><div class="line">  </div><div class="line">  /**</div><div class="line">   * Returns true only if this build was compiled with support for ZStandard.</div><div class="line">   */</div><div class="line">  public static native boolean buildSupportsZstd();</div><div class="line"></div><div class="line">  /**</div><div class="line">   * Returns true only if this build was compiled with support for openssl.</div><div class="line">   */</div><div class="line">  public static native boolean buildSupportsOpenssl();</div><div class="line"></div><div class="line">  public static native String getLibraryName();</div><div class="line"></div><div class="line">  /**</div><div class="line">   * Return if native hadoop libraries, if present, can be used for this job.</div><div class="line">   * @param conf configuration</div><div class="line">   * </div><div class="line">   * @return &lt;code&gt;true&lt;/code&gt; if native hadoop libraries, if present, can be </div><div class="line">   *         used for this job; &lt;code&gt;false&lt;/code&gt; otherwise.</div><div class="line">   */</div><div class="line">  public boolean getLoadNativeLibraries(Configuration conf) &#123;</div><div class="line">    return conf.getBoolean(CommonConfigurationKeys.IO_NATIVE_LIB_AVAILABLE_KEY, </div><div class="line">                           CommonConfigurationKeys.IO_NATIVE_LIB_AVAILABLE_DEFAULT);</div><div class="line">  &#125;</div><div class="line">  </div><div class="line">  /**</div><div class="line">   * Set if native hadoop libraries, if present, can be used for this job.</div><div class="line">   * </div><div class="line">   * @param conf configuration</div><div class="line">   * @param loadNativeLibraries can native hadoop libraries be loaded</div><div class="line">   */</div><div class="line">  public void setLoadNativeLibraries(Configuration conf, </div><div class="line">                                     boolean loadNativeLibraries) &#123;</div><div class="line">    conf.setBoolean(CommonConfigurationKeys.IO_NATIVE_LIB_AVAILABLE_KEY,</div><div class="line">                    loadNativeLibraries);</div><div class="line">  &#125;</div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>ref<br><a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/ShortCircuitLocalReads.html" target="_blank" rel="external">Short-Circuit Local Reads</a><br><a href="https://www.zybuluo.com/jewes/note/37713" target="_blank" rel="external">详解HDFS Short Circuit Local Reads</a><br><a href="http://blog.csdn.net/jack85986370/article/details/51902871" target="_blank" rel="external">Unable to load native-hadoop library for your platform</a></p>
<hr>
<p>您的鼓励是我写作最大的动力</p>
<p>俗话说，投资效率是最好的投资。 如果您感觉我的文章质量不错，读后收获很大，预计能为您提高 10% 的工作效率，不妨小额捐助我一下，让我有动力继续写出更多好文章。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;Background&quot;&gt;&lt;a href=&quot;#Background&quot; class=&quot;headerlink&quot; title=&quot;Background&quot;&gt;&lt;/a&gt;&lt;b&gt;Background&lt;/b&gt;&lt;/h5&gt;&lt;p&gt;In HDFS, reads normally go thro
    
    </summary>
    
    
  </entry>
  
</feed>
