<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>t1ger的茶馆</title>
  <subtitle>头顶有光终是幻，足下生云未是仙</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://t1ger.github.io/"/>
  <updated>2017-06-23T09:47:53.094Z</updated>
  <id>https://t1ger.github.io/</id>
  
  <author>
    <name>t1ger</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>How to change MON IP</title>
    <link href="https://t1ger.github.io/2017/06/21/How-to-change-MON-IP/"/>
    <id>https://t1ger.github.io/2017/06/21/How-to-change-MON-IP/</id>
    <published>2017-06-21T09:12:32.000Z</published>
    <updated>2017-06-23T09:47:53.094Z</updated>
    
    <content type="html"><![CDATA[<p>Ceph 客户端和其他 Ceph 守护进程通过 ceph.conf 来发现 monitor。但是 monitor 之间是通过 mon map 而非 ceph.conf 来发现彼此</p>
<h5 id="修改-MON-IP-推荐-方法一"><a href="#修改-MON-IP-推荐-方法一" class="headerlink" title="修改 MON IP (推荐)方法一 "></a><b>修改 MON IP (推荐)方法一 </b></h5><p>仅修改 ceoh.conf 中 mon 的 IP 是不足以确保集群中的其他 monitor 收到更新的。<br>要修改一个 mon 的 IP，你必须先新增一个使用新 IP 的 monitor，确保这个新 mon 成功加入集群并形成法定人数。<br>然后，删除使用旧 IP 的 mon。最后，更新 ceph.conf ，以便客户端和其他守护进程可以知道新 mon 的 IP。<br>比如，假设现有 3 个 monitors：<br>[mon.node1]<br>        host = node1<br>        addr = 192.168.138.141:6789<br>[mon.node2]<br>        host = node2<br>        addr = 192.168.138.142:6789<br>[mon.node3]<br>        host = node3<br>        addr = 192.168.138.143:6789</p>
<p>把 mon.node3 变更为 mon.node4 。增加一个 mon.node4 ，host 设为 node4，IP 地址设为 192.168.138.144。先启动 mon.node4 ，再 删除 mon.node3 ，否则会破坏法定人数。</p>
<h5 id="修改-MON-IP-方法二"><a href="#修改-MON-IP-方法二" class="headerlink" title="修改 MON IP 方法二 "></a><b>修改 MON IP 方法二 </b></h5><p>有时，monitor 需要迁移到一个新的网络中、数据中心的其他位置或另一个数据中心。这时，需要为集群中所有的 monitors 生成一个新的 mon map （指定了新的 MON IP），再注入每一个 monitor 中</p>
<p>还以前面的 mon 配置为例。假定想把 monitor 从 192.168.138.x 网段改为 192.168.139.x 网段，这两个网段直接是不通的。执行下列步骤：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div></pre></td><td class="code"><pre><div class="line">1、获取 mon map</div><div class="line">ceph mon getmap -o &#123;tmp&#125;/&#123;filename&#125;</div><div class="line">2、下面的例子说明了 monmap 的内容。</div><div class="line">monmaptool --print &#123;tmp&#125;/&#123;filename&#125;</div><div class="line">3、删除已有的 monitors</div><div class="line">monmaptool --rm a --rm b --rm c &#123;tmp&#125;/&#123;filename&#125;</div><div class="line"></div><div class="line">monmaptool: monmap file &#123;tmp&#125;/&#123;filename&#125;</div><div class="line">monmaptool: removing a</div><div class="line">monmaptool: removing b</div><div class="line">monmaptool: removing c</div><div class="line">monmaptool: writing epoch 1 to &#123;tmp&#125;/&#123;filename&#125; (0 monitors)</div><div class="line">4、新增 monitor</div><div class="line">monmaptool --add node1 192.168.139.141:6789 --add node2 192.168.139.142:6789 --add node3 192.168.139.143:6789 &#123;tmp&#125;/&#123;filename&#125;</div><div class="line"></div><div class="line">monmaptool: monmap file &#123;tmp&#125;/&#123;filename&#125;</div><div class="line">monmaptool: writing epoch 1 to &#123;tmp&#125;/&#123;filename&#125; (3 monitors)</div><div class="line">5、检查 monmap 的新内容</div><div class="line">$ monmaptool --print &#123;tmp&#125;/&#123;filename&#125;</div><div class="line">monmaptool: monmap file &#123;tmp&#125;/&#123;filename&#125;</div><div class="line"></div><div class="line">此时，我们假定 monitor 已在新位置安装完毕。下面的步骤就是分发新的 monmap 并注入到各新 monitor 中。</div><div class="line">1、停止所有的 monitor 。必须停止 mon 守护进程才能进行 monmap 注入。</div><div class="line">2、注入 monmap。</div><div class="line">ceph-mon -i &#123;mon-id&#125; --inject-monmap &#123;tmp&#125;/&#123;filename&#125;</div><div class="line">3、重启各 monitors </div><div class="line"></div><div class="line"></div><div class="line">[root@admin ~]# ceph mon getmap -o /tmp/map</div><div class="line">got monmap epoch 5</div><div class="line">[root@admin ~]#  monmaptool --print /tmp/map </div><div class="line">monmaptool: monmap file /tmp/map</div><div class="line">epoch 5</div><div class="line">fsid d6d92de4-2a08-4bd6-a749-6c104c88fc40</div><div class="line">last_changed 2017-06-21 12:59:00.947896</div><div class="line">created 2017-05-27 11:43:56.428001</div><div class="line">0: 192.168.138.141:6789/0 mon.node1</div><div class="line">1: 192.168.138.142:6789/0 mon.node2</div><div class="line">2: 192.168.138.143:6789/0 mon.node3</div><div class="line">[root@admin ~]# monmaptool --rm node1 --rm node2 --rm node3 /tmp/map </div><div class="line">monmaptool: removing node1</div><div class="line">monmaptool: removing node2</div><div class="line">monmaptool: removing node3</div><div class="line">monmaptool: writing epoch 5 to tmp/map (0 monitors)</div><div class="line">[root@admin ~]# monmaptool --add node1 192.168.139.141:6789 --add node2 192.168.139.142:6789 --add node3 192.168.139.143:6789 /tmp/map</div><div class="line">monmaptool: monmap file /tmp/map</div><div class="line">monmaptool: writing epoch 5 to /tmp/map (3 monitors)</div><div class="line"></div><div class="line">[root@admin ~]#  monmaptool --print /tmp/map </div><div class="line">epoch 5</div><div class="line">fsid 224e376d-c5fe-4504-96bb-ea6332a19e61</div><div class="line">last_changed 2017-06-21 12:59:00.947896</div><div class="line">created 2017-05-27 11:43:56.428001</div><div class="line">0: 192.168.139.141:6789/0 mon.node1</div><div class="line">1: 192.168.139.142:6789/0 mon.node2</div><div class="line">2: 192.168.139.143:6789/0 mon.node3</div><div class="line"></div><div class="line"></div><div class="line">systemctl stop ceph-mon@node1</div><div class="line">systemctl stop ceph-mon@node2</div><div class="line">systemctl stop ceph-mon@node3</div><div class="line">ceph-mon -i node1 --inject-monmap tmp/map</div><div class="line">ceph-mon -i node2 --inject-monmap tmp/map</div><div class="line">ceph-mon -i node3 --inject-monmap tmp/map</div><div class="line">systemctl restart ceph-mon@node1</div><div class="line">systemctl restart ceph-mon@node2</div><div class="line">systemctl restart ceph-mon@node3</div></pre></td></tr></table></figure></p>
<h5 id="修改-MON-IP-方法三"><a href="#修改-MON-IP-方法三" class="headerlink" title="修改 MON IP 方法三 "></a><b>修改 MON IP 方法三 </b></h5><p>假如我们在机房迁移时候没有导出monmap,我们如何修改ip呢<br>这里我放弃使用导出monmap的方法而选择新建monmap，因为新建可以解决这里MON修改IP后无法启动提取monmap的问题。</p>
<p>假定，MON的IP从192.168.56.x 迁到了172.16.56.x ，我们首先创建一个使用新的IP的monmap，这里还是使用了三个MON：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div></pre></td><td class="code"><pre><div class="line">[root@node1 ~]# cat /etc/ceph/ceph.conf |grep fsid</div><div class="line">fsid = d6d92de4-2a08-4bd6-a749-6c104c88fc40</div><div class="line">[root@node1 ~]# monmaptool -h</div><div class="line"> usage: [--print] [--create [--clobber][--fsid uuid]] [--generate] [--set-initial-members] [--add name 1.2.3.4:567] [--rm name] &lt;mapfilename&gt;</div><div class="line"> </div><div class="line">[root@node1 ~]# monmaptool --create --fsid d6d92de4-2a08-4bd6-a749-6c104c88fc40 --add node1 172.16.56.141 --add node2 172.16.56.142 --add  node3 172.16.56.143 /tmp/monmap</div><div class="line">monmaptool: monmap file /tmp/monmap</div><div class="line">monmaptool: set fsid to d6d92de4-2a08-4bd6-a749-6c104c88fc40</div><div class="line">monmaptool: writing epoch 0 to /tmp/monmap (3 monitors)</div><div class="line"></div><div class="line">[root@ceph-1 ~]# monmaptool --print /tmp/monmap </div><div class="line">monmaptool: monmap file /tmp/monmap</div><div class="line">epoch 0</div><div class="line">fsid d6d92de4-2a08-4bd6-a749-6c104c88fc40</div><div class="line">last_changed 2017-06-21 12:59:00.947896</div><div class="line">created 2017-05-27 11:43:56.428001</div><div class="line">0: 172.16.56.141:6789/0 mon.node1</div><div class="line">1: 172.16.56.142:6789/0 mon.node2</div><div class="line">2: 172.16.56.143:6789/0 mon.node3</div><div class="line"></div><div class="line">通过打印monmap可以看到已经成功添加了三个MON，只是这里的epoch为0，实际的epoch肯定大于0的，</div><div class="line">不用担心，monmaptool的代码里面写死了是0，并且不影响注入到MON的数据库里</div><div class="line"></div><div class="line">[root@node1 ~]# scp /tmp/monmap node2:/tmp/monmap</div><div class="line">[root@node1 ~]# scp /tmp/monmap node3:/tmp/monmap</div><div class="line">[root@node1 ~]# ceph-mon -i node1 --inject-monmap /tmp/monmap </div><div class="line">[root@node2 ~]# ceph-mon -i node2 --inject-monmap /tmp/monmap </div><div class="line">[root@node3 ~]# ceph-mon -i node3 --inject-monmap /tmp/monmap</div><div class="line"></div><div class="line">注意这里是在三个主机上分别注入的，最后修改配置文件，发放到各个节点，开启MON服务</div><div class="line"></div><div class="line">[root@node1 cluster]# vim ceph.conf </div><div class="line">[root@node1 cluster]# cat /root/cluster/ceph.conf |grep mon</div><div class="line">mon_initial_members = node1,node2,node3</div><div class="line">mon_host = 172.16.56.141,172.16.56.142,172.16.56.143</div><div class="line">[root@node1 cluster]# ceph-deploy --overwrite-conf config push node1 node2 node3</div><div class="line">[root@node1 ~]# systemctl start ceph.target</div><div class="line">[root@node2 ~]# systemctl start ceph.target</div><div class="line">[root@node3 ~]# systemctl start ceph.target</div><div class="line">[root@node1 cluster]# ceph -s</div><div class="line">    cluster d6d92de4-2a08-4bd6-a749-6c104c88fc40</div><div class="line">     health HEALTH_WARN</div><div class="line">            clock skew detected on mon.node2, mon.node3</div><div class="line">            Monitor clock skew detected </div><div class="line">     monmap e6: 3 mons at &#123;node1=172.16.56.141:6789/0,node2=172.16.56.142:6789/0,node3=172.16.56.143:6789/0&#125;</div><div class="line">            election epoch 6, quorum 0,1,2 node1,node2,node3</div><div class="line">     osdmap e30: 3 osds: 3 up, 3 in</div><div class="line">      pgmap v48: 64 pgs, 1 pools, 0 bytes data, 0 objects</div><div class="line">            101 MB used, 6125 GB / 6125 GB avail</div><div class="line">                  64 active+clean</div><div class="line"></div><div class="line">这样我们就完成了MON的IP迁移</div></pre></td></tr></table></figure>
<h5 id="修改-MON-IP-方法四"><a href="#修改-MON-IP-方法四" class="headerlink" title="修改 MON IP 方法四"></a><b>修改 MON IP 方法四</b></h5><p>如果三个MON的数据库都被损坏了,我们可以参考<a href="http://www.zphj1987.com/2016/09/20/Ceph%E7%9A%84Mon%E6%95%B0%E6%8D%AE%E9%87%8D%E6%96%B0%E6%9E%84%E5%BB%BA%E5%B7%A5%E5%85%B7/" target="_blank" rel="external">这里</a>重建MON<br>主要使用新工具ceph-monstore-tool来重建丢失的MON数据库。当然，如果每天备份一次MON数据库，就不用担心故障了</p>
<h5 id="Monitor的备份"><a href="#Monitor的备份" class="headerlink" title="Monitor的备份 "></a><b>Monitor的备份 </b></h5><p>备份,重要的事情强调三遍.简单讲基本思路就是，停止一个MON，然后将这个MON的数据库压缩保存到其他路径，再开启MON，文中提到了之所以要停止MON是要保证levelDB数据库的完整性.</p>
<p>当某个集群的所有的MON节点都挂掉之后，我们可以将最新的备份的数据库解压到任意一个节点上，用同样的方法新建monmap，注入，开启MON，推送config,重启OSD就好了</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">systemctl stop ceph-mon@node1</div><div class="line">tar czf /var/backups/ceph-mon-backup_$(date +&apos;%a&apos;).tar.gz /var/lib/ceph/mon</div><div class="line">systemctl start ceph-mon@node1</div><div class="line">#for safety, copy it to other nodes</div><div class="line">scp /var/backups/* someNode:/backup/</div></pre></td></tr></table></figure>
<p>ref</p>
<p><a href="http://www.xuxiaopang.com/2016/10/26/exp-monitor-operation/" target="_blank" rel="external">monitor的增删改备</a><br><a href="https://lihaijing.gitbooks.io/ceph-handbook/content/Operation/modify_mon_ip.html" target="_blank" rel="external">修改 MON IP</a><br><a href="https://github.com/angapov/ceph-systemd" target="_blank" rel="external">Systemd script for CEPH object storage</a></p>
<hr>
<p>您的鼓励是我写作最大的动力</p>
<p>俗话说，投资效率是最好的投资。 如果您感觉我的文章质量不错，读后收获很大，预计能为您提高 10% 的工作效率，不妨小额捐助我一下，让我有动力继续写出更多好文章。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Ceph 客户端和其他 Ceph 守护进程通过 ceph.conf 来发现 monitor。但是 monitor 之间是通过 mon map 而非 ceph.conf 来发现彼此&lt;/p&gt;
&lt;h5 id=&quot;修改-MON-IP-推荐-方法一&quot;&gt;&lt;a href=&quot;#修改-MON-
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>monitor的添加或删除</title>
    <link href="https://t1ger.github.io/2017/06/15/monitor%E7%9A%84%E6%B7%BB%E5%8A%A0%E6%88%96%E5%88%A0%E9%99%A4/"/>
    <id>https://t1ger.github.io/2017/06/15/monitor的添加或删除/</id>
    <published>2017-06-15T11:09:13.000Z</published>
    <updated>2017-06-21T07:07:49.788Z</updated>
    
    <content type="html"><![CDATA[<p>一般来说，在实际运行中，ceph monitor的个数是2n+1(n&gt;=0)个，在线上至少3个，只要正常的节点数&gt;=n+1，ceph的paxos算法能保证系统的正常运行。所以,对于3个节点，同时只能挂掉一个。建议（但不是强制）部署奇数个 monitor ,不建议把监视器和 OSD 置于同一主机上,后续如果要增加，请一次增加 2 个</p>
<p>一般来说，同时挂掉2个节点的概率比较小，但是万一挂掉2个呢？<br>如果ceph的monitor节点超过半数挂掉，paxos算法就无法正常进行仲裁(quorum)，此时，ceph集群会阻塞对集群的操作，直到超过半数的monitor节点恢复</p>
<ul>
<li>如果挂掉的2个节点至少有一个可以恢复，也就是monitor的监控数据还是OK的，那么只需要重启ceph-mon进程即可。所以，对于monitor，最好运行在RAID的机器上。这样，即使机器出现故障，恢复也比较容易</li>
<li>如果挂掉的2个节点的监控数据都损坏了呢？</li>
</ul>
<p>带着这些疑问,后续几篇文章我们来提供了一些常见场景的处理方法，包括增加monitor，移除某个monitor，机房搬迁需要修改IP，备份MON的数据库等</p>
<p>本文讲一下如何对一个已经存在的ceph storage cluster添加或删除一个监控节点.<br>用 ceph-deploy 增加和删除监视器很简单，只要一个命令就可以增加或删除一或多个监视器<br>大致步骤:<br>1.环境准备<br>2.安装软件<br>3.添加节点</p>
<h5 id="添加一个monitor-ceph-deploy"><a href="#添加一个monitor-ceph-deploy" class="headerlink" title="添加一个monitor(ceph-deploy )"></a><b>添加一个monitor(ceph-deploy )</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div></pre></td><td class="code"><pre><div class="line">[neo@admin cluster]$ sudo ceph -s</div><div class="line">    cluster d6d92de4-2a08-4bd6-a749-6c104c88fc40</div><div class="line">     health HEALTH_OK</div><div class="line">     monmap e2: 2 mons at &#123;node1=192.168.138.141:6789/0,node2=192.168.138.142:6789/0&#125;</div><div class="line">            election epoch 62, quorum 0,1 node1,node2</div><div class="line">     osdmap e246: 9 osds: 9 up, 9 in</div><div class="line">            flags sortbitwise,require_jewel_osds</div><div class="line">      pgmap v1167: 256 pgs, 1 pools, 4 bytes data, 1 objects</div><div class="line">            350 MB used, 18377 GB / 18378 GB avail</div><div class="line">                 256 active+clean</div><div class="line">[neo@admin cluster]$ cat ~/cluster/ceph.conf |grep mon     </div><div class="line">mon_initial_members = node1, node2</div><div class="line">mon_host = 192.168.138.141,192.168.138.142</div><div class="line"></div><div class="line">#修改配置文件,添加新的节点</div><div class="line">[neo@admin cluster]$ vi ceph.conf </div><div class="line">[neo@admin cluster]$ cat ~/cluster/ceph.conf |grep mon</div><div class="line">mon_initial_members = node1, node2, node3</div><div class="line">mon_host = 192.168.138.141,192.168.138.142,192.168.138.143</div><div class="line"></div><div class="line">[neo@admin cluster]$ ceph-deploy --overwrite-conf config push  node1 node2</div><div class="line"></div><div class="line">#添加MON，注意如果如果要添加多个MON，需要一个个add</div><div class="line">需要注意,往存在的cluster里添加monitor时，需要修改配置文件ceph.conf在global章节中</div><div class="line">指定public network或者mon.nodeX中指定public addr，配置文件中写成代</div><div class="line">下划线的public_network =也是可以的</div><div class="line"></div><div class="line">[neo@admin cluster]$ ceph-deploy --overwrite-conf mon add node3</div><div class="line">[ceph_deploy.conf][DEBUG ] found configuration file at: /home/neo/.cephdeploy.conf</div><div class="line">[ceph_deploy.cli][INFO  ] Invoked (1.5.37): /usr/bin/ceph-deploy --overwrite-conf mon add node3</div><div class="line">[ceph_deploy.cli][INFO  ] ceph-deploy options:</div><div class="line">[ceph_deploy.cli][INFO  ]  username                      : None</div><div class="line">[ceph_deploy.cli][INFO  ]  verbose                       : False</div><div class="line">[ceph_deploy.cli][INFO  ]  overwrite_conf                : True</div><div class="line">[ceph_deploy.cli][INFO  ]  subcommand                    : add</div><div class="line">[ceph_deploy.cli][INFO  ]  quiet                         : False</div><div class="line">[ceph_deploy.cli][INFO  ]  cd_conf                       : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x164fc68&gt;</div><div class="line">[ceph_deploy.cli][INFO  ]  cluster                       : ceph</div><div class="line">[ceph_deploy.cli][INFO  ]  mon                           : [&apos;node3&apos;]</div><div class="line">[ceph_deploy.cli][INFO  ]  func                          : &lt;function mon at 0x1647320&gt;</div><div class="line">[ceph_deploy.cli][INFO  ]  address                       : None</div><div class="line">[ceph_deploy.cli][INFO  ]  ceph_conf                     : None</div><div class="line">[ceph_deploy.cli][INFO  ]  default_release               : False</div><div class="line">[ceph_deploy.mon][INFO  ] ensuring configuration of new mon host: node3</div><div class="line">[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to node3</div><div class="line">[node3][DEBUG ] connection detected need for sudo</div><div class="line">[node3][DEBUG ] connected to host: node3 </div><div class="line">[node3][DEBUG ] detect platform information from remote host</div><div class="line">[node3][DEBUG ] detect machine type</div><div class="line">[node3][DEBUG ] write cluster configuration to /etc/ceph/&#123;cluster&#125;.conf</div><div class="line">[ceph_deploy.mon][DEBUG ] Adding mon to cluster ceph, host node3</div><div class="line">[ceph_deploy.mon][DEBUG ] using mon address by resolving host: 192.168.138.143</div><div class="line">[ceph_deploy.mon][DEBUG ] detecting platform for host node3 ...</div><div class="line">[node3][DEBUG ] connection detected need for sudo</div><div class="line">[node3][DEBUG ] connected to host: node3 </div><div class="line">[node3][DEBUG ] detect platform information from remote host</div><div class="line">[node3][DEBUG ] detect machine type</div><div class="line">[node3][DEBUG ] find the location of an executable</div><div class="line">[ceph_deploy.mon][INFO  ] distro info: CentOS Linux 7.3.1611 Core</div><div class="line">[node3][DEBUG ] determining if provided host has same hostname in remote</div><div class="line">[node3][DEBUG ] get remote short hostname</div><div class="line">[node3][DEBUG ] adding mon to node3</div><div class="line">[node3][DEBUG ] get remote short hostname</div><div class="line">[node3][DEBUG ] write cluster configuration to /etc/ceph/&#123;cluster&#125;.conf</div><div class="line">[node3][DEBUG ] create the mon path if it does not exist</div><div class="line">[node3][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-node3/done</div><div class="line">[node3][DEBUG ] create a done file to avoid re-doing the mon deployment</div><div class="line">[node3][DEBUG ] create the init path if it does not exist</div><div class="line">[node3][INFO  ] Running command: sudo systemctl enable ceph.target</div><div class="line">[node3][INFO  ] Running command: sudo systemctl enable ceph-mon@node3</div><div class="line">[node3][INFO  ] Running command: sudo systemctl start ceph-mon@node3</div><div class="line">[node3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.node3.asok mon_status</div><div class="line">[node3][WARNIN] monitor node3 does not exist in monmap</div><div class="line">[node3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.node3.asok mon_status</div><div class="line">[node3][DEBUG ] ********************************************************************************</div><div class="line">[node3][DEBUG ] status for monitor: mon.node3</div><div class="line">[node3][DEBUG ] &#123;</div><div class="line">[node3][DEBUG ]   &quot;election_epoch&quot;: 0, </div><div class="line">[node3][DEBUG ]   &quot;extra_probe_peers&quot;: [], </div><div class="line">[node3][DEBUG ]   &quot;monmap&quot;: &#123;</div><div class="line">[node3][DEBUG ]     &quot;created&quot;: &quot;2017-05-27 11:43:56.428001&quot;, </div><div class="line">[node3][DEBUG ]     &quot;epoch&quot;: 2, </div><div class="line">[node3][DEBUG ]     &quot;fsid&quot;: &quot;d6d92de4-2a08-4bd6-a749-6c104c88fc40&quot;, </div><div class="line">[node3][DEBUG ]     &quot;modified&quot;: &quot;2017-06-20 11:52:25.801159&quot;, </div><div class="line">[node3][DEBUG ]     &quot;mons&quot;: [</div><div class="line">[node3][DEBUG ]       &#123;</div><div class="line">[node3][DEBUG ]         &quot;addr&quot;: &quot;192.168.138.141:6789/0&quot;, </div><div class="line">[node3][DEBUG ]         &quot;name&quot;: &quot;node1&quot;, </div><div class="line">[node3][DEBUG ]         &quot;rank&quot;: 0</div><div class="line">[node3][DEBUG ]       &#125;, </div><div class="line">[node3][DEBUG ]       &#123;</div><div class="line">[node3][DEBUG ]         &quot;addr&quot;: &quot;192.168.138.142:6789/0&quot;, </div><div class="line">[node3][DEBUG ]         &quot;name&quot;: &quot;node2&quot;, </div><div class="line">[node3][DEBUG ]         &quot;rank&quot;: 1</div><div class="line">[node3][DEBUG ]       &#125;</div><div class="line">[node3][DEBUG ]     ]</div><div class="line">[node3][DEBUG ]   &#125;, </div><div class="line">[node3][DEBUG ]   &quot;name&quot;: &quot;node3&quot;, </div><div class="line">[node3][DEBUG ]   &quot;outside_quorum&quot;: [], </div><div class="line">[node3][DEBUG ]   &quot;quorum&quot;: [], </div><div class="line">[node3][DEBUG ]   &quot;rank&quot;: -1, </div><div class="line">[node3][DEBUG ]   &quot;state&quot;: &quot;synchronizing&quot;, </div><div class="line">[node3][DEBUG ]   &quot;sync&quot;: &#123;</div><div class="line">[node3][DEBUG ]     &quot;sync_cookie&quot;: 1040187393, </div><div class="line">[node3][DEBUG ]     &quot;sync_provider&quot;: &quot;mon.0 192.168.138.141:6789/0&quot;, </div><div class="line">[node3][DEBUG ]     &quot;sync_start_version&quot;: 2944</div><div class="line">[node3][DEBUG ]   &#125;, </div><div class="line">[node3][DEBUG ]   &quot;sync_provider&quot;: []</div><div class="line">[node3][DEBUG ] &#125;</div><div class="line">[node3][DEBUG ] ********************************************************************************</div><div class="line">[node3][INFO  ] monitor: mon.node3 is currently at the state of synchronizing</div></pre></td></tr></table></figure>
<p>错误1:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">[node3][ERROR ] admin_socket: exception getting command descriptions: [Errno 2] No such file or directory</div><div class="line">[node3][WARNIN] monitor: mon.node3, might not be running yet</div><div class="line">[node3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.node3.asok mon_status</div><div class="line">[node3][ERROR ] admin_socket: exception getting command descriptions: [Errno 2] No such file or directory</div><div class="line">[node3][WARNIN] monitor node3 does not exist in monmap</div><div class="line">[node3][WARNIN] neither `public_addr` nor `public_network` keys are defined for monitors</div><div class="line">[node3][WARNIN] monitors may not be able to form quorum</div></pre></td></tr></table></figure></p>
<p>原因: 未配置public network</p>
<h5 id="添加一个monitor-手动"><a href="#添加一个monitor-手动" class="headerlink" title="添加一个monitor(手动)"></a><b>添加一个monitor(手动)</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line">添加之前查看当前节点 ceph mon stat</div><div class="line"></div><div class="line">1、在目标节点上，新建 mon 的默认目录。&#123;mon-id&#125; 一般取为节点的 hostname 。</div><div class="line">ssh &#123;new-mon-host&#125;</div><div class="line">sudo mkdir /var/lib/ceph/mon/ceph-&#123;mon-id&#125;</div><div class="line">2、创建一个临时目录（和第 1 步中的目录不同，添加 mon 完毕后需要删除该临时目录），来存放新增 mon 所需的各种文件，</div><div class="line">mkdir &#123;tmp&#125;</div><div class="line">3、获取 mon 的 keyring 文件，保存在临时目录下。</div><div class="line">ceph auth get mon. -o &#123;tmp&#125;/&#123;key-filename&#125;</div><div class="line">4、获取集群的 mon map 并保存到临时目录下。</div><div class="line">ceph mon getmap -o &#123;tmp&#125;/&#123;map-filename&#125;</div><div class="line">5.Optional. 更新所有mon节点的配置文件，添加新节点的IP地址到ceph.conf [global]字段的mon_host</div><div class="line">[mon.node3] </div><div class="line">        host                  = node3</div><div class="line">        mon addr              = 192.168.138.143:6789</div><div class="line">		</div><div class="line">6、格式化在第 1 步中建立的 mon 数据目录。需要指定 mon map 文件的路径（获取法定人数的信息和集群的 fsid ）和 keyring 文件的路径。</div><div class="line">sudo ceph-mon -i &#123;mon-id&#125; --mkfs --monmap &#123;tmp&#125;/&#123;map-filename&#125; --keyring &#123;tmp&#125;/&#123;key-filename&#125;</div><div class="line">7、启动节点上的 mon 进程，它会自动加入集群。守护进程需要知道绑定到哪个 IP 地址，可以通过 --public-addr &#123;ip:port&#125; 选择指定，或在 ceph.conf 文件中进行配置 mon addr。</div><div class="line">ceph-mon -i &#123;mon-id&#125; --public-addr &#123;ip:port&#125;</div><div class="line"></div><div class="line">[root@node3 ~]# mkdir /var/lib/ceph/mon/ceph-node3/ -p</div><div class="line">[root@node3 ~]# mkdir /var/lib/ceph/tmp -p</div><div class="line">[root@node3 ~]# cd /var/lib/ceph/mon/ceph-node3/</div><div class="line">[root@node3 ceph-node3]# ceph auth get mon. -o ../../tmp/key-node3</div><div class="line">exported keyring for mon.</div><div class="line">[root@node3 ceph-node3]# ceph mon getmap -o ../../tmp/map-node3</div><div class="line">2017-06-21 12:56:30.078870 7fd654086700  0 -- :/2208778235 &gt;&gt; 192.168.138.143:6789/0 pipe(0x7fd65805cc80 sd=3 :0 s=1 pgs=0 cs=0 l=1 c=0x7fd65805df40).fault</div><div class="line">got monmap epoch 4</div><div class="line">[root@node3 ceph-node3]# ceph-mon  -i node3 --mkfs --monmap ../../tmp/map-node3  --keyring ../../tmp/key-node3 </div><div class="line">ceph-mon: set fsid to d6d92de4-2a08-4bd6-a749-6c104c88fc40</div><div class="line">ceph-mon: created monfs at /var/lib/ceph/mon/ceph-node3 for mon.node3</div><div class="line"></div><div class="line">#启动节点上的 mon 进程，它会自动加入集群,此步骤可略</div><div class="line">[root@node3 ceph-node3]# ceph mon add node3 192.169.138.143:6789</div><div class="line">[root@node3 ceph-node3]# ceph-mon -i node3 --public-addr 192.169.138.143:6789</div></pre></td></tr></table></figure>
<h5 id="删除一个monitor-ceph-deploy"><a href="#删除一个monitor-ceph-deploy" class="headerlink" title="删除一个monitor(ceph-deploy )"></a><b>删除一个monitor(ceph-deploy )</b></h5><p>这里我们删除node3节点的mon,修改部署目录的配置文件，去除node3及其IP,再推送到三个节点<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div></pre></td><td class="code"><pre><div class="line">[neo@admin cluster]$ cat ~/cluster/ceph.conf |grep mon     </div><div class="line">mon_initial_members = node1, node2, node3</div><div class="line">mon_host = 192.168.138.141,192.168.138.142,192.168.138.143</div><div class="line"></div><div class="line">[neo@admin cluster]$ ceph-deploy --overwrite-conf config push node1 node2 node3</div><div class="line">[neo@admin cluster]$ ceph-deploy mon destroy node3</div><div class="line">[ceph_deploy.conf][DEBUG ] found configuration file at: /home/neo/.cephdeploy.conf</div><div class="line">[ceph_deploy.cli][INFO  ] Invoked (1.5.37): /usr/bin/ceph-deploy mon destroy node3</div><div class="line">[ceph_deploy.cli][INFO  ] ceph-deploy options:</div><div class="line">[ceph_deploy.cli][INFO  ]  username                      : None</div><div class="line">[ceph_deploy.cli][INFO  ]  verbose                       : False</div><div class="line">[ceph_deploy.cli][INFO  ]  overwrite_conf                : False</div><div class="line">[ceph_deploy.cli][INFO  ]  subcommand                    : destroy</div><div class="line">[ceph_deploy.cli][INFO  ]  quiet                         : False</div><div class="line">[ceph_deploy.cli][INFO  ]  cd_conf                       : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x1481c68&gt;</div><div class="line">[ceph_deploy.cli][INFO  ]  cluster                       : ceph</div><div class="line">[ceph_deploy.cli][INFO  ]  mon                           : [&apos;node3&apos;]</div><div class="line">[ceph_deploy.cli][INFO  ]  func                          : &lt;function mon at 0x1479320&gt;</div><div class="line">[ceph_deploy.cli][INFO  ]  ceph_conf                     : None</div><div class="line">[ceph_deploy.cli][INFO  ]  default_release               : False</div><div class="line">[ceph_deploy.mon][DEBUG ] Removing mon from node3</div><div class="line">[node3][DEBUG ] connection detected need for sudo</div><div class="line">[node3][DEBUG ] connected to host: node3 </div><div class="line">[node3][DEBUG ] detect platform information from remote host</div><div class="line">[node3][DEBUG ] detect machine type</div><div class="line">[node3][DEBUG ] find the location of an executable</div><div class="line">[node3][DEBUG ] get remote short hostname</div><div class="line">[node3][INFO  ] Running command: sudo ceph --cluster=ceph -n mon. -k /var/lib/ceph/mon/ceph-node3/keyring mon remove node3</div><div class="line">[node3][WARNIN] removing mon.node3 at 192.168.138.143:6789/0, there will be 2 monitors</div><div class="line">[node3][INFO  ] polling the daemon to verify it stopped</div><div class="line">[node3][INFO  ] Running command: sudo systemctl stop ceph-mon@node3.service</div><div class="line">[node3][INFO  ] Running command: sudo mkdir -p /var/lib/ceph/mon-removed</div><div class="line">[node3][DEBUG ] move old monitor data</div><div class="line"></div><div class="line">[neo@admin cluster]$ sudo ceph -s</div><div class="line">    cluster d6d92de4-2a08-4bd6-a749-6c104c88fc40</div><div class="line">     health HEALTH_OK</div><div class="line">     monmap e2: 2 mons at &#123;node1=192.168.138.141:6789/0,node2=192.168.138.142:6789/0&#125;</div><div class="line">            election epoch 62, quorum 0,1 node1,node2</div><div class="line">     osdmap e246: 9 osds: 9 up, 9 in</div><div class="line">            flags sortbitwise,require_jewel_osds</div><div class="line">      pgmap v1167: 256 pgs, 1 pools, 4 bytes data, 1 objects</div><div class="line">            350 MB used, 18377 GB / 18378 GB avail</div><div class="line">                 256 active+clean</div><div class="line">				 </div><div class="line">备注:</div><div class="line">ceph-deploy删除MON的时候调用的指令是ceph mon remove node3,删除的MON的文件夹被移到了/var/lib/ceph/mon-removed</div></pre></td></tr></table></figure></p>
<p>注意： 确保你删除某个 Mon 后，其余 Mon 仍能达成一致。如果不可能，删除它之前可能需要先增加一个</p>
<h5 id="删除一个monitor-手动"><a href="#删除一个monitor-手动" class="headerlink" title="删除一个monitor(手动)"></a><b>删除一个monitor(手动)</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">1、停止 mon 进程。</div><div class="line">[neo@admin cluster]$ sudo systemctl stop ceph-mon@node3</div><div class="line">2、从集群中删除 monitor。</div><div class="line">[neo@admin cluster]$ sudo ceph mon remove node3</div><div class="line">removing mon.node3 at 192.168.138.143:6789/0, there will be 2 monitors</div><div class="line">[neo@admin cluster]$ sudo ceph -s</div><div class="line">    cluster d6d92de4-2a08-4bd6-a749-6c104c88fc40</div><div class="line">     health HEALTH_OK</div><div class="line">     monmap e4: 2 mons at &#123;node1=192.168.138.141:6789/0,node2=192.168.138.142:6789/0&#125;</div><div class="line">            election epoch 74, quorum 0,1 node1,node2</div><div class="line">     osdmap e264: 9 osds: 9 up, 9 in</div><div class="line">            flags sortbitwise,require_jewel_osds</div><div class="line">      pgmap v1218: 256 pgs, 1 pools, 4 bytes data, 1 objects</div><div class="line">            354 MB used, 18377 GB / 18378 GB avail</div><div class="line">                 256 active+clean</div><div class="line"></div><div class="line">3、从 ceph.conf 中移除 mon 的入口部分（如果有）。</div></pre></td></tr></table></figure>
<h5 id="删除-Monitor（从不健康的集群中）"><a href="#删除-Monitor（从不健康的集群中）" class="headerlink" title="删除 Monitor（从不健康的集群中）"></a><b>删除 Monitor（从不健康的集群中）</b></h5><p>从一个不健康的集群（比如集群中的 monitor 无法达成法定人数）中删除 ceph-mon 守护进程。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">1、停止集群中所有的 ceph-mon 守护进程。</div><div class="line">ssh &#123;mon-host&#125;</div><div class="line">systemctl stop ceph-mon@mon-host</div><div class="line"># and repeat for all mons</div><div class="line">2、确认存活的 mon 并登录该节点。</div><div class="line">ssh &#123;mon-host&#125;</div><div class="line">3、提取 mon map。</div><div class="line">ceph-mon -i &#123;mon-id&#125; --extract-monmap &#123;map-path&#125;</div><div class="line"># in most cases, that&apos;s</div><div class="line">ceph-mon -i `hostname` --extract-monmap /tmp/monmap</div><div class="line">4、删除未存活或有问题的的 monitor。比如，有 3 个 monitors，mon.node1 、mon.node2 和 mon.node3，现在仅有 mon.node1 存活，执行下列步骤：</div><div class="line">monmaptool &#123;map-path&#125; --rm &#123;mon-id&#125;</div><div class="line"># for example,</div><div class="line">monmaptool /tmp/monmap --rm node2</div><div class="line">monmaptool /tmp/monmap --rm node3</div><div class="line">5、向存活的 monitor(s) 注入修改后的 mon map。比如，把 mon map 注入 mon.node1，执行下列步骤：</div><div class="line">ceph-mon -i &#123;mon-id&#125; --inject-monmap &#123;map-path&#125;</div><div class="line"># for example,</div><div class="line">ceph-mon -i a --inject-monmap /tmp/monmap</div><div class="line">6、启动存活的 monitor。</div><div class="line">7、确认 monitor 是否达到法定人数（ ceph -s ）。</div><div class="line">8、你可能需要把已删除的 monitor 的数据目录 /var/lib/ceph/mon 归档到一个安全的位置。或者，如果你确定剩下的 monitor 是健康的且数量足够，也可以直接删除数据目录。</div></pre></td></tr></table></figure>
<h5 id="挂掉的2个节点的监控数据的恢复"><a href="#挂掉的2个节点的监控数据的恢复" class="headerlink" title="挂掉的2个节点的监控数据的恢复"></a><b>挂掉的2个节点的监控数据的恢复</b></h5><p>前边我们说如果挂掉的2个节点的监控数据都损坏了呢？恢复方法请参考<a href="http://www.cnblogs.com/hustcat/p/3925971.html" target="_blank" rel="external">这里</a></p>
<p>ref<br><a href="http://www.xuxiaopang.com/2016/10/26/exp-monitor-operation/" target="_blank" rel="external">monitor的增删改备</a><br><a href="http://blog.csdn.net/scaleqiao/article/details/50513655" target="_blank" rel="external">Ceph Monitor挂了之后对集群的影响</a><br><a href="https://lihaijing.gitbooks.io/ceph-handbook/content/Operation/add_rm_mon.html" target="_blank" rel="external">增加/删除 Monitor</a><br><a href="https://github.com/thesues/cephdoc/blob/master/ceph-deploy-cn.markdown" target="_blank" rel="external">thesues/cephdoc</a><br><a href="http://blog.sina.com.cn/s/blog_8ea8e9d50102xhbq.html" target="_blank" rel="external">Ceph集群</a></p>
<hr>
<p>您的鼓励是我写作最大的动力</p>
<p>俗话说，投资效率是最好的投资。 如果您感觉我的文章质量不错，读后收获很大，预计能为您提高 10% 的工作效率，不妨小额捐助我一下，让我有动力继续写出更多好文章。    </p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;一般来说，在实际运行中，ceph monitor的个数是2n+1(n&amp;gt;=0)个，在线上至少3个，只要正常的节点数&amp;gt;=n+1，ceph的paxos算法能保证系统的正常运行。所以,对于3个节点，同时只能挂掉一个。建议（但不是强制）部署奇数个 monitor ,不建议
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>ceph-Monitor clock skew detected</title>
    <link href="https://t1ger.github.io/2017/06/13/ceph-Monitor-clock-skew-detected/"/>
    <id>https://t1ger.github.io/2017/06/13/ceph-Monitor-clock-skew-detected/</id>
    <published>2017-06-13T07:00:09.000Z</published>
    <updated>2017-06-13T06:59:44.990Z</updated>
    
    <content type="html"><![CDATA[<p>ceph异常警告:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">[root@admin ~]# ceph -s</div><div class="line">    cluster d6d92de4-2a08-4bd6-a749-6c104c88fc40</div><div class="line">     health HEALTH_WARN</div><div class="line">            clock skew detected on mon.node2</div><div class="line">            Monitor clock skew detected </div><div class="line">     monmap e1: 3 mons at &#123;node1=192.168.138.141:6789/0,node2=192.168.138.142:6789/0,node3=192.168.138.143:6789/0&#125;</div><div class="line">            election epoch 46, quorum 0,1,2 node1,node2,node3</div><div class="line">     osdmap e218: 9 osds: 9 up, 9 in</div><div class="line">            flags sortbitwise,require_jewel_osds</div><div class="line">      pgmap v1007: 256 pgs, 1 pools, 4 bytes data, 1 objects</div><div class="line">            345 MB used, 18377 GB / 18378 GB avail</div><div class="line">                 256 active+clean</div><div class="line">				 			 </div><div class="line">[root@admin ~]# ceph health detail</div><div class="line">HEALTH_WARN clock skew detected on mon.node2, mon.node3; Monitor clock skew detected </div><div class="line">mon.node2 addr 192.168.138.142:6789/0 clock skew 0.434161s &gt; max 0.05s (latency 0.00740637s)</div><div class="line">mon.node3 addr 192.168.138.143:6789/0 clock skew 0.687451s &gt; max 0.05s (latency 0.00722567s)</div></pre></td></tr></table></figure></p>
<p>解决方法:</p>
<ul>
<li><p>方法一:配置ntp server<br>本来以为配置ntp server了,时间应该就一致了,原来ceph默认容忍的时间偏差不到1秒,随意只能用本地的ntp server了<br>下面来说说配置ntp server(If Iptables is running, allow NTP port. NTP uses 123/UDP.)<br>chrony_server</p>
<p>  1.install chrony</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">yum -y install chrony</div></pre></td></tr></table></figure>
<p>  2.config chrony</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">[root@admin ~]# cat /etc/chrony.conf </div><div class="line"># These servers were defined in the installation:</div><div class="line">server 3.centos.pool.ntp.org iburst</div><div class="line">server 0.centos.pool.ntp.org iburst</div><div class="line">server cn.pool.ntp.org iburst</div><div class="line">server 1.centos.pool.ntp.org iburst</div><div class="line">server 2.centos.pool.ntp.org iburst</div><div class="line"># Serve time even if not synchronized to any NTP server.</div><div class="line">local stratum 10</div><div class="line">....</div><div class="line">allow 192.168/16</div></pre></td></tr></table></figure>
<p>  3.run chrony</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">systemctl enable chronyd</div><div class="line">systemctl start chronyd</div></pre></td></tr></table></figure>
<p>  4.timedatectl</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">## 查看时间</div><div class="line">timedatectl</div><div class="line">## 开启ntp时间同步</div><div class="line">timedatectl set-ntp true</div></pre></td></tr></table></figure>
<p>  5.chronyc</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">## 查看ntp_servers状态</div><div class="line">chronyc sources -v</div><div class="line">## 查看ntp_sync状态</div><div class="line">chronyc sourcestats -v</div><div class="line">## 查看ntp_servers 是否在线</div><div class="line">chronyc activity -v</div><div class="line">## 查看ntp时间详细信息</div><div class="line">chronyc tracking -v</div></pre></td></tr></table></figure>
<p>  chrony_client<br>  1.install chrony</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">yum -y install chrony</div></pre></td></tr></table></figure>
<p>  2.config chrony</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[root@node3 ~]# cat /etc/chrony.conf</div><div class="line"># These servers were defined in the installation:</div><div class="line">server 192.168.138.140 iburst</div></pre></td></tr></table></figure>
<p>  3.run chrony</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">systemctl enable chronyd</div><div class="line">systemctl start chronyd</div></pre></td></tr></table></figure>
<p>  4.timedatectl</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">## 查看时间</div><div class="line">timedatectl</div><div class="line">## 开启ntp时间同步</div><div class="line">timedatectl set-ntp true</div></pre></td></tr></table></figure>
<p>  5.chronyc</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">## 查看ntp_servers状态</div><div class="line">chronyc sources -v</div><div class="line">## 查看ntp_sync状态</div><div class="line">chronyc sourcestats -v</div><div class="line">## 查看ntp_servers 是否在线</div><div class="line">chronyc activity -v</div><div class="line">## 查看ntp时间详细信息</div><div class="line">chronyc tracking -v</div><div class="line"></div><div class="line">[root@node3 ~]# chronyc tracking</div><div class="line">Reference ID    : 192.168.138.140 (192.168.138.140)</div><div class="line">Stratum         : 5</div><div class="line">Ref time (UTC)  : Tue Jun 13 05:52:48 2017</div><div class="line">System time     : 0.000494327 seconds fast of NTP time</div><div class="line">Last offset     : +0.000704829 seconds</div><div class="line">RMS offset      : 0.001765276 seconds</div><div class="line">Frequency       : 71.667 ppm slow</div><div class="line">Residual freq   : +0.269 ppm</div><div class="line">Skew            : 6.979 ppm</div><div class="line">Root delay      : 0.324675 seconds</div><div class="line">Root dispersion : 0.039305 seconds</div><div class="line">Update interval : 64.8 seconds</div><div class="line">Leap status     : Normal</div></pre></td></tr></table></figure>
<p>  备注:chrony手动校时</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">chronyc -a makestep</div></pre></td></tr></table></figure>
</li>
<li><p>方法二:调整ceph参数避免<br>1.在admin结点上，修改ceph.conf，添加：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">mon_clock_drift_allowed = 5</div><div class="line">mon_clock_drift_warn_backoff = 30</div></pre></td></tr></table></figure>
<p>  详细参数参考<a href="http://docs.ceph.com/docs/hammer/rados/configuration/mon-config-ref/#clock" target="_blank" rel="external">这里</a></p>
<p>  2.执行下面命令，node1等是monitor结点的名称</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ceph-deploy --overwrite-conf admin node1 node2 node3</div></pre></td></tr></table></figure>
<p>  3.重启monitor</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">systemctl restart ceph-mon@node1</div><div class="line">systemctl restart ceph-mon@node2</div><div class="line">systemctl restart ceph-mon@node3</div></pre></td></tr></table></figure>
<p>  4.验证</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">[root@admin ~]# ceph -s</div><div class="line">    cluster d6d92de4-2a08-4bd6-a749-6c104c88fc40</div><div class="line">     health HEALTH_OK</div><div class="line">     monmap e1: 3 mons at &#123;node1=192.168.138.141:6789/0,node2=192.168.138.142:6789/0,node3=192.168.138.143:6789/0&#125;</div><div class="line">            election epoch 48, quorum 0,1,2 node1,node2,node3</div><div class="line">     osdmap e218: 9 osds: 9 up, 9 in</div><div class="line">            flags sortbitwise,require_jewel_osds</div><div class="line">      pgmap v1007: 256 pgs, 1 pools, 4 bytes data, 1 objects</div><div class="line">            345 MB used, 18377 GB / 18378 GB avail</div><div class="line">                 256 active+clean</div></pre></td></tr></table></figure>
</li>
</ul>
<p>ref<br><a href="https://www.zfl9.com/chrony.html" target="_blank" rel="external">chrony时间同步</a><br><a href="https://dywang.csie.cyut.edu.tw/dywang/rhel7/node70.html" target="_blank" rel="external">chronyd 使用</a><br><a href="https://www.server-world.info/en/note?os=CentOS_6&amp;p=ntp&amp;f=3" target="_blank" rel="external">Chrony : Configure NTP Server</a><br><a href="https://my.oschina.net/u/2475751/blog/704375" target="_blank" rel="external">ceph: HEALTH_WARN: Monitor clock skew detected</a></p>
<hr>
<p>您的鼓励是我写作最大的动力</p>
<p>俗话说，投资效率是最好的投资。 如果您感觉我的文章质量不错，读后收获很大，预计能为您提高 10% 的工作效率，不妨小额捐助我一下，让我有动力继续写出更多好文章。    </p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;ceph异常警告:&lt;br&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div cl
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>ceph-crush algorithm(二)</title>
    <link href="https://t1ger.github.io/2017/06/06/ceph-crush%20algorithm(%E4%BA%8C)/"/>
    <id>https://t1ger.github.io/2017/06/06/ceph-crush algorithm(二)/</id>
    <published>2017-06-06T08:23:40.000Z</published>
    <updated>2017-06-13T09:07:09.977Z</updated>
    
    <content type="html"><![CDATA[<h5 id="源码文件"><a href="#源码文件" class="headerlink" title="源码文件"></a><b>源码文件</b></h5><p>crush目录下的源码文件如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">CrushCompiler.cc	</div><div class="line">CrushCompiler.h	</div><div class="line">CrushLocation.cc	</div><div class="line">CrushLocation.h	</div><div class="line">CrushTester.cc	</div><div class="line">CrushTester.h	</div><div class="line">CrushTreeDumper.h	</div><div class="line">CrushWrapper.cc	</div><div class="line">CrushWrapper.h	</div><div class="line">CrushWrapper.i	</div><div class="line">builder.c	</div><div class="line">builder.h	</div><div class="line">crush.c	</div><div class="line">crush.h	</div><div class="line">crush_compat.h	</div><div class="line">crush_ln_table.h	</div><div class="line">grammar.h	</div><div class="line">hash.c	</div><div class="line">hash.h	</div><div class="line">mapper.c	</div><div class="line">mapper.h	</div><div class="line">old_sample.txt	</div><div class="line">sample.txt	</div><div class="line">types.h</div></pre></td></tr></table></figure></p>
<h5 id="CRUSH-maps"><a href="#CRUSH-maps" class="headerlink" title="CRUSH maps"></a><b>CRUSH maps</b></h5><p>crush maps组成: osd list,Bucket list ,rule list</p>
<p>Device<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">device 0 osd.0</div><div class="line">device 1 osd.1</div><div class="line">device 2 osd.2</div><div class="line">device 3 osd.3</div><div class="line">device 4 osd.4</div><div class="line">device 5 osd.5</div><div class="line">device 6 osd.6</div><div class="line">device 7 osd.7</div></pre></td></tr></table></figure></p>
<p>Bucket<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line">host ceph-osd-ssd-server-1 &#123;</div><div class="line">           id -1</div><div class="line">           alg straw</div><div class="line">           hash 0</div><div class="line">           item osd.0 weight 1.00</div><div class="line">           item osd.1 weight 1.00</div><div class="line">   &#125;</div><div class="line">   host ceph-osd-ssd-server-2 &#123;</div><div class="line">           id -2</div><div class="line">           alg straw</div><div class="line">           hash 0</div><div class="line">           item osd.2 weight 1.00</div><div class="line">           item osd.3 weight 1.00</div><div class="line">   &#125;</div><div class="line">  host ceph-osd-platter-server-1 &#123;</div><div class="line">           id -3</div><div class="line">           alg straw</div><div class="line">           hash 0</div><div class="line">           item osd.4 weight 1.00</div><div class="line">           item osd.5 weight 1.00</div><div class="line">   &#125;</div><div class="line">   host ceph-osd-platter-server-2 &#123;</div><div class="line">           id -4</div><div class="line">           alg straw</div><div class="line">           hash 0</div><div class="line">           item osd.6 weight 1.00</div><div class="line">           item osd.7 weight 1.00</div><div class="line">   &#125;</div><div class="line">   root platter &#123;</div><div class="line">           id -5</div><div class="line">           alg straw</div><div class="line">           hash 0</div><div class="line">           item ceph-osd-platter-server-1 weight 2.00</div><div class="line">           item ceph-osd-platter-server-2 weight 2.00</div><div class="line">   &#125;</div><div class="line">   root ssd &#123;</div><div class="line">           id -6</div><div class="line">           alg straw</div><div class="line">           hash 0</div><div class="line">           item ceph-osd-ssd-server-1 weight 2.00</div><div class="line">           item ceph-osd-ssd-server-2 weight 2.00</div><div class="line">   &#125;</div></pre></td></tr></table></figure></p>
<p>Rule<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div></pre></td><td class="code"><pre><div class="line">rule data &#123;</div><div class="line">              ruleset 0</div><div class="line">              type replicated</div><div class="line">              min_size 2</div><div class="line">              max_size 2</div><div class="line">              step take platter</div><div class="line">              step chooseleaf firstn 0 type host</div><div class="line">              step emit</div><div class="line">      &#125;</div><div class="line">      rule metadata &#123;</div><div class="line">              ruleset 1</div><div class="line">              type replicated</div><div class="line">              min_size 0</div><div class="line">              max_size 10</div><div class="line">              step take platter</div><div class="line">              step chooseleaf firstn 0 type host</div><div class="line">              step emit</div><div class="line">      &#125;</div><div class="line">      rule rbd &#123;</div><div class="line">              ruleset 2</div><div class="line">              type replicated</div><div class="line">              min_size 0</div><div class="line">              max_size 10</div><div class="line">              step take platter</div><div class="line">              step chooseleaf firstn 0 type host</div><div class="line">              step emit</div><div class="line">      &#125;</div><div class="line">      rule platter &#123;</div><div class="line">              ruleset 3</div><div class="line">              type replicated</div><div class="line">              min_size 0</div><div class="line">              max_size 10</div><div class="line">              step take platter</div><div class="line">              step chooseleaf firstn 0 type host</div><div class="line">              step emit</div><div class="line">      &#125;</div><div class="line">      rule ssd &#123;</div><div class="line">              ruleset 4</div><div class="line">              type replicated</div><div class="line">              min_size 0</div><div class="line">              max_size 4</div><div class="line">              step take ssd</div><div class="line">              step chooseleaf firstn 0 type host</div><div class="line">              step emit</div><div class="line">      &#125;</div><div class="line">      rule ssd-primary &#123;</div><div class="line">              ruleset 5</div><div class="line">              type replicated</div><div class="line">              min_size 5</div><div class="line">              max_size 10</div><div class="line">              step take ssd</div><div class="line">              step chooseleaf firstn 1 type host</div><div class="line">              step emit</div><div class="line">              step take platter</div><div class="line">              step chooseleaf firstn -1 type host</div><div class="line">              step emit</div><div class="line">      &#125;</div></pre></td></tr></table></figure></p>
<h5 id="Do-Rule"><a href="#Do-Rule" class="headerlink" title="Do Rule"></a><b>Do Rule</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">void do_rule(int rule, int x, vector&lt;int&gt;&amp; out, int maxout,</div><div class="line">	       const WeightVector&amp; weight,</div><div class="line">	       uint64_t choose_args_index) const &#123;</div><div class="line">    int rawout[maxout];</div><div class="line">    char work[crush_work_size(crush, maxout)];</div><div class="line">    crush_init_workspace(crush, work);</div><div class="line">    crush_choose_arg_map arg_map = choose_args_get(choose_args_index);</div><div class="line">    int numrep = crush_do_rule(crush, rule, x, rawout, maxout, &amp;weight[0],</div><div class="line">			       weight.size(), work, arg_map.args);</div></pre></td></tr></table></figure>
<p>@rule: 使用的crush_rule在crush_map的rules列表中所在index<br>@x: 输入Hash ID，object_id或者pg_id)<br>@out: 输出Device ID列表<br>@maxout: 在输出Device ID的个数，副本的个数<br>@weight: 输出Device列表对应的权重</p>
<p>具体工作调用crush_do_rule完成</p>
<p>Mapper.c<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">/**</div><div class="line"> * crush_do_rule - calculate a mapping with the given input and rule</div><div class="line"> * @map: the crush_map</div><div class="line"> * @ruleno: the rule id</div><div class="line"> * @x: hash input</div><div class="line"> * @result: pointer to result vector</div><div class="line"> * @result_max: maximum result size</div><div class="line"> * @weight: weight vector (for map leaves)</div><div class="line"> * @weight_max: size of weight vector</div><div class="line"> * @cwin: Pointer to at least map-&gt;working_size bytes of memory or NULL.</div><div class="line"> */</div><div class="line">int crush_do_rule(const struct crush_map *map,</div><div class="line">		  int ruleno, int x, int *result, int result_max,</div><div class="line">		  const __u32 *weight, int weight_max,</div><div class="line">		  void *cwin, const struct crush_choose_arg *choose_args)</div></pre></td></tr></table></figure></p>
<p>值得说的变量emit 通常用在规则的结束,同时可以被用在在形相同规则下选择不同的树.更多详细信息看<a href="http://docs.ceph.com/docs/master/rados/operations/crush-map/#crush-map-rules" target="_blank" rel="external">这里</a></p>
<p>ref<br><a href="http://www.shalandis.com/original/2016/05/19/CEPH-CRUSH-algorithm-source-code-analysis/" target="_blank" rel="external">CEPH CRUSH algorithm source code analysis</a><br><a href="http://way4ever.com/?p=123" target="_blank" rel="external">ceph的CRUSH算法的源码分析</a><br><a href="http://www.xuxiaopang.com/2016/11/08/easy-ceph-CRUSH/" target="_blank" rel="external">大话Ceph–CRUSH那点事儿</a><br><a href="http://blog.csdn.net/scaleqiao/article/details/51165575" target="_blank" rel="external">Ceph源码目录架构</a></p>
<hr>
<p>您的鼓励是我写作最大的动力</p>
<p>俗话说，投资效率是最好的投资。 如果您感觉我的文章质量不错，读后收获很大，预计能为您提高 10% 的工作效率，不妨小额捐助我一下，让我有动力继续写出更多好文章。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;源码文件&quot;&gt;&lt;a href=&quot;#源码文件&quot; class=&quot;headerlink&quot; title=&quot;源码文件&quot;&gt;&lt;/a&gt;&lt;b&gt;源码文件&lt;/b&gt;&lt;/h5&gt;&lt;p&gt;crush目录下的源码文件如下:&lt;br&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;ta
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>ceph-crush algorithm(一)</title>
    <link href="https://t1ger.github.io/2017/06/02/ceph-crush%20algorithm(%E4%B8%80)/"/>
    <id>https://t1ger.github.io/2017/06/02/ceph-crush algorithm(一)/</id>
    <published>2017-06-02T10:28:17.000Z</published>
    <updated>2017-06-06T09:37:53.304Z</updated>
    
    <content type="html"><![CDATA[<h5 id="CRUSH简介"><a href="#CRUSH简介" class="headerlink" title="CRUSH简介"></a><b>CRUSH简介</b></h5><p>CRUSH是ceph的一个模块，主要解决可控、可扩展、去中心化的数据副本分布问题.<br>CRUSH全称Controlled Replication Under Scalable Hashing，是一种数据分发算法，类似于哈希和一致性哈希。<br>哈希的问题在于数据增长时不能动态加Bucket，一致性哈希的问题在于加Bucket时数据迁移量比较大，<br>其他数据分发算法依赖中心的Metadata服务器来存储元数据效率较低，CRUSH则是通过计算、接受多维参数的来解决动态数据分发的场景</p>
<p>CRUSH实现了一种伪随机数据分布算法，它能够在层级结构的存储集群中有效的分布对象的副本,它的参数是object id或object group id，并返回一组存储设备(用于保存object副本)<br>CRUSH需要cluster map(描述存储集群的层级结构)、和副本分布策略(rule)</p>
<h5 id="算法基础"><a href="#算法基础" class="headerlink" title="算法基础"></a><b>算法基础</b></h5><p>在学习CRUSH之前，需要了解以下的内容。<br>CRUSH算法通过每个设备的权重来计算数据对象的分布。对象分布是由cluster map和data distribution policy决定的。<br>cluster map描述了可用存储资源和层级结构(比如有多少个机架，每个机架上有多少个服务器，每个服务器上有多少个磁盘)。<br>data distribution policy由placement rules组成。rule决定了每个数据对象有多少个副本，这些副本存储的限制条件(比如3个副本放在不同的机架中)。<br>每个rule就是一系列操作，take操作就是就是选一个bucket，select操作就是选择n个类型是t的项，emit操作就是提交最后的返回结果。<br>select要考虑的东西主要包括是否冲突、是否有失败和负载问题.</p>
<p>CRUSH算法还通过输入一个整数x，输出则是一个包含n个目标的列表R，例如三备份的话输出可能是[1, 3, 5]。<br>(osd0, osd1, osd2 … osdn) = CRUSH(x)<br>CRUSH利用多参数HASH函数，HASH函数中的参数包括x，使得从x到OSD集合是确定性的和独立的。<br>CRUSH只使用了cluster map、placement rules、x。CRUSH是伪随机算法，相似输入的结果之间没有相关性。</p>
<ul>
<li><p>Cluster map<br>Cluster map由device和bucket组成，它们都有id和权重值。Bucket可以包含任意数量item。item可以都是的devices或者都是buckets。<br>管理员控制存储设备的权重。权重和存储设备的容量有关。Bucket的权重被定义为它所包含所有item的权重之和。<br>CRUSH基于4种不同的bucket type，每种有不同的选择算法。</p>
</li>
<li><p>副本分布<br>副本在存储设备上的分布影响数据的安全。cluster map反应了存储系统的物理结构。CRUSH placement policies决定把对象副本分布在不同的区域(某个区域发生故障时并不会影响其他区域)。每个rule包含一系列操作(用在层级结构上)<br>这些操作包括：<br>1.take(a) ：选择一个item，一般是bucket，并返回bucket所包含的所有item。这些item是后续操作的参数，这些item组成向量i。<br>2.select(n, t)：迭代操作每个item(向量i中的item)，对于每个item(向量i中的item)向下遍历(遍历这个item所包含的item)，都返回n个不同的item(type为t的item)，并把这些item都放到向量i中。select函数会调用c(r, x)函数，这个函数会在每个bucket中伪随机选择一个item。<br>3.emit：把向量i放到result中。</p>
<p>存储设备有一个确定的类型。每个bucket都有type属性值，用于区分不同的bucket类型(比如”row”、”rack”、”host”等，type可以自定义)。rules可以包含多个take和emit语句块，这样就允许从不同的存储池中选择副本的storage target</p>
</li>
<li><p>冲突、故障、超载<br>select(n, t)操作会循环选择第 r=1,…,n 个副本，r作为选择参数。在这个过程中，假如选择到的item遇到三种情况(冲突，故障，超载)时，CRUSH会拒绝选择这个item，并使用r’(r’和r、出错次数、firstn参数有关)作为选择参数重新选择item。<br>1.冲突：这个item已经在向量i中，已被选择。<br>2.故障：设备发生故障，不能被选择。<br>3.超载：设备使用容量超过警戒线，没有剩余空间保存数据对象。<br>故障设备和超载设备会在cluster map上标记(还留在系统中)，这样避免了不必要的数据迁移。</p>
</li>
<li><p>MAP改变和数据迁移<br>当添加移除存储设备，或有存储设备发生故障时(cluster map发生改变时)，存储系统中的数据会发生迁移。好的数据分布算法可以最小化数据迁移大小。</p>
</li>
</ul>
<h5 id="CRUSH总结"><a href="#CRUSH总结" class="headerlink" title="CRUSH总结"></a><b>CRUSH总结</b></h5><ul>
<li><p>算法总结<br>CRUSH与一致性哈希最大的区别在于接受的参数多了cluster map和placement rules，这样就可以根据目前cluster的状态动态调整数据位置，同时通过算法得到一致的结果</p>
</li>
<li><p>算法补充<br>前面介绍了bucket根据不同场景有四种类型，分别是Uniform、List、Tree和Straw，他们对应运行数据和数据迁移量有不同的tradeoff，目前大家都在用Straw因此不太需要关注其他。<br>目前erasing code可以大大减小三备份的数据量，但除了会导致数据恢复慢，部分ceph支持的功能也是不能直接用的，而且功能仍在开发中不建议使用。</p>
<p>  有兴趣的读者可以拜读下Sega本人的博士论文作品:<br>长论文包含了RADOS、CRUSH等所有内容的介绍，但篇幅相当长，如果感兴趣可以阅读，标题为《CEPH: RELIABLE, SCALABLE, AND HIGH-PERFORMANCE DISTRIBUTED STORAGE》，地址 <a href="http://ceph.com/papers/weil-thesis.pdf" target="_blank" rel="external">http://ceph.com/papers/weil-thesis.pdf</a> </p>
<p>  CRUSH论文标题为《CRUSH: Controlled, Scalable, Decentralized Placement of Replicated Data》，地址 <a href="http://ceph.com/papers/weil-crush-sc06.pdf" target="_blank" rel="external">http://ceph.com/papers/weil-crush-sc06.pdf</a> ，介绍了CRUSH的设计与实现细节</p>
<p>  RADOS沦为标题为《RADOS: A Scalable, Reliable Storage Service for Petabyte-scale Storage Clusters》，地址为 <a href="http://ceph.com/papers/weil-rados-pdsw07.pdf" target="_blank" rel="external">http://ceph.com/papers/weil-rados-pdsw07.pdf</a> ，介绍了RADOS的设计与实现细节</p>
<p>  CephFS论文标题为《Ceph: A Scalable, High-Performance Distributed File System》，地址为 <a href="http://ceph.com/papers/weil-ceph-osdi06.pdf" target="_blank" rel="external">http://ceph.com/papers/weil-ceph-osdi06.pdf</a> ，介绍了Ceph的基本架构和Ceph的设计与实现细节</p>
</li>
</ul>
<p>ref<br><a href="http://way4ever.com/?p=122" target="_blank" rel="external">ceph的CRUSH数据分布算法介绍</a><br><a href="https://tobegit3hub1.gitbooks.io/ceph_from_scratch/content/architecture/crush.html" target="_blank" rel="external">CRUSH详解</a><br><a href="http://www.xuxiaopang.com/2016/11/08/easy-ceph-CRUSH/" target="_blank" rel="external">大话Ceph–CRUSH那点事儿</a></p>
<hr>
<p>您的鼓励是我写作最大的动力</p>
<p>俗话说，投资效率是最好的投资。 如果您感觉我的文章质量不错，读后收获很大，预计能为您提高 10% 的工作效率，不妨小额捐助我一下，让我有动力继续写出更多好文章。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;CRUSH简介&quot;&gt;&lt;a href=&quot;#CRUSH简介&quot; class=&quot;headerlink&quot; title=&quot;CRUSH简介&quot;&gt;&lt;/a&gt;&lt;b&gt;CRUSH简介&lt;/b&gt;&lt;/h5&gt;&lt;p&gt;CRUSH是ceph的一个模块，主要解决可控、可扩展、去中心化的数据副本分布问题.&lt;b
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>ceph硬件推荐</title>
    <link href="https://t1ger.github.io/2017/06/01/ceph%E7%A1%AC%E4%BB%B6%E6%8E%A8%E8%8D%90/"/>
    <id>https://t1ger.github.io/2017/06/01/ceph硬件推荐/</id>
    <published>2017-06-01T11:41:45.000Z</published>
    <updated>2017-06-02T05:42:40.066Z</updated>
    
    <content type="html"><![CDATA[<h4 id="cpu和内存"><a href="#cpu和内存" class="headerlink" title="cpu和内存"></a><b>cpu和内存</b></h4><ul>
<li>CPU<br>metadata servers：属于CPU敏感型的角色服务器<br>OSDs：负责运行RADOS service，使用CRUSH算法计算数据分布，复制数据，并维护它们自己的拷贝，所以CPU也需要较好的<br>Monitors：监控服务器芝士简单的维护一份集群mapping的主拷贝，所以并非是CPU敏感的</li>
<li>内存<br>metadata servers和monitors需要快速响应它们存储的数据，所以它们需要不少的内存<br>每个daemon实例最少1GB<br>OSDs不需要太多的内存做日常的操作，500MB即可，但是在recovery阶段<br>每1TB的存储，需要消耗约1GB的内存，所以说，内存是越多越好</li>
</ul>
<h4 id="硬盘"><a href="#硬盘" class="headerlink" title="硬盘"></a><b>硬盘</b></h4><p>这里说下固态盘</p>
<ul>
<li>固态硬盘: SSD 和硬盘相比每 GB 成本通常要高 10 倍以上，但访问时间至少比硬盘快 100 倍.SSD 没有可移动机械部件，所以不存在和硬盘一样的局限性。但 SSD 也有局限性，评估SSD 时，顺序读写性能很重要，在为多个 OSD 存储日志时，有着 400MB/s 顺序读写吞吐量的 SSD 其性能远高于 120MB/s 的.<br>备注:我们建议发掘 SSD 的用法来提升性能。然而在大量投入 SSD 前，我们强烈建议核实 SSD 的性能指标，并在测试环境下衡量性能<br>可接受的 IOPS 指标对选择用于 Ceph 的 SSD 还不够，用于日志和 SSD 时还有几个重要考量：<br>写密集语义： 记日志涉及写密集语义，所以你要确保选用的 SSD 写入性能和硬盘相当或好于硬盘。廉价 SSD 可能在加速访问的同时引入写延时，有时候高性能硬盘的写入速度可以和便宜 SSD 相媲美<br>顺序写入： 在一个 SSD 上为多个 OSD 存储多个日志时也必须考虑 SSD 的顺序写入极限，因为它们要同时处理多个 OSD 日志的写入请求。<br>分区对齐： 采用了 SSD 的一个常见问题是人们喜欢分区，却常常忽略了分区对齐，这会导致 SSD 的数据传输速率慢很多，所以请确保分区对齐了</li>
<li>其他注意事项<br>你可以在同一主机上运行多个 OSD ，但要确保 OSD 硬盘总吞吐量不超过为客户端提供读写服务所需的网络带宽；还要考虑集群在每台主机上所存储的数据占总体的百分比，如果一台主机所占百分比太大而它挂了，就可能导致诸如超过 full ratio 的问题，此问题会使 Ceph 中止运作以防数据丢失。<br>如果每台主机运行多个 OSD ，也得保证内核是最新的。参阅<a href="http://docs.ceph.org.cn/start/os-recommendations/" target="_blank" rel="external">操作系统推荐</a>里关于 glibc 和 syncfs(2) 的部分，确保硬件性能可达期望值。<br>OSD 数量较多（如 20 个以上）的主机会派生出大量线程，尤其是在恢复和重均衡期间。很多 Linux 内核默认的最大线程数较小（如 32k 个），如果您遇到了这类问题，可以把 kernel.pid_max 值调高些。理论最大值是 4194303 。例如把下列这行加入 /etc/sysctl.conf 文件：<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">kernel.pid_max = 4194303</div></pre></td></tr></table></figure>
</li>
</ul>
<p><b>结论: OSDs建议最小1TB,Ceph 最佳实践指示，应该分别在单独的硬盘运行操作系统、 OSD 数据和 OSD 日志</b></p>
<h4 id="网络"><a href="#网络" class="headerlink" title="网络"></a><b>网络</b></h4><p>建议每台机器最少两个千兆网卡，现在大多数机械硬盘都能达到大概 100MB/s 的吞吐量，网卡应该能处理所有 OSD 硬盘总吞吐量，所以推荐最少两个千兆网卡，分别用于公网（前端）和集群网络（后端）。集群网络（最好别连接到国际互联网）用于处理由数据复制产生的额外负载，而且可防止拒绝服务攻击，拒绝服务攻击会干扰数据归置组，使之在 OSD 数据复制时不能回到 active + clean 状态。请考虑部署万兆网卡。通过 1Gbps 网络复制 1TB 数据耗时 3 小时，而 3TB （典型配置）需要 9 小时，相比之下，如果使用 10Gbps 复制时间可分别缩减到 20 分钟和 1 小时。在一个 PB 级集群中， OSD 磁盘失败是常态，而非异常；在性价比合理的的前提下，系统管理员想让 PG 尽快从 degraded （降级）状态恢复到 active + clean 状态。另外，一些部署工具（如 Dell 的 Crowbar ）部署了 5 个不同的网络，但使用了 VLAN 以提高网络和硬件可管理性。 VLAN 使用 802.1q 协议，还需要采用支持 VLAN 功能的网卡和交换机，增加的硬件成本可用节省的运营（网络安装、维护）成本抵消。使用 VLAN 来处理集群和计算栈（如 OpenStack 、 CloudStack 等等）之间的 VM 流量时，采用 10G 网卡仍然值得。每个网络的机架路由器到核心路由器应该有更大的带宽，如 40Gbps 到 100Gbps 。</p>
<p>服务器应配置底板管理控制器（ Baseboard Management Controller, BMC ），管理和部署工具也应该大规模使用 BMC ，所以请考虑带外网络管理的成本/效益平衡，此程序管理着 SSH 访问、 VM 映像上传、操作系统安装、端口管理、等等，会徒增网络负载。运营 3 个网络有点过分，但是每条流量路径都指示了部署一个大型数据集群前要仔细考虑的潜能力、吞吐量、性能瓶颈</p>
<p><b>结论:每台机器最少是两块前兆网卡（物理交换机也需要隔离）,最少是10Gbps在机架</b></p>
<h4 id="故障域"><a href="#故障域" class="headerlink" title="故障域"></a><b>故障域</b></h4><p>故障域指任何导致不能访问一个或多个 OSD 的故障，可以是主机上停止的进程、硬盘故障、操作系统崩溃、有问题的网卡、损坏的电源、断网、断电等等。规划硬件需求时，要在多个需求间寻求平衡点，像付出很多努力减少故障域带来的成本削减、隔离每个潜在故障域增加的成本。</p>
<h4 id="最低硬件推荐"><a href="#最低硬件推荐" class="headerlink" title="最低硬件推荐"></a><b>最低硬件推荐</b></h4><ul>
<li><p>Dell 实例: Ceph 集群项目(2012年)使用了 2 个相当强悍的 OSD 硬件配置，和稍逊的监视器配置</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">|Configuration	|Criteria      	|Minimum Recommended</div><div class="line">|---		|---		|---</div><div class="line">|Dell PE R510  	|Processor	|2x 64-bit quad-core Xeon CPUs</div><div class="line">|		|RAM		|16 GB</div><div class="line">|		|Volume Storage	|8x 2TB drives. 1 OS, 7 Storage</div><div class="line">|		|Client Network	|2x 1GB Ethernet NICs</div><div class="line">|		|OSD Network	|2x 1GB Ethernet NICs</div><div class="line">|		|Mgmt. Network	|2x 1GB Ethernet NICs</div><div class="line">|Dell PE R515   |Processor	|1x hex-core Opteron CPU</div><div class="line">|		|RAM		|16 GB</div><div class="line">|		|Volume Storage	|12x 3TB drives. Storage</div><div class="line">|		|OS Storage	|1x 500GB drive. Operating System.</div><div class="line">|		|Client Network	|2x 1GB Ethernet NICs</div><div class="line">|		|OSD Network	|2x 1GB Ethernet NICs</div><div class="line">|		|Mgmt. Network	|2x 1GB Ethernet NICs</div></pre></td></tr></table></figure>
</li>
<li><p>低配(RedHat提供)<br>1.osd</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">|Criteria	|Minimum Recommended|</div><div class="line">|: --:		|:–		|</div><div class="line">|Processor	|1x AMD64 and Intel 64|</div><div class="line">|RAM 		|2 GB of RAM per deamon|</div><div class="line">|Volume Storage	|1x storage drive per daemon|</div><div class="line">|Journal	|1x SSD partition per daemon (optional)|</div><div class="line">|Network	|2x 1GB Ethernet NICs|</div></pre></td></tr></table></figure>
<p>  2.mon</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">| Criteria	| Minimum Recommended|</div><div class="line">| : --:		| :–		|</div><div class="line">|Processor	| 1x AMD64 and Intel 64|</div><div class="line">|RAM 		| 1 GB of RAM per deamon|</div><div class="line">|Disk Space	| 10 GB per daemon|</div><div class="line">| Network	| 2x 1GB Ethernet NICs|</div></pre></td></tr></table></figure>
</li>
<li><p>高配(Intel提供)<br>1.osd –Good</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">|Criteria	|Minimum Recommended|</div><div class="line">|: --:		|:–		|</div><div class="line">|Processor	|Inter(R) Xeon(R)CPU E5-2650v3|</div><div class="line">|RAM 		|64GB |</div><div class="line">|Volume Storage	|1x1.6TB P3700+12x4TB HDDs(1:12 ratio) P3700 as Journal and caching|</div><div class="line">|Caching software	|Intel(R) CAS3.0,option: Intel(R) RSTe/MD4.3|</div><div class="line">|Network	|10Gbe|</div></pre></td></tr></table></figure>
<p>  2.osd –Better</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">|Criteria	|Minimum Recommended|</div><div class="line">|: --:		|:–		|</div><div class="line">|Processor	|Inter(R) Xeon(R)CPU E5-2690|</div><div class="line">|RAM 		|128GB |</div><div class="line">|Volume Storage	|1xIntel(R) DC P3700(800G)+4xIntel(R) DC S3510 1.6TB|</div><div class="line">|Network	|Duel 10Gbe|</div></pre></td></tr></table></figure>
<p>  3.osd –Best</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">|Criteria	|Minimum Recommended|</div><div class="line">|: --:		|:–		|</div><div class="line">|Processor	|Inter(R) Xeon(R)CPU E5-2699v3|</div><div class="line">|RAM 		|&gt;=128GB |</div><div class="line">|Volume Storage	|4 to 6xIntel(R)DC P3700 2TB|</div><div class="line">|Network	|2x 40GbE,4xdual 10GbE|</div></pre></td></tr></table></figure>
</li>
<li><p>vpsee生产实例</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">| Hostname  | IP Address    | Role  |                                           Hardware Info |</div><div class="line">|-----------+---------------+-------|---------------------------------------------------------|</div><div class="line">| ceph-adm  | 192.168.2.100 | adm   |                             2 Cores, 4GB RAM, 20GB DISK |</div><div class="line">| ceph-mon1 | 192.168.2.101 | mon   |                         24 Cores，64GB RAM, 2x750GB SAS |</div><div class="line">| ceph-mon2 | 192.168.2.102 | mon   |                         24 Cores，64GB RAM, 2x750GB SAS |</div><div class="line">| ceph-mon3 | 192.168.2.103 | mon   |                         24 Cores，64GB RAM, 2x750GB SAS |</div><div class="line">| ceph-osd1 | 192.168.2.121 | osd   | 12 Cores，64GB RAM, 10x4TB SAS，2x400GB SSD，2x80GB SSD |</div><div class="line">| ceph-osd2 | 192.168.2.122 | osd   | 12 Cores，64GB RAM, 10x4TB SAS，2x400GB SSD，2x80GB SSD |</div></pre></td></tr></table></figure>
</li>
<li><p>ADM 服务器硬件配置比较随意，用1台低配置的虚拟机就可以了，只是用来操作和管理 Ceph；</p>
</li>
<li>MON 服务器2块硬盘做成 RAID1，用来安装操作系统；</li>
<li>OSD 服务器上用10块 4TB 硬盘做 Ceph 存储，每个 osd 对应1块硬盘，每个 osd 需要1个 Journal，所以10块硬盘需要10个 Journal，我们用2块大容量 SSD 硬盘做 journal，每个 SSD 等分成5个区，这样每个区分别对应一个 osd 硬盘的 journal，剩下的2块小容量 SSD 装操作系统，采用 RAID1.</li>
</ul>
<p>ref<br><a href="http://docs.ceph.org.cn/start/hardware-recommendations/#id9" target="_blank" rel="external">硬件推荐</a><br><a href="https://github.com/lemonhall/node_note/blob/master/ceph%E7%9A%84%E7%A1%AC%E4%BB%B6%E9%85%8D%E7%BD%AE.txt" target="_blank" rel="external">ceph的硬件配置.txt</a><br><a href="http://www.vpsee.com/2015/07/install-ceph-on-centos-7/" target="_blank" rel="external">在 CentOS 7.1 上安装分布式存储系统 Ceph</a><br><a href="http://www.xuxiaopang.com/2016/10/10/ceph-full-install-el7-jewel/" target="_blank" rel="external">CEPH部署完整版(el7+jewel)</a><br><a href="http://www.xuxiaopang.com/2016/11/11/doc-ceph-table/" target="_blank" rel="external">Ceph常用表格汇总</a></p>
<hr>
<p>您的鼓励是我写作最大的动力</p>
<p>俗话说，投资效率是最好的投资。 如果您感觉我的文章质量不错，读后收获很大，预计能为您提高 10% 的工作效率，不妨小额捐助我一下，让我有动力继续写出更多好文章。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;cpu和内存&quot;&gt;&lt;a href=&quot;#cpu和内存&quot; class=&quot;headerlink&quot; title=&quot;cpu和内存&quot;&gt;&lt;/a&gt;&lt;b&gt;cpu和内存&lt;/b&gt;&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;CPU&lt;br&gt;metadata servers：属于CPU敏感型的角色服务器&lt;br
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>ceph维护命令手册</title>
    <link href="https://t1ger.github.io/2017/06/01/ceph%E7%BB%B4%E6%8A%A4%E5%91%BD%E4%BB%A4%E6%89%8B%E5%86%8C/"/>
    <id>https://t1ger.github.io/2017/06/01/ceph维护命令手册/</id>
    <published>2017-06-01T09:09:48.000Z</published>
    <updated>2017-06-06T09:37:46.837Z</updated>
    
    <content type="html"><![CDATA[<p>本文操作都是centos7环境</p>
<h5 id="crush-map-管理方法"><a href="#crush-map-管理方法" class="headerlink" title="crush map 管理方法"></a><b>crush map 管理方法</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">#获得默认 crushmap (加密)</div><div class="line">ceph osd getcrushmap -o crushmap.dump</div><div class="line"></div><div class="line">#转换 crushmap 格式 (加密 -&gt; 明文格式)</div><div class="line">crushtool -d crushmap.dump -o crushmap.txt</div><div class="line"></div><div class="line">#转换 crushmap 格式(明文 -&gt; 加密格式)</div><div class="line">crushtool -c crushmap.txt -o crushmap.done</div><div class="line"></div><div class="line">#重新使用新 crushmap</div><div class="line">ceph osd setcrushmap -i crushmap.done</div></pre></td></tr></table></figure>
<p>这里要说下,对crush map进行定义:<br>1 物理主机划分<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line">root default &#123;</div><div class="line">    id -1           # do not change unnecessarily</div><div class="line">    # weight 264.000</div><div class="line">    alg straw</div><div class="line">    hash 0  # rjenkins1</div><div class="line">    item 240.30.128.33 weight 12.000</div><div class="line">    item 240.30.128.32 weight 12.000</div><div class="line">    item 240.30.128.215 weight 12.000</div><div class="line">    item 240.30.128.209 weight 12.000</div><div class="line">    item 240.30.128.213 weight 12.000</div><div class="line">    item 240.30.128.214 weight 12.000</div><div class="line">    item 240.30.128.212 weight 12.000</div><div class="line">    item 240.30.128.211 weight 12.000</div><div class="line">    item 240.30.128.210 weight 12.000</div><div class="line">    item 240.30.128.208 weight 12.000</div><div class="line">    item 240.30.128.207 weight 12.000</div><div class="line">    item 240.30.128.63 weight 12.000</div><div class="line">    item 240.30.128.34 weight 12.000</div><div class="line">    item 240.30.128.35 weight 12.000</div><div class="line">    item 240.30.128.36 weight 12.000</div><div class="line">    item 240.30.128.37 weight 12.000</div><div class="line">    item 240.30.128.39 weight 12.000</div><div class="line">    item 240.30.128.38 weight 12.000</div><div class="line">    item 240.30.128.58 weight 12.000</div><div class="line">    item 240.30.128.59 weight 12.000</div><div class="line">    item 240.30.128.60 weight 12.000</div><div class="line">    item 240.30.128.29 weight 12.000</div><div class="line">&#125;</div><div class="line"></div><div class="line">root registry &#123;</div><div class="line">    id -26</div><div class="line">    # weight 36.000</div><div class="line">    alg straw</div><div class="line">    item 240.30.128.206 weight 12.000</div><div class="line">    item 240.30.128.40 weight 12.000</div><div class="line">    item 240.30.128.30 weight 12.000</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>说明:上面划分了两个物理区域</p>
<ul>
<li>root 区域, 包含了 264TB 空间</li>
<li>registry 区域,  包含了 36TB 空间<br>需要注意的问题:<br>建议在存放数据前就对物理池进行规划, 否则会出现大量数据迁移现象, 或者会出现 osd full 现象</li>
</ul>
<p>2.规则划分<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">rule replicated_ruleset &#123;</div><div class="line">    ruleset 0</div><div class="line">    type replicated</div><div class="line">    min_size 1</div><div class="line">    max_size 10</div><div class="line">    step take default</div><div class="line">    step chooseleaf firstn 0 type host</div><div class="line">    step emit</div><div class="line">&#125;</div><div class="line"></div><div class="line">rule registry_ruleset &#123;</div><div class="line">    ruleset 1</div><div class="line">    type replicated</div><div class="line">    min_size 2</div><div class="line">    max_size 3</div><div class="line">    step take registry</div><div class="line">    step chooseleaf firstn 0 type host</div><div class="line">    step emit</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h5 id="pool-创建-删除方法"><a href="#pool-创建-删除方法" class="headerlink" title="pool 创建, 删除方法"></a><b>pool 创建, 删除方法</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">#创建</div><div class="line"> ceph osd  pool  create  volumes 10240 10240</div><div class="line"> ceph osd  pool  create  paas 2048 2048</div><div class="line">#删除</div><div class="line"> ceph osd pool delete paas paas --yes-i-really-really-mean-it</div><div class="line">#查询</div><div class="line">[root@node1 ~]# ceph osd dump | grep replica</div><div class="line">#指定</div><div class="line"> ceph osd pool set paas crush_ruleset 1</div></pre></td></tr></table></figure>
<h5 id="MON操作"><a href="#MON操作" class="headerlink" title="MON操作"></a><b>MON操作</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">systemctl start ceph-mon@mon-host</div><div class="line">systemctl stop ceph-mon@mon-host</div><div class="line">systemctl restart ceph-mon@mon-host</div><div class="line">systemctl status ceph-mon@mon-host</div></pre></td></tr></table></figure>
<h5 id="OSD操作"><a href="#OSD操作" class="headerlink" title="OSD操作"></a><b>OSD操作</b></h5><p>把@*替换为OSD的ID 如@1，即可执行对应ID OSD的操作<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">systemctl start ceph-osd@*</div><div class="line">systemctl stop  ceph-osd@*</div><div class="line">systemctl restart ceph-osd@*</div><div class="line">systemctl status  ceph-osd@*</div></pre></td></tr></table></figure></p>
<h5 id="OSD机器重启"><a href="#OSD机器重启" class="headerlink" title="OSD机器重启"></a><b>OSD机器重启</b></h5><p>1.设置noout 避免在异常情况下触发集群数据重新平衡<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ceph osd set noout</div></pre></td></tr></table></figure></p>
<p>2.关闭OSD机器上所有OSD进程<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">ceph osd down x    #把这个机器上的OSD都设置为down状态</div><div class="line">systemctl stop ceph-osd@* #用systemctl重启OSD进程</div></pre></td></tr></table></figure></p>
<p>3.重启OSD机器<br>4.恢复noout 设置<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ceph osd unset noout</div></pre></td></tr></table></figure></p>
<h5 id="扩容"><a href="#扩容" class="headerlink" title="扩容"></a><b>扩容</b></h5><ul>
<li>PB级的集群的容量超过50%，就要考虑扩容了。 假如OSD主机的磁盘容量为48TB（12 4TB），则需要backfill的数据为24TB（48TB 50%） ，假设网卡为10Gb，则新加一个OSD时，集群大约需要19200s（24TB/(10Gb/8)） 约3小时完成backfill，而backfill后台数据填充将会涉及大量的IO读和网络传输，必将影响生产业务运行。 如果集群容量到80%再扩容会导致更长的backfill时间，近8个小时。</li>
<li>OSD对应的磁盘利用率如果超过50%，也需要尽快扩容。<br>在业务闲时,向集群中增加OSD主机。</li>
</ul>
<h5 id="升级Ceph软件版本"><a href="#升级Ceph软件版本" class="headerlink" title="升级Ceph软件版本"></a><b>升级Ceph软件版本</b></h5><p>1.在MON和OSD机器上升级安装指定的ceph版本的软件包<br>2.逐个重启MON进程<br>3.设置noout 避免在异常情况下触发集群数据重新平衡<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ceph osd set noout</div></pre></td></tr></table></figure></p>
<p>4.逐个重启OSD进程<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">ceph osd down x        #提前mark down， 减少slow request</div><div class="line">systemctl restart ceph-osd@x #用systemctl重启OSD进程</div></pre></td></tr></table></figure></p>
<p>5.恢复noout 设置<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ceph osd unset noout</div></pre></td></tr></table></figure></p>
<p>ref<br><a href="https://forest.gitbooks.io/ceph-practice/content/operation.html" target="_blank" rel="external">维护操作</a><br><a href="https://forest.gitbooks.io/ceph-practice/content/troubleshoot.html" target="_blank" rel="external">故障定位和处理</a><br><a href="https://yq.aliyun.com/articles/70814" target="_blank" rel="external">ceph - crush map 与 pool</a></p>
<hr>
<p>您的鼓励是我写作最大的动力</p>
<p>俗话说，投资效率是最好的投资。 如果您感觉我的文章质量不错，读后收获很大，预计能为您提高 10% 的工作效率，不妨小额捐助我一下，让我有动力继续写出更多好文章。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文操作都是centos7环境&lt;/p&gt;
&lt;h5 id=&quot;crush-map-管理方法&quot;&gt;&lt;a href=&quot;#crush-map-管理方法&quot; class=&quot;headerlink&quot; title=&quot;crush map 管理方法&quot;&gt;&lt;/a&gt;&lt;b&gt;crush map 管理方法&lt;/b&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Ceph-Placement Group States</title>
    <link href="https://t1ger.github.io/2017/05/31/Ceph-Placement-Group-States/"/>
    <id>https://t1ger.github.io/2017/05/31/Ceph-Placement-Group-States/</id>
    <published>2017-05-31T11:02:33.000Z</published>
    <updated>2017-06-01T08:08:23.953Z</updated>
    
    <content type="html"><![CDATA[<h5 id="PG状态简介"><a href="#PG状态简介" class="headerlink" title="PG状态简介"></a><b>PG状态简介</b></h5><p>PG，Placement Group,让我们简单了解几个PG的状态,可以分为:</p>
<ul>
<li>Creating: Ceph is still creating the placement group.</li>
<li>Scrubbing: Ceph is checking the placement group for inconsistencies.</li>
<li>Degraded: Ceph has not replicated some objects in the placement group the correct number of times yet.</li>
<li>Inconsistent: Ceph detects inconsistencies in the one or more replicas of an object in the placement group (e.g. objects are the wrong size, objects are missing from one replica after recovery finished, etc.).</li>
<li>Peering: The placement group is undergoing the peering process</li>
<li>Repair: Ceph is checking the placement group and repairing any inconsistencies it finds (if possible).</li>
<li>Recovering: Ceph is migrating/synchronizing objects and their replicas.</li>
<li>Backfill: Ceph is scanning and synchronizing the entire contents of a placement group instead of inferring what contents need to be synchronized from the logs of recent operations. Backfill is a special case of recovery.</li>
<li>Wait-backfill: The placement group is waiting in line to start backfill</li>
<li>Backfill-toofull: A backfill operation is waiting because the destination OSD is over its full ratio.</li>
<li>Incomplete: Ceph detects that a placement group is missing information about writes that may have occurred, or does not have any healthy copies. If you see this state, try to start any failed OSDs that may contain the needed information. In the case of an erasure coded pool temporarily reducing min_size may allow recovery</li>
<li>Remapped: The placement group is temporarily mapped to a different set of OSDs from what CRUSH specified.</li>
<li>Undersized: The placement group fewer copies than the configured pool replication level.</li>
<li>Peered: The placement group has peered, but cannot serve client IO due to not having enough copies to reach the pool’s configured min_size parameter. Recovery may occur in this state, so the pg may heal up to min_size eventually.<br>具体参考<a href="http://docs.ceph.com/docs/master/rados/operations/pg-states/" target="_blank" rel="external">这里</a></li>
</ul>
<h5 id="如何理解-PG"><a href="#如何理解-PG" class="headerlink" title="如何理解 PG"></a><b>如何理解 PG</b></h5><p>让我们来看下先前搭建的集群状态<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">[root@admin ~]# ceph osd tree</div><div class="line">ID WEIGHT   TYPE NAME      UP/DOWN REWEIGHT PRIMARY-AFFINITY </div><div class="line">-1 17.94685 root default                                     </div><div class="line">-2  5.98228     host node1                                   </div><div class="line"> 0  1.99409         osd.0       up  1.00000          1.00000 </div><div class="line"> 1  1.99409         osd.1       up  1.00000          1.00000 </div><div class="line"> 2  1.99409         osd.2       up  1.00000          1.00000 </div><div class="line">-3  5.98228     host node2                                   </div><div class="line"> 3  1.99409         osd.3       up  1.00000          1.00000 </div><div class="line"> 4  1.99409         osd.4       up  1.00000          1.00000 </div><div class="line"> 5  1.99409         osd.5       up  1.00000          1.00000 </div><div class="line">-4  5.98228     host node3                                   </div><div class="line"> 6  1.99409         osd.6       up  1.00000          1.00000 </div><div class="line"> 7  1.99409         osd.7       up  1.00000          1.00000 </div><div class="line"> 8  1.99409         osd.8       up  1.00000          1.00000 </div><div class="line">[root@admin ~]#</div></pre></td></tr></table></figure></p>
<p>每一个pool都有一个id，系统默认生成的rbd池的id号为0，那么rbd池内的所有PG都会以0.开头，让我们看一下osd.0下面的current目录的内容：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">[root@node1 ~]# ls /var/lib/ceph/osd/ceph-0/current/</div><div class="line">0.0_head   0.16_TEMP  0.1c_head  0.23_TEMP  0.2b_head  0.38_TEMP  0.46_head  0.4d_TEMP  0.50_head  0.55_TEMP  0.64_head  0.6e_TEMP  0.72_head  0.79_TEMP  0.8_head</div><div class="line">0.0_TEMP   0.17_head  0.1c_TEMP  0.24_head  0.2b_TEMP  0.42_head  0.46_TEMP  0.4e_head  0.50_TEMP  0.59_head  0.64_TEMP  0.6_head   0.72_TEMP  0.7b_head  0.8_TEMP</div><div class="line">0.10_head  0.17_TEMP  0.1d_head  0.24_TEMP  0.2c_head  0.42_TEMP  0.48_head  0.4e_TEMP  0.53_head  0.59_TEMP  0.65_head  0.6_TEMP   0.75_head  0.7b_TEMP  commit_op_seq</div><div class="line">0.10_TEMP  0.18_head  0.1d_TEMP  0.25_head  0.2c_TEMP  0.43_head  0.48_TEMP  0.4f_head  0.53_TEMP  0.5c_head  0.65_TEMP  0.70_head  0.75_TEMP  0.7d_head  meta</div><div class="line">0.13_head  0.18_TEMP  0.20_head  0.25_TEMP  0.2_head   0.43_TEMP  0.49_head  0.4f_TEMP  0.54_head  0.5c_TEMP  0.6d_head  0.70_TEMP  0.77_head  0.7d_TEMP  nosnap</div><div class="line">0.13_TEMP  0.19_head  0.20_TEMP  0.2a_head  0.2_TEMP   0.44_head  0.49_TEMP  0.4_head   0.54_TEMP  0.63_head  0.6d_TEMP  0.71_head  0.77_TEMP  0.7e_head  omap</div><div class="line">0.16_head  0.19_TEMP  0.23_head  0.2a_TEMP  0.38_head  0.44_TEMP  0.4d_head  0.4_TEMP   0.55_head  0.63_TEMP  0.6e_head  0.71_TEMP  0.79_head  0.7e_TEMP</div></pre></td></tr></table></figure></p>
<p>每个OSD的current目录下都保存了部分的PG，而rbd池的PG以0.xxx_head的目录形式存在！</p>
<p>现在我们通过rados向集群写入一个文件(char.txt)，在集群内保存名为char,通过下面的指令查看该文件实际保存的位置<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">[root@admin ~]# ceph osd map rbd char</div><div class="line">osdmap e89 pool &apos;rbd&apos; (0) object &apos;char&apos; -&gt; pg 0.98165844 (0.44) -&gt; up ([0,3,8], p0) acting ([0,3,8], p0)</div></pre></td></tr></table></figure></p>
<p>可见，文件会保存在PG 0.44中，而这个PG位于osd.0,osd.3,osd.8中，之所以有这里有三个OSD，是因为集群副本数为3，可以在0/3/8这三个OSD的current下找到0.44_head目录。而同一份文件(比如这里的char.txt)的三个副本会分别保存在这三个同名的PG中</p>
<p>执行指令，将char.txt文件存入集群，并查看各个OSD的PG目录内容：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">[root@admin ~]# rados -p rbd put char char.txt </div><div class="line"></div><div class="line">[root@node1 ~]# ll /var/lib/ceph/osd/ceph-0/current/0.44_head/</div><div class="line">total 4</div><div class="line">-rw-r--r-- 1 ceph ceph 4 May 31 18:20 char__head_98165844__0</div><div class="line">-rw-r--r-- 1 ceph ceph 0 May 31 10:41 __head_00000044__0</div><div class="line"></div><div class="line">[root@node2 ~]# ll /var/lib/ceph/osd/ceph-3/current/0.44_head/</div><div class="line">total 4</div><div class="line">-rw-r--r-- 1 ceph ceph 4 May 31 18:20 char__head_98165844__0</div><div class="line">-rw-r--r-- 1 ceph ceph 0 May 31 10:41 __head_00000044__0</div><div class="line"></div><div class="line">[root@node3 ~]# ll /var/lib/ceph/osd/ceph-8/current/0.44_head/</div><div class="line">total 4</div><div class="line">-rw-r--r-- 1 ceph ceph 4 May 31 18:20 char__head_98165844__0</div><div class="line">-rw-r--r-- 1 ceph ceph 0 May 31 10:41 __head_00000044__0</div></pre></td></tr></table></figure></p>
<p>可见，这三个OSD保存了这个文件的三分副本，这就是ceph的多副本的基础，将一份数据保存成多个副本，按照一定规则分布在各个OSD中，而多副本的数据的一个特点就是，他们都保存在同名的PG下面，也就是同名的目录下。这样，我们就对PG有了一个直接的理解</p>
<h5 id="PG-troubleshooting"><a href="#PG-troubleshooting" class="headerlink" title="PG troubleshooting"></a><b>PG troubleshooting</b></h5><ul>
<li><p>Degraded<br>降级：由上文可以得知，每个PG有三个副本，分别保存在不同的OSD中，在非故障情况下，这个PG是a+c状态，那么，如果保存0.44这个PG的osd.3挂掉了，这个PG是什么状态呢，操作如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[neo@node2 ~]$ sudo systemctl stop  ceph-osd@3</div><div class="line">[neo@node2 ~]$ sudo ceph pg dump_stuck |egrep ^0.44</div><div class="line">ok</div><div class="line">0.44    active+undersized+degraded      [0,8]   0       [0,8]   0</div></pre></td></tr></table></figure>
<p>  这里我们前往node2节点，手动停止了osd.3，然后查看此时pg 0.44的状态，可见，它此刻的状态是active+undersized+degraded,当一个PG所在的OSD挂掉之后，这个PG就会进入undersized+degraded状态，而后面的[0,8]的意义就是还有两个0.44的副本存活在osd.0和osd.8上。那么现在可以读取刚刚写入的那个文件吗？是可以正常读取的！</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[root@admin test]# rados  -p rbd get char char.txt</div><div class="line">[root@admin test]# cat char.txt </div><div class="line">123</div></pre></td></tr></table></figure>
<p>  降级就是在发生了一些故障比如OSD挂掉之后，ceph将这个OSD上的所有PG标记为degraded，但是此时的集群还是可以正常读写数据的<br>而另一个词undersized,我的理解就是当前存活的PG 0.44数为2，小于副本数3，将其做此标记，也不是严重的问题</p>
</li>
<li>Remapped<br>ceph强大的自我恢复能力,在OSD挂掉5min(default 300s)之后，这个OSD会被标记为out状态，可以理解为ceph认为这个OSD已经不属于集群了，然后就会把PG 0.44 map到别的OSD上去，这个map也是按照一定的规则的，重映射之后呢，就会在另外两个OSD上找到0.44这个PG，而这只是创建了这个目录而已，丢失的数据是要从仅存的OSD上回填到新的OSD上的，处于回填状态的PG就会被标记为backfilling<br>所以当一个PG处于remapped+backfilling状态时，可以认为其处于自我克隆复制的自愈过程</li>
<li><p>Recover<br>这里我们先来做一个测试,首先开启所有OSD使得集群处于健康状态，然后前往osd.3的PG 0.44下面删除char<strong>head_98165844</strong>0这个文件，再通知ceph扫描(scrub)这个PG：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">[root@node2 0.44_head]# pwd &amp;&amp; rm -f char__head_98165844__0</div><div class="line">/var/lib/ceph/osd/ceph-3/current/0.44_head</div><div class="line">[root@node2 0.44_head]# ceph pg scrub 0.44</div><div class="line">instructing pg 0.44 on osd.0 to scrub</div><div class="line">[root@node2 0.44_head]# ceph pg dump |egrep ^0.44 </div><div class="line">dumped all in format plain</div><div class="line">0.44    1       0       0       0       0       4       1       1       active+clean+inconsistent       2017-06-01 11:54:54.980396      89&apos;1    137:181 [0,3,8] 0       [0,3,8] 0       89&apos;1 2017-06-01 11:54:54.980279       0&apos;0     2017-05-27 11:44:16.205723</div></pre></td></tr></table></figure>
<p>  可见，0.44多了个inconsistent状态，顾名思义，ceph发现了这个PG的不一致状态，这样就可以理解这个状态的意义了。<br>想要修复丢失的文件呢，只需要执行ceph pg repair 0.44，ceph就会从别的副本中将丢失的文件拷贝过来，这也是ceph自愈的一个情形。<br>现在再假设一个情形，在osd.4挂掉的过程中呢，我们对char文件进行了写操作，因为集群内还存在着两个副本，是可以正常写入的，但是osd.3内的数据并没有得到更新，过了一会，osd.3上线了，ceph就发现，osd.3的char文件是陈旧的，就通过别的OSD向osd.3进行数据的恢复，使其数据为最新的，而这个恢复的过程中，PG就会被标记为recover</p>
</li>
</ul>
<p>ref<br><a href="http://www.xuxiaopang.com/2016/10/09/easy-ceph-PG/" target="_blank" rel="external">大话Ceph–PG那点事儿</a><br><a href="http://docs.ceph.com/docs/master/rados/operations/pg-states/" target="_blank" rel="external">PLACEMENT GROUP STATES</a><br><a href="https://www.gitbook.com/book/forest/ceph-practice/details" target="_blank" rel="external">Ceph 实战</a><br><a href="http://www.wzxue.com/ceph-osd-and-pg/" target="_blank" rel="external">解析 Ceph : OSD , OSDMap 和 PG, PGMap</a></p>
<hr>
<p>您的鼓励是我写作最大的动力</p>
<p>俗话说，投资效率是最好的投资。 如果您感觉我的文章质量不错，读后收获很大，预计能为您提高 10% 的工作效率，不妨小额捐助我一下，让我有动力继续写出更多好文章。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;PG状态简介&quot;&gt;&lt;a href=&quot;#PG状态简介&quot; class=&quot;headerlink&quot; title=&quot;PG状态简介&quot;&gt;&lt;/a&gt;&lt;b&gt;PG状态简介&lt;/b&gt;&lt;/h5&gt;&lt;p&gt;PG，Placement Group,让我们简单了解几个PG的状态,可以分为:&lt;/p&gt;
&lt;ul
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>INSTALL CEPH (QUICK)</title>
    <link href="https://t1ger.github.io/2017/05/26/INSTALL-CEPH-QUICK/"/>
    <id>https://t1ger.github.io/2017/05/26/INSTALL-CEPH-QUICK/</id>
    <published>2017-05-26T08:25:56.000Z</published>
    <updated>2017-06-21T08:03:26.097Z</updated>
    
    <content type="html"><![CDATA[<h5 id="概念介绍"><a href="#概念介绍" class="headerlink" title="概念介绍"></a><b>概念介绍</b></h5><p>Ceph的部署模式下主要包含以下几个类型的节点:</p>
<ul>
<li>Ceph OSD: 主要用来存储数据，处理数据的replication,恢复，填充，调整资源组合以及通过检查其他OSD进程的心跳信息提供一些监控信息给Ceph Monitors . 当Ceph Storage Cluster 要准备2份数据备份时，要求至少有2个Ceph OSD进程的状态是active+clean状态</li>
<li>Monitor: 维护集群map的状态，主要包括monitor map, OSD map, Placement Group (PG) map, 以及CRUSH map. Ceph 维护了 Ceph Monitors, Ceph OSD Daemons, 以及PGs状态变化的历史记录 (called an “epoch”)</li>
<li>MDS: Ceph Metadata Server (MDS)存储的元数据代表Ceph的文件系统 (i.e., Ceph Block Devices 以及Ceph Object Storage 不适用 MDS). Ceph Metadata Servers 让系统用户可以执行一些POSIX文件系统的基本命令，例如ls,find 等</li>
<li>PG全称Placement Grouops，是一个逻辑的概念，一个PG包含多个OSD。引入PG这一层其实是为了更好的分配数据和定位数据</li>
<li>RBD全称RADOS block device，是Ceph对外提供的块设备服务</li>
<li>RGW全称RADOS gateway，是Ceph对外提供的对象存储服务，接口与S3和Swift兼容</li>
<li>RADOS全称Reliable Autonomic Distributed Object Store，是Ceph集群的精华，用户实现数据分配、Failover等集群操作</li>
<li>Librados是Rados提供库，因为RADOS是协议很难直接访问，因此上层的RBD、RGW和CephFS都是通过librados访问的，目前提供PHP、Ruby、Java、Python、C和C++支持</li>
<li>CephFS全称Ceph File System，是Ceph对外提供的文件系统服务</li>
<li>CRUSH是Ceph使用的数据分布算法，类似一致性哈希，让数据分配到预期的地方</li>
</ul>
<h5 id="部署方案"><a href="#部署方案" class="headerlink" title="部署方案"></a><b>部署方案</b></h5><p>通过ceph-deploy可以快速便捷的安装上ceph,此方法部署需要一个管理节点（admin node)和3个节点来做ceph的存储集群<br>三台装有CentOS 7的主机，每台主机有三个磁盘(虚拟机磁盘要大于100G),详细信息如</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">[root@node1 ~]# cat /etc/redhat-release </div><div class="line">CentOS Linux release 7.3.1611 (Core) </div><div class="line"></div><div class="line">[root@node1 ~]# lsblk</div><div class="line">NAME            MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT</div><div class="line">sda               8:0    0  100G  0 disk </div><div class="line">├─sda1            8:1    0  500M  0 part /boot</div><div class="line">└─sda2            8:2    0 99.5G  0 part </div><div class="line">  ├─system-root 253:0    0 95.6G  0 lvm  /</div><div class="line">  └─system-swap 253:1    0  3.9G  0 lvm  [SWAP]</div><div class="line">sdb               8:16   0    2T  0 disk </div><div class="line">sdc               8:32   0    2T  0 disk </div><div class="line">sdd               8:48   0    2T  0 disk </div><div class="line"></div><div class="line">[root@node1 ~]# cat /etc/hosts</div><div class="line">192.168.138.140 admin</div><div class="line">192.168.138.141 node1</div><div class="line">192.168.138.142 node2</div><div class="line">192.168.138.143 node3</div></pre></td></tr></table></figure>
<p>集群配置如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">admin	192.168.138.140  deploy</div><div class="line">node1   192.168.138.141  mon*1 osd*3</div><div class="line">node2   192.168.138.142  mon*1 osd*3</div><div class="line">node3   192.168.138.143  mon*1 osd*3</div></pre></td></tr></table></figure></p>
<p>备注:在生产环境,每个osd对应一块硬盘,每个osd需要一个journal,建议使用ssd作为osd硬盘的journal.<br>这里测试环境将采用data和journal在一个硬盘的做法</p>
<h5 id="预检Preflight"><a href="#预检Preflight" class="headerlink" title="预检Preflight"></a><b>预检Preflight</b></h5><p>在admin node 节点<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">1. install epel</div><div class="line">sudo yum install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm</div><div class="line">2. add ceph source</div><div class="line">sudo cat &gt; /etc/yum.repos.d/ceph-deploy.repo &lt;&lt; EOF</div><div class="line">[ceph-noarch]</div><div class="line">name=Ceph noarch packages</div><div class="line">baseurl=https://download.ceph.com/rpm-kraken/el7/noarch</div><div class="line">enabled=1</div><div class="line">gpgcheck=1</div><div class="line">type=rpm-md</div><div class="line">gpgkey=https://download.ceph.com/keys/release.asc</div><div class="line">EOF</div><div class="line">3. install ceph-deploy</div><div class="line">sudo yum update &amp;&amp; sudo yum install ceph-deploy -y</div></pre></td></tr></table></figure></p>
<p>在admin node上创建一个拥有sudo权限的用户，并有sudo权限,不要使用ceph这个用户名<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">sudo useradd -g wheel  neo &amp;&amp; su - neo</div><div class="line"></div><div class="line">ssh-keygen</div><div class="line">ssh-copy-id neo@node1</div><div class="line">ssh-copy-id neo@node2</div><div class="line">ssh-copy-id neo@node3</div></pre></td></tr></table></figure></p>
<p>如果开启了防火墙需要<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">sudo firewall-cmd --zone=public --add-service=ceph-mon --permanent</div><div class="line">sudo firewall-cmd --zone=public --add-service=ceph --permanent</div><div class="line">sudo firewalld-cmd --reload</div><div class="line">或者</div><div class="line"># firewall-cmd --zone=public --add-port=6789/tcp --permanent</div><div class="line"># firewall-cmd --zone=public --add-port=6800-7100/tcp --permanent</div><div class="line"># firewall-cmd --reload</div></pre></td></tr></table></figure></p>
<p>For iptables, add port 6789 for Ceph Monitors and ports 6800:7300 for Ceph OSDs</p>
<h5 id="开始安装"><a href="#开始安装" class="headerlink" title="开始安装"></a><b>开始安装</b></h5><p>1.建立部署目录<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[neo@admin ~]$ ceph-deploy --version</div><div class="line">1.5.37</div><div class="line">[neo@admin ~]$ mkdir cluster</div><div class="line">[neo@admin ~]$ cd cluster/</div></pre></td></tr></table></figure></p>
<p>2.清理旧配置,全新安装略过<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">ceph-deploy purge node1 node2 node3</div><div class="line">ceph-deploy purgedata node1 node2 node3</div><div class="line">ceph-deploy forgetkeys</div></pre></td></tr></table></figure></p>
<p>3.Create the cluster<br>初始化集群，告诉 ceph-deploy 哪些节点是监控节点，命令成功执行后会在 cluster 目录下生成 ceph.conf, ceph.log, ceph.mon.keyring 等相关文件：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[neo@admin cluster]$ ceph-deploy new node1 node2 node3</div></pre></td></tr></table></figure></p>
<p>4.在每个 Ceph 节点上都安装 Ceph：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[neo@admin cluster]$ ceph-deploy install node1 node2 node3</div><div class="line">或者登陆节点,手动安装</div><div class="line">yum -y install ceph ceph-radosgw</div></pre></td></tr></table></figure></p>
<p>5.初始化监控节点：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[neo@admin cluster]$ ceph-deploy mon create-initial</div></pre></td></tr></table></figure></p>
<p>6.查看一下 Ceph 存储节点的硬盘情况<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[neo@admin cluster]$ ceph-deploy disk list node1</div><div class="line">[neo@admin cluster]$ ceph-deploy disk list node2</div><div class="line">[neo@admin cluster]$ ceph-deploy disk list node3</div></pre></td></tr></table></figure></p>
<p>7.初始化 Ceph 硬盘，然后创建 osd 存储节点,(存储节点:单个硬盘:对应的 journal 分区，一一对应)：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">ceph-deploy disk zap node1:sdb node1:sdc node1:sdd </div><div class="line">ceph-deploy osd create node1:sdb node1:sdc node1:sdd </div><div class="line"></div><div class="line">ceph-deploy disk zap node2:sdb node2:sdc node2:sdd </div><div class="line">ceph-deploy osd create node2:sdb node2:sdc node2:sdd </div><div class="line"></div><div class="line">ceph-deploy disk zap node3:sdb node3:sdc node3:sdd</div><div class="line">ceph-deploy osd create node3:sdb node3:sdc node3:sdd</div></pre></td></tr></table></figure></p>
<p>8.最后，我们把生成的配置文件从 ceph-adm 同步部署到其他几个节点，使得每个节点的 ceph 配置一致：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[neo@admin cluster]$ ceph-deploy --overwrite-conf admin admin node1 node2 node3</div></pre></td></tr></table></figure>
<h5 id="测试"><a href="#测试" class="headerlink" title="测试"></a><b>测试</b></h5><p>看一下配置成功了没？<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">[root@admin ~]# ceph -s</div><div class="line">    cluster d6d92de4-2a08-4bd6-a749-6c104c88fc40</div><div class="line">     health HEALTH_WARN</div><div class="line">            too few PGs per OSD (21 &lt; min 30)</div><div class="line">     monmap e1: 3 mons at &#123;node1=192.168.138.141:6789/0,node2=192.168.138.142:6789/0,node3=192.168.138.143:6789/0&#125;</div><div class="line">            election epoch 6, quorum 0,1,2 node1,node2,node3</div><div class="line">     osdmap e53: 9 osds: 9 up, 9 in</div><div class="line">            flags sortbitwise,require_jewel_osds</div><div class="line">      pgmap v148: 64 pgs, 1 pools, 0 bytes data, 0 objects</div><div class="line">            307 MB used, 18377 GB / 18378 GB avail</div><div class="line">                  64 active+clean</div><div class="line">[root@admin ~]# ceph health</div><div class="line">HEALTH_WARN too few PGs per OSD (21 &lt; min 30)</div></pre></td></tr></table></figure></p>
<p>备注: 这里需要注意的事执行命令是在root权限下执行的，<br>因为etc/ceph/ceph.client.admin.keyring权限只允许root读写导致，亦可以给文件加权限解决<br>如果在neo权限下,报错如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">[neo@admin cluster]$ ceph health</div><div class="line">2017-05-27 16:58:23.374074 7f567813a700 -1 auth: unable to find a keyring on /etc/ceph/ceph.client.admin.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin: (2) No such file or directory</div><div class="line">2017-05-27 16:58:23.374109 7f567813a700 -1 monclient(hunting): ERROR: missing keyring, cannot use cephx for authentication</div><div class="line">2017-05-27 16:58:23.374113 7f567813a700  0 librados: client.admin initialization error (2) No such file or directory</div><div class="line">Error connecting to cluster: ObjectNotFound</div></pre></td></tr></table></figure></p>
<p>在我们执行ceph -s 时警告PG太少,我们接下来解决,由于是新配置的集群，只有一个pool</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">[neo@admin ~]$ sudo ceph osd lspools</div><div class="line">0 rbd,</div></pre></td></tr></table></figure>
<p>查看rbd pool的PGS<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">[neo@admin ~]$ sudo ceph osd pool get rbd pg_num</div><div class="line">pg_num: 64</div></pre></td></tr></table></figure></p>
<p>pgs为64，因为是3副本的配置，所以当有9个osd的时候, 每个osd上均分了64/9<em>3=21个pgs,也就是出现了如上的错误 小于最小配置30个<br>根据<a href="http://docs.ceph.com/docs/master/rados/configuration/pool-pg-config-ref/" target="_blank" rel="external">官方推荐</a>,Total PGs = (#OSDs </em> 100) / pool size 公式来决定 pg_num（pgp_num 应该设成和 pg_num 一样）,我们可以算出 100*9/3=300,Ceph推荐取最接近2的指数倍，所以选择256<br>解决办法：修改默认pool rbd的pgs和pgp_num,默认两个pg_num和pgp_num一样大小均为64，此处也将两个的值都设为256<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">[neo@admin ~]$ sudo ceph osd pool set rbd pg_num 256</div><div class="line">set pool 0 pg_num to 256</div><div class="line"></div><div class="line">[neo@admin ~]$ sudo ceph osd pool set rbd pgp_num 256</div><div class="line">set pool 0 pgp_num to 128set pool 0 pgp_num to 256</div><div class="line"></div><div class="line">[neo@admin ~]$ sudo ceph health</div><div class="line">HEALTH_OK</div><div class="line"></div><div class="line">[neo@admin ~]$ sudo ceph -s</div><div class="line">    cluster d6d92de4-2a08-4bd6-a749-6c104c88fc40</div><div class="line">     health HEALTH_OK</div><div class="line">     monmap e1: 3 mons at &#123;node1=192.168.138.141:6789/0,node2=192.168.138.142:6789/0,node3=192.168.138.143:6789/0&#125;</div><div class="line">            election epoch 18, quorum 0,1,2 node1,node2,node3</div><div class="line">     osdmap e116: 9 osds: 9 up, 9 in</div><div class="line">            flags sortbitwise,require_jewel_osds</div><div class="line">      pgmap v412: 256 pgs, 1 pools, 4 bytes data, 1 objects</div><div class="line">            324 MB used, 18377 GB / 18378 GB avail</div><div class="line">                 256 active+clean</div></pre></td></tr></table></figure></p>
<p>ref<br><a href="http://www.xuxiaopang.com/2016/10/10/ceph-full-install-el7-jewel/" target="_blank" rel="external">CEPH部署完整版(el7+jewel)</a><br><a href="http://ceph.com/install/" target="_blank" rel="external">INSTALL CEPH (QUICK)</a><br><a href="http://www.codexiu.cn/linux/blog/42056/" target="_blank" rel="external">centos7 ceph-deploy 安装jewel</a><br><a href="http://zedshadow.leanote.com/post/%E4%BD%BF%E7%94%A8ceph-deploy%E5%BF%AB%E9%80%9F%E9%83%A8%E7%BD%B2Ceph" target="_blank" rel="external">centos7使用ceph-deploy快速部署Kraken版Ceph</a><br><a href="http://www.vpsee.com/2015/07/install-ceph-on-centos-7/" target="_blank" rel="external">在 CentOS 7.1 上安装分布式存储系统 Ceph</a><br><a href="http://www.cnblogs.com/zhangzhengyan/p/5839897.html" target="_blank" rel="external">ceph 创建和删除osd</a><br><a href="http://blog.csdn.net/xiongwenwu/article/details/53120415" target="_blank" rel="external">三种增删osd的方法数据量迁移大小</a><br><a href="https://www.virtualtothecore.com/en/quickly-build-a-new-ceph-cluster-with-ceph-deploy-on-centos-7/" target="_blank" rel="external">Quickly build a new Ceph cluster with ceph-deploy on CentOS 7</a><br><a href="http://blog.csdn.net/chinagissoft/article/details/50491429" target="_blank" rel="external">Ceph 集群部署</a><br><a href="https://my.oschina.net/diluga/blog/528618" target="_blank" rel="external">PG设置与规划</a><br><a href="https://my.oschina.net/xiaozhublog/blog/664560" target="_blank" rel="external">health HEALTH_WARN too few PGs per OSD</a><br><a href="https://github.com/thesues/cephdoc/blob/master/ceph-deploy-cn.markdown" target="_blank" rel="external">thesues/cephdoc</a><br><a href="http://www.isjian.com/ceph/deploy-a-ceph-cluster-manually/" target="_blank" rel="external">手工部署一个ceph集群</a></p>
<hr>
<p>您的鼓励是我写作最大的动力</p>
<p>俗话说，投资效率是最好的投资。 如果您感觉我的文章质量不错，读后收获很大，预计能为您提高 10% 的工作效率，不妨小额捐助我一下，让我有动力继续写出更多好文章。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;概念介绍&quot;&gt;&lt;a href=&quot;#概念介绍&quot; class=&quot;headerlink&quot; title=&quot;概念介绍&quot;&gt;&lt;/a&gt;&lt;b&gt;概念介绍&lt;/b&gt;&lt;/h5&gt;&lt;p&gt;Ceph的部署模式下主要包含以下几个类型的节点:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ceph OSD: 主要用来存储数
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Install SpiderFoot on Centos7</title>
    <link href="https://t1ger.github.io/2017/05/25/Install-SpiderFoot-on-centos7/"/>
    <id>https://t1ger.github.io/2017/05/25/Install-SpiderFoot-on-centos7/</id>
    <published>2017-05-25T09:48:22.000Z</published>
    <updated>2017-05-25T09:20:40.754Z</updated>
    
    <content type="html"><![CDATA[<h5 id="简介"><a href="#简介" class="headerlink" title="简介"></a><b>简介</b></h5><p>SpiderFoot是一个Python编写的免费开源的网站信息收集类工具，并且支持跨平台运行，适用于Linux、*BSD和Windows系统。<br>此外，它还为用户提供了一个易于使用的GUI界面。<br>在功能方面SpiderFoot也为我们考虑的非常周全，通过SpiderFoot我们可以获取相关目标的各种信息，例如网站子域、电子邮件地址、web服务器版本等等。<br>SpiderFoot简单的基于Web的界面，使你能够在安装后立即启动扫描 – 只需简单的设置扫描目标域名，并启用相应的扫描模块即可</p>
<h5 id="环境依赖"><a href="#环境依赖" class="headerlink" title="环境依赖"></a><b>环境依赖</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pip install lxml netaddr M2Crypto cherrypy mako requests bs4</div></pre></td></tr></table></figure>
<h5 id="安装"><a href="#安装" class="headerlink" title="安装"></a><b>安装</b></h5><p><a href="http://www.spiderfoot.net/download/" target="_blank" rel="external">点击这里下载</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">~$ tar zxvf spiderfoot-X.X.X-src.tar.gz</div><div class="line">~$ cd spiderfoot-X.X.X</div><div class="line">~/spiderfoot-X.X.X$</div></pre></td></tr></table></figure></p>
<h6 id=""><a href="#" class="headerlink" title=""></a><b?启动 spiderfoot<="" b=""></b?启动></h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">python ./sf.py</div><div class="line">or</div><div class="line">python ./sf.py 0.0.0.0:5001</div><div class="line"></div><div class="line">也可以指定端口</div><div class="line">python ./sf.py 127.0.0.1:9999</div></pre></td></tr></table></figure>
<h5 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a><b>遇到的问题</b></h5><ol>
<li><p>Python.h：No such file or directory<br>出现No such file or directory的错误，有两种情况，一种是真的没有Python.h这个文件，一种是Python的版本不对<br>可以进入/usr/include/文件夹下的Python2.x文件夹里查找是否有Python.h这个文件<br>这里是第一种,yum install python-devel 解决</p>
</li>
<li><p>fatal error: openssl/err.h:<br>执行 yum install openssl-devel 解决</p>
</li>
</ol>
<hr>
<p>您的鼓励是我写作最大的动力</p>
<p>俗话说，投资效率是最好的投资。 如果您感觉我的文章质量不错，读后收获很大，预计能为您提高 10% 的工作效率，不妨小额捐助我一下，让我有动力继续写出更多好文章。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;&lt;b&gt;简介&lt;/b&gt;&lt;/h5&gt;&lt;p&gt;SpiderFoot是一个Python编写的免费开源的网站信息收集类工具，并且支持跨平台运行，适用于Linux、*BSD和Wi
    
    </summary>
    
    
  </entry>
  
</feed>
