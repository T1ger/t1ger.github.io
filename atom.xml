<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>t1ger的茶馆</title>
  <subtitle>头顶有光终是幻，足下生云未是仙</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://t1ger.github.io/"/>
  <updated>2017-08-08T10:03:21.139Z</updated>
  <id>https://t1ger.github.io/</id>
  
  <author>
    <name>t1ger</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>a simplified command-line interface to VMware vCenter</title>
    <link href="https://t1ger.github.io/2017/08/08/a-simplified-command-line-interface-to-VMware-vCenter/"/>
    <id>https://t1ger.github.io/2017/08/08/a-simplified-command-line-interface-to-VMware-vCenter/</id>
    <published>2017-08-08T07:38:20.000Z</published>
    <updated>2017-08-08T10:03:21.139Z</updated>
    
    <content type="html"><![CDATA[<p>govc allows you to interface with VMware vCenter without the need for the dreaded vClient,Windows machines or to write you own scripts to access the horrible VMware API. This way you can easily automate many tasks on VMware directly from the command line or your bash scripts</p>
<p>by the way ,I don’t like interface with ESX/vCenter,this is so cool tools what I desired</p>
<p>First thing,you can compile it yourself, but there’s  handly binaries already available on the project github page <a href="https://github.com/vmware/govmomi/releases" target="_blank" rel="external">here</a></p>
<p>I downloaded it and created a small wrapper for it:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"># curl -LO https://github.com/vmware/govmomi/releases/download/v0.15.0/govc_linux_amd64.gz</div><div class="line"># gunzip govc_linux_amd64.gz</div><div class="line"># cat &gt;govc &lt;&lt;EOF</div><div class="line">#!/bin/bash</div><div class="line"> </div><div class="line">export GOVC_URL=&apos;https://username:password@vsphere-ip-or-hostname/sdk&apos;</div><div class="line">export GOVC_DATACENTER=VSPHERE_DC</div><div class="line">export GOVC_INSECURE=true</div><div class="line"> </div><div class="line">/usr/bin/govc_linux_amd64 \$@</div><div class="line">EOF</div><div class="line"># chmod +x govc*</div><div class="line"># cp -i govc* /usr/bin/</div></pre></td></tr></table></figure></p>
<p>Note that you only need GOVC_INSECURE=true if you are using self-signed certificates and you don’t have CA added to you local trusted certs.</p>
<p>For a start,you can get some basic info about you environment:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line">[root@localhost daily]# ./govc about</div><div class="line">Name:         VMware vCenter Server</div><div class="line">Vendor:       VMware, Inc.</div><div class="line">Version:      5.1.0</div><div class="line">Build:        880146</div><div class="line">OS type:      win32-x64</div><div class="line">API type:     VirtualCenter</div><div class="line">API version:  5.1</div><div class="line">Product ID:   vpx</div><div class="line">UUID:         4AC51BFC-DC4E-47D3-A912-B51A1A28BAFA</div><div class="line"></div><div class="line">[root@localhost daily]# ./govc datacenter.info</div><div class="line">Name:                ws_dc01</div><div class="line">  Path:              /ws_dc01</div><div class="line">  Hosts:             5</div><div class="line">  Clusters:          0</div><div class="line">  Virtual Machines:  69</div><div class="line">  Networks:          3</div><div class="line">  Datastores:        10</div><div class="line">Name:                ws_dc02</div><div class="line">  Path:              /ws_dc02</div><div class="line">  Hosts:             5</div><div class="line">  Clusters:          0</div><div class="line">  Virtual Machines:  118</div><div class="line">  Networks:          32</div><div class="line">  Datastores:        5</div></pre></td></tr></table></figure></p>
<p>if you want to get usage about govc,just run govc without any argument. To see what parameters a command supports, run govc command –help </p>
<p>eg,you can easily get some info with one simple command:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div></pre></td><td class="code"><pre><div class="line">[root@localhost daily]# ./govc datastore.info </div><div class="line">Name:        datastore21</div><div class="line">  Path:      /ws_dc01/datastore/datastore21</div><div class="line">  Type:      VMFS</div><div class="line">  URL:       ds:///vmfs/volumes/522460d8-f202b496-a69f-90b11c2afe3d/</div><div class="line">  Capacity:  2508.2 GB</div><div class="line">  Free:      470.2 GB</div><div class="line">Name:        datastore20</div><div class="line">  Path:      /ws_dc02/datastore/datastore20</div><div class="line">  Type:      VMFS</div><div class="line">  URL:       ds:///vmfs/volumes/52778cbb-107633af-7209-90b11c2a9f0f/</div><div class="line">  Capacity:  2508.2 GB</div><div class="line">  Free:      176.5 GB</div><div class="line">  </div><div class="line">[root@localhost daily]# ./govc host.info</div><div class="line">/usr/bin/govc_linux_amd64: default host resolves to multiple instances, please specify</div><div class="line">[root@localhost daily]# ./govc host.info -host.ip=192.168.1.20</div><div class="line">Name:              192.168.1.20</div><div class="line">  Path:            /ws_dc01/host/192.168.1.20/192.168.1.20</div><div class="line">  Manufacturer:    Dell Inc.</div><div class="line">  Logical CPUs:    32 CPUs @ 2599MHz</div><div class="line">  Processor type:  Intel(R) Xeon(R) CPU E5-2670 0 @ 2.60GHz</div><div class="line">  CPU usage:       2703 MHz (3.3%)</div><div class="line">  Memory:          130976MB</div><div class="line">  Memory usage:    92697 MB (-1.2%)</div><div class="line">  Boot time:       2017-06-26 11:09:53.406999 +0000 UTC</div><div class="line"></div><div class="line">  </div><div class="line">[root@localhost daily]# ./govc vm.info  -vm.ip=192.168.1.180</div><div class="line">Name:           ws-180</div><div class="line">  Path:         /ws_dc01/vm/ws-180</div><div class="line">  UUID:         4232a9b7-a1de-e45b-453b-beade5b7935a</div><div class="line">  Guest name:   CentOS 4/5/6 (64-bit)</div><div class="line">  Memory:       4096MB</div><div class="line">  CPU:          8 vCPU(s)</div><div class="line">  Power state:  poweredOn</div><div class="line">  Boot time:    2017-06-26 11:34:54.042422 +0000 UTC</div><div class="line">  IP address:   192.168.1.180</div><div class="line">  Host:         192.168.1.20</div></pre></td></tr></table></figure></p>
<p>If you want to look someting , it good way to use ls command<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">[root@localhost daily]# ./govc ls</div><div class="line">/ws_dc01/vm</div><div class="line">/ws_dc01/network</div><div class="line">/ws_dc01/host</div><div class="line">/ws_dc01/datastore</div><div class="line">[root@localhost daily]# ./govc ls /ws_dc01/vm</div><div class="line">/ws_dc01/vm/elk149</div><div class="line">/ws_dc01/vm/elk148</div><div class="line">/ws_dc01/vm/elk147</div><div class="line">...</div><div class="line"></div><div class="line">[root@localhost daily]# ./govc ls /ws_dc01/host</div><div class="line">/ws_dc01/host/192.168.1.20</div><div class="line">/ws_dc01/host/192.168.1.21</div><div class="line">...</div><div class="line"></div><div class="line">[root@localhost daily]# ./govc ls /ws_dc01/datastore</div><div class="line">/ws_dc01/datastore/datastore21</div><div class="line">/ws_dc01/datastore/datastore20</div><div class="line">...</div></pre></td></tr></table></figure></p>
<p>govc also allow you to run esxcli on the specified host, for example in one of my many experiments I ran:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">[root@localhost daily]# ./govc host.esxcli  --host.ip=172.16.56.20 vm process list|grep DisplayName | awk &#123;&apos;print $2&apos;&#125; | sort</div><div class="line">debug140</div><div class="line">debug141</div><div class="line">debug142</div><div class="line">elk147</div><div class="line">elk148</div><div class="line">elk149</div><div class="line">...</div></pre></td></tr></table></figure></p>
<p>A very import thing to notice is that every command can be run with the parameters -json=true.This output a machine-parsable format that includes many more details than the normal text output.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line">[root@localhost daily]# ./govc ls -json /ws_dc01/datastore/datastore20|python -m json.tool</div><div class="line">&#123;</div><div class="line">    &quot;DeRef&quot;: false,</div><div class="line">    &quot;Dump&quot;: false,</div><div class="line">    &quot;JSON&quot;: true,</div><div class="line">    &quot;Long&quot;: false,</div><div class="line">    &quot;Out&quot;: &#123;&#125;,</div><div class="line">    &quot;TTY&quot;: false,</div><div class="line">    &quot;ToRef&quot;: false,</div><div class="line">    &quot;Type&quot;: &quot;&quot;,</div><div class="line">    &quot;elements&quot;: [</div><div class="line">        &#123;</div><div class="line">            &quot;Object&quot;: &#123;</div><div class="line">                &quot;AlarmActionsEnabled&quot;: true,</div><div class="line">                &quot;AvailableField&quot;: null,</div><div class="line">                &quot;Browser&quot;: &#123;</div><div class="line">                    &quot;Type&quot;: &quot;HostDatastoreBrowser&quot;,</div><div class="line">                    &quot;Value&quot;: &quot;datastoreBrowser-datastore-514&quot;</div><div class="line">                &#125;,</div><div class="line">                &quot;Capability&quot;: &#123;</div><div class="line">                    &quot;DirectoryHierarchySupported&quot;: true,</div><div class="line">                    &quot;NativeSnapshotSupported&quot;: false,</div><div class="line">                    &quot;PerFileThinProvisioningSupported&quot;: true,</div><div class="line">                    &quot;RawDiskMappingsSupported&quot;: true,</div><div class="line">                    &quot;SeSparseSupported&quot;: null,</div><div class="line">                    &quot;StorageIORMSupported&quot;: true,</div><div class="line">                    &quot;TopLevelDirectoryCreateSupported&quot;: null,</div><div class="line">                    &quot;UpitSupported&quot;: null,</div><div class="line">                    &quot;VmfsSparseSupported&quot;: null,</div><div class="line">                    &quot;VsanSparseSupported&quot;: null</div><div class="line">                &#125;,</div><div class="line">[...]</div></pre></td></tr></table></figure></p>
<p>of course,could be very handy for your scripts!</p>
<p>ref<br><a href="http://www.virtuallyghetto.com/2014/09/govmomi-vsphere-sdk-for-go-govc-cli-kubernetes-on-vsphere-part-1.html" target="_blank" rel="external">govmomi (vSphere SDK for Go), govc CLI &amp; Kubernetes on vSphere</a><br><a href="https://velenux.wordpress.com/2016/09/19/automate-your-vcenter-interactions-from-the-linux-commandline-with-govmomi-and-govc/" target="_blank" rel="external">Automate your vCenter interactions from the Linux commandline with govmomi and govc</a><br><a href="https://dellaert.org/2013/03/04/pysphere-script-to-clone-a-template-into-multiple-vms-with-post-processing/" target="_blank" rel="external">PySphere script to clone a template into multiple VMs with post processing</a><br><a href="https://github.com/tkak/terraform-provider-vsphere" target="_blank" rel="external">terraform-provider-vsphere</a></p>
<hr>
<p>您的鼓励是我写作最大的动力</p>
<p>俗话说，投资效率是最好的投资。 如果您感觉我的文章质量不错，读后收获很大，预计能为您提高 10% 的工作效率，不妨小额捐助我一下，让我有动力继续写出更多好文章。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;govc allows you to interface with VMware vCenter without the need for the dreaded vClient,Windows machines or to write you own scripts to
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Prometheus 监控方案</title>
    <link href="https://t1ger.github.io/2017/07/04/Prometheus-%E7%9B%91%E6%8E%A7%E6%96%B9%E6%A1%88/"/>
    <id>https://t1ger.github.io/2017/07/04/Prometheus-监控方案/</id>
    <published>2017-07-04T08:01:05.000Z</published>
    <updated>2017-07-10T10:09:00.288Z</updated>
    
    <content type="html"><![CDATA[<h5 id="Prometheus-安装"><a href="#Prometheus-安装" class="headerlink" title="Prometheus 安装"></a><b>Prometheus 安装</b></h5><p>centos6的安装参考<a href="http://deadline.top/2016/11/16/%E7%9B%91%E6%8E%A7%E5%B7%A5%E5%85%B7%E4%B9%8BPrometheus/" target="_blank" rel="external">这里</a>,以下以centos7为例介绍:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">sudo yum install pygpgme yum-utils</div><div class="line">#For centos7</div><div class="line">sudo cat &gt; /etc/yum.repos.d/prometheus-rpm_release.repo &lt;&lt; EOF</div><div class="line">[prometheus]</div><div class="line">name=prometheus</div><div class="line">baseurl=https://packagecloud.io/prometheus-rpm/release/el/7/$basearch</div><div class="line">repo_gpgcheck=1</div><div class="line">enabled=1</div><div class="line">gpgkey=https://packagecloud.io/prometheus-rpm/release/gpgkey</div><div class="line">       https://raw.githubusercontent.com/lest/prometheus-rpm/master/RPM-GPG-KEY-prometheus-rpm</div><div class="line">gpgcheck=1</div><div class="line">sslverify=1</div><div class="line">sslcacert=/etc/pki/tls/certs/ca-bundle.crt</div><div class="line">metadata_expire=300</div><div class="line">EOF</div><div class="line"></div><div class="line">[root@localhost ~]# yum install prometheus -y</div><div class="line">[root@localhost ~]# prometheus -version</div><div class="line">prometheus, version 1.7.1 (branch: master, revision: 3afb3fffa3a29c3de865e1172fb740442e9d0133)</div><div class="line">  build user:       root@0aa1b7fc430d</div><div class="line">  build date:       20170612-11:44:05</div><div class="line">  go version:       go1.8.3</div><div class="line"></div><div class="line">[root@localhost ~]# systemctl start prometheus</div></pre></td></tr></table></figure></p>
<p>可以通过<a href="http://localhost:9090/metrics访问啦" target="_blank" rel="external">http://localhost:9090/metrics访问啦</a></p>
<h5 id="grafana-安装"><a href="#grafana-安装" class="headerlink" title="grafana 安装"></a><b>grafana 安装</b></h5><p>grafana安装方法参考<a href="http://docs.grafana.org/installation/rpm/" target="_blank" rel="external">这里</a>,这里选择yum安装<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[root@localhost ~]# sudo yum install https://s3-us-west-2.amazonaws.com/grafana-releases/release/grafana-4.3.1-1.x86_64.rpm</div><div class="line">[root@localhost ~]# systemctl start grafana-server.service</div><div class="line">[root@localhost ~]# sudo systemctl enable grafana-server.service</div></pre></td></tr></table></figure></p>
<p>可以通过<a href="http://localhost:3000/metrics访问啦,默认密码admin/admin" target="_blank" rel="external">http://localhost:3000/metrics访问啦,默认密码admin/admin</a><br>如果单独的导入模板,可以忽略以下步骤<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">#uncomment config file on /etc/grafana/grafana.ini </div><div class="line">[dashboards.json]</div><div class="line">enabled = true</div><div class="line">path = /var/lib/grafana/dashboards</div><div class="line"></div><div class="line">#install dashboards</div><div class="line">git clone https://github.com/percona/grafana-dashboards.git</div><div class="line">cp -r grafana-dashboards/dashboards /var/lib/grafana/</div><div class="line"></div><div class="line">#Restart Grafana</div><div class="line">systemctl restart grafana-server.service</div></pre></td></tr></table></figure></p>
<p>这里需要注意的是,如果你的grafana数据源名字不是Prometheus,请注意导入数据库模板时重新关联数据源,否则会包模板初始化失败<br>添加数据源和模板参考<a href="https://www.hi-linux.com/posts/25047.html" target="_blank" rel="external">这里</a></p>
<h5 id="Configuring-Prometheus"><a href="#Configuring-Prometheus" class="headerlink" title="Configuring Prometheus"></a><b>Configuring Prometheus</b></h5><ul>
<li><p>linux node监控配置<br>首先,配置好prometheus.repo,如果有防火墙开放9100端口.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">#install node_exporter</div><div class="line">[root@localhost ~]# yum install node_exporter -y</div><div class="line">[root@localhost ~]# systemctl start node_exporter</div><div class="line"></div><div class="line">#prometheus server configure,add to /etc/prometheus/prometheus.yml</div><div class="line"></div><div class="line">  - job_name: &apos;linux&apos;</div><div class="line"></div><div class="line">    static_configs:</div><div class="line">      - targets: [&apos;192.168.1.106:9100&apos;]</div><div class="line">        labels:</div><div class="line">          instance: &apos;dev_106&apos;</div></pre></td></tr></table></figure>
</li>
<li><p>mysql node 监控配置,如果有防火墙开放9104端口.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line">#install mysqld_exporter</div><div class="line">[root@localhost ~]# yum install mysqld_exporter -y</div><div class="line">[root@localhost ~]# systemctl start mysqld-exporter</div><div class="line"></div><div class="line">#prometheus server configure,add to /etc/prometheus/prometheus.yml</div><div class="line"></div><div class="line">  - job_name: &apos;mysql&apos;</div><div class="line"></div><div class="line">    static_configs:</div><div class="line">      - targets: [&apos;192.168.1.106:9104&apos;]</div><div class="line">        labels:</div><div class="line">          instance: &apos;dev_106_db&apos; </div><div class="line"></div><div class="line">#mysqld_exporter需要连接到Mysql，创建用户并赋予所需的权限： </div><div class="line"></div><div class="line">mysql&gt; create user monitor@localhost identified by &apos;monitor&apos; with max_user_connections 3;</div><div class="line">mysql&gt; grant process,replication client,select on *.* to monitor@localhost;</div><div class="line"></div><div class="line">#mysqld_exporter默认会读取~/.my.cnf文件</div><div class="line">#my.cnf file for MySQL exporter should be as follows: </div><div class="line">cat &lt;&lt; EOF &gt; .my.cnf</div><div class="line">[client]</div><div class="line">user=monitor</div><div class="line">password=monitor</div><div class="line">host=192.168.1.106</div><div class="line">EOF</div></pre></td></tr></table></figure>
</li>
<li><p>网络监控,提供 http、dns、tcp、icmp（ping）的监控,如果有防火墙开放9115端口.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"># install blackbox_exporter </div><div class="line">[root@localhost ~]# yum install blackbox_exporter -y</div><div class="line">[root@localhost ~]# systemctl start blackbox_exporter</div></pre></td></tr></table></figure>
<p>  1.Ping 应用</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line">#prometheus server configure,add to /etc/prometheus/prometheus.yml</div><div class="line"></div><div class="line">  - job_name: &apos;ping_all&apos;</div><div class="line">    scrape_interval: 5s</div><div class="line">    metrics_path: /probe</div><div class="line">    params:</div><div class="line">      module: [icmp]  #ping</div><div class="line">    static_configs:</div><div class="line">      - targets: [&apos;219.150.32.132&apos;, &apos;219.148.204.66&apos;]</div><div class="line">        labels:</div><div class="line">          group: &apos;一线城市-电信网络监控&apos;</div><div class="line">      - targets: [&apos;218.8.251.163&apos;, &apos;218.107.51.1&apos;]</div><div class="line">        labels:</div><div class="line">          group: &apos;一线城市-联通网络监控&apos;</div><div class="line">    relabel_configs:</div><div class="line">      - source_labels: [__address__]</div><div class="line">        regex: (.*)(:80)?</div><div class="line">        target_label: __param_target</div><div class="line">        replacement: $&#123;1&#125;</div><div class="line">      - source_labels: [__param_target]</div><div class="line">        regex: (.*)</div><div class="line">        target_label: ping</div><div class="line">        replacement: $&#123;1&#125;</div><div class="line">      - source_labels: []</div><div class="line">        regex: .*</div><div class="line">        target_label: __address__</div><div class="line">        replacement: 127.0.0.1:9115  # Blackbox exporter.</div></pre></td></tr></table></figure>
<p>  在 grafana中增加 Data Sources 选 prometheus,然后按照grafana的文档新定制一个面板<br>ROW中指标选probe_duration_seconds{job=”ping_all”}<br>2.检测ssl 证书失效</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line">#prometheus server configure,add to /etc/prometheus/prometheus.yml</div><div class="line"></div><div class="line">rule_files:</div><div class="line">  - ssl_expiry.rules</div><div class="line">scrape_configs:</div><div class="line">  - job_name: &apos;blackbox&apos;</div><div class="line">    metrics_path: /probe</div><div class="line">    params:</div><div class="line">      module: [http_2xx]  # Look for a HTTP 200 response.</div><div class="line">    static_configs:</div><div class="line">      - targets:</div><div class="line">        - example.com  # Target to probe</div><div class="line">    relabel_configs:</div><div class="line">      - source_labels: [__address__]</div><div class="line">        regex: (.*?)(:80)?</div><div class="line">        target_label: __param_target</div><div class="line">        replacement: https://$&#123;1&#125;</div><div class="line">      - source_labels: [__param_target]</div><div class="line">        target_label: instance</div><div class="line">      - target_label: __address__</div><div class="line">        replacement: 127.0.0.1:9115  # Blackbox exporter.</div><div class="line"></div><div class="line">cat &lt;&lt; &apos;EOF&apos; &gt; ssl_expiry.rules</div><div class="line">ALERT SSLCertExpiringSoon</div><div class="line"> IF probe_ssl_earliest_cert_expiry&#123;job=&quot;blackbox&quot;&#125; - time() &lt; 86400 * 30</div><div class="line"> FOR 10m</div><div class="line">EOF</div></pre></td></tr></table></figure>
<p>  可以通过 <a href="http://localhost:9090/alerts访问,失效前30天将收到告警" target="_blank" rel="external">http://localhost:9090/alerts访问,失效前30天将收到告警</a>.</p>
</li>
<li><p>snmp_exporter 监控配置,如果有防火墙开放9116端口.<br>安装snmp_exporter,参考<a href="https://github.com/prometheus/snmp_exporter" target="_blank" rel="external">这里</a>,下面为安装脚本</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">version=v0.4.0</div><div class="line">file=snmp_exporter-0.4.0.linux-amd64</div><div class="line"> </div><div class="line">wget https://github.com/prometheus/snmp_exporter/releases/download/$version/$file.tar.gz \</div><div class="line">  -O /tmp/$file.tar.gz</div><div class="line">cd /tmp</div><div class="line">tar xvf /tmp/$file.tar.gz</div><div class="line">cp /tmp/$file/snmp_exporter /usr/local/bin/snmp_exporter</div><div class="line"> </div><div class="line">tee /usr/lib/systemd/system/snmp_exporter.service &lt;&lt; EOS</div><div class="line">[Unit]</div><div class="line">Description=SNMP Exporter</div><div class="line">[Service]</div><div class="line">ExecStart=/usr/local/bin/snmp_exporter -config.file /etc/prometheus/snmp.yml</div><div class="line">[Install]</div><div class="line">WantedBy=default.target</div><div class="line">EOS</div><div class="line"> </div><div class="line">systemctl daemon-reload</div><div class="line">systemctl enable snmp_exporter</div><div class="line">systemctl start snmp_exporter </div><div class="line">cd -</div></pre></td></tr></table></figure>
<p>  我们可以通过 <a href="http://localhost:9116来验证是否允许,接下来我们进行prometheus配置" target="_blank" rel="external">http://localhost:9116来验证是否允许,接下来我们进行prometheus配置</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">#prometheus server configure,add to /etc/prometheus/prometheus.yml	</div><div class="line">	scrape_configs:</div><div class="line">  - job_name: &apos;snmp&apos;</div><div class="line">    static_configs:</div><div class="line">      - targets:</div><div class="line">        - 192.168.1.2  # SNMP device.</div><div class="line">    metrics_path: /snmp</div><div class="line">    params:</div><div class="line">      module: [default]</div><div class="line">    relabel_configs:</div><div class="line">      - source_labels: [__address__]</div><div class="line">        target_label: __param_target</div><div class="line">      - source_labels: [__param_target]</div><div class="line">        target_label: instance</div><div class="line">      - target_label: __address__</div><div class="line">        replacement: 127.0.0.1:9116  # SNMP exporter.</div></pre></td></tr></table></figure>
<p>  稍等一会儿,我们就可以通过 <a href="http://localhost:9090/consoles/snmp.html" target="_blank" rel="external">http://localhost:9090/consoles/snmp.html</a> 查看接口统计</p>
</li>
<li><p>redis_exporter 监控配置,如果有防火墙开放9121端口.<br>安装 redis_exporter ,具体参考<a href="https://github.com/oliver006/redis_exporter" target="_blank" rel="external">这里</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">$ go get</div><div class="line">$ go build</div><div class="line">$ ./redis_exporter &lt;flags&gt;</div><div class="line"></div><div class="line">#prometheus server configure,add to /etc/prometheus/prometheus.yml</div><div class="line">- job_name: redis_exporter</div><div class="line">  static_configs:</div><div class="line">  - targets: [&apos;localhost:9121&apos;]</div></pre></td></tr></table></figure>
<p>  grafana中模板redis模板参考<a href="https://grafana.com/dashboards/763/revisions" target="_blank" rel="external">这里</a></p>
</li>
<li><p>nginx_exporter 监控配置<br>1.通过nginx-vts-exporter 监控,安装参考<a href="https://www.hi-linux.com/posts/27014.html" target="_blank" rel="external">这里</a>,grafana模板参考<a href="https://grafana.com/dashboards/1623" target="_blank" rel="external">这里</a><br>2.通过nginx-lua-prometheus 监控,安装参考<a href="https://github.com/knyar/nginx-lua-prometheus" target="_blank" rel="external">这里</a>,grafana模板参考<a href="https://grafana.com/dashboards/462" target="_blank" rel="external">这里</a><br>3.通过nginx-exporter 监控,安装参考<a href="https://github.com/discordianfish/nginx_exporter" target="_blank" rel="external">这里</a>,未找到相应grafana模板,不推荐<br>4.监控Nginx流量的扩展程序,安装参考<a href="https://github.com/vovolie/lua-nginx-prometheus" target="_blank" rel="external">这里</a><br>5.通过日志监控,参考<a href="https://github.com/martin-helmich/prometheus-nginxlog-exporter" target="_blank" rel="external">这里</a></p>
</li>
<li><p>ceph_exporter 监控配置<br>1.通过ceph_exporter 监控,安装参考<a href="https://github.com/digitalocean/ceph_exporter" target="_blank" rel="external">这里</a>,grafana模板参考<a href="https://grafana.com/dashboards/917" target="_blank" rel="external">这里</a></p>
</li>
<li><p>gluster_exporter 监控配置<br>1.通过gluster_exporter 监控,安装参考<a href="https://github.com/ofesseler/gluster_exporter" target="_blank" rel="external">这里</a></p>
</li>
<li><p>JMX Exporter 监控配置<br>1.通过JMX Exporter 监控,安装参考<a href="https://github.com/prometheus/jmx_exporter" target="_blank" rel="external">这里</a></p>
</li>
</ul>
<p>ref<br><a href="https://prometheus.io/" target="_blank" rel="external">prometheus</a><br><a href="https://www.iamle.com/archives/2130.html" target="_blank" rel="external">用Prometheus进行网络质量ping监控Grafana进行监控数据展示</a><br><a href="https://www.robustperception.io/get-alerted-before-your-ssl-certificates-expire/" target="_blank" rel="external">Get alerted before your SSL certificates expire</a><br><a href="https://github.com/fstab/prometheus-for-java-developers" target="_blank" rel="external">Prometheus Monitoring for Java Developers</a><br><a href="https://prometheus.io/docs/instrumenting/exporters/" target="_blank" rel="external">EXPORTERS AND INTEGRATIONS</a><br><a href="https://www.digitalocean.com/community/tutorials/how-to-add-a-prometheus-dashboard-to-grafana" target="_blank" rel="external">How To Add a Prometheus Dashboard to Grafana</a><br><a href="http://www.cnblogs.com/vovlie/p/Nginx_monitoring.html" target="_blank" rel="external">Prometheus 监控 Nginx 流量</a><br><a href="https://github.com/martin-helmich/prometheus-nginxlog-exporter" target="_blank" rel="external">NGINX Performance Metrics with Prometheus</a></p>
<hr>
<p>您的鼓励是我写作最大的动力</p>
<p>俗话说，投资效率是最好的投资。 如果您感觉我的文章质量不错，读后收获很大，预计能为您提高 10% 的工作效率，不妨小额捐助我一下，让我有动力继续写出更多好文章。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;Prometheus-安装&quot;&gt;&lt;a href=&quot;#Prometheus-安装&quot; class=&quot;headerlink&quot; title=&quot;Prometheus 安装&quot;&gt;&lt;/a&gt;&lt;b&gt;Prometheus 安装&lt;/b&gt;&lt;/h5&gt;&lt;p&gt;centos6的安装参考&lt;a href
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>增加/删除 OSD</title>
    <link href="https://t1ger.github.io/2017/06/23/%E5%A2%9E%E5%8A%A0-%E5%88%A0%E9%99%A4-OSD/"/>
    <id>https://t1ger.github.io/2017/06/23/增加-删除-OSD/</id>
    <published>2017-06-23T10:51:21.000Z</published>
    <updated>2017-06-30T07:36:24.133Z</updated>
    
    <content type="html"><![CDATA[<h5 id="增加-OSD-手动"><a href="#增加-OSD-手动" class="headerlink" title="增加 OSD(手动)"></a><b>增加 OSD(手动)</b></h5><p>首先修改各个节点的/etc/hosts信息,增加新节点信息,并添加ceph.client.admin.keyring,确保ceph.client.admin.keyring有正确的权限<br>要增加一个 OSD,要依次创建数据目录、把硬盘挂载到数据目录、把 OSD 加入集群、然后把它加入 CRUSH Map<br><b>备注</b>:Ceph 喜欢统一的硬件,与存储池无关。如果你要新增容量不一的硬盘驱动器,还需调整它们的权重。但是,为实现最佳性能，CRUSH 的分级结构最好按类型、容量来组织</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">[root@node4 ~]# parted -s /dev/sdb mklabel gpt</div><div class="line">#删除所有分区</div><div class="line">[root@node4 ~]# sgdisk --zap-all --clear --mbrtogpt /dev/sdb</div><div class="line">GPT data structures destroyed! You may now partition the disk using fdisk or</div><div class="line">other utilities.</div><div class="line">The operation has completed successfully.</div><div class="line">#打印硬盘信息</div><div class="line">[root@node4 ~]# sgdisk -p /dev/sdb   </div><div class="line">[root@node4 ~]# ceph-disk prepare --cluster ceph --fs-type xfs /dev/sdb</div><div class="line">[root@node4 ~]# ceph-disk activate /dev/sdb</div><div class="line">备注:</div><div class="line">ceph-disk prepare --cluster  --cluster-uuid  --fs-type xfs|ext4|btrfs /device </div><div class="line">cluster-uuid ( b71a3eb1-e253-410a-bf11-84ae01bad654 )</div><div class="line">cluster name – default name is ceph unless specified otherwise when ran ceph-deploy</div><div class="line">eg ceph-deploy –cluster=cluster_name</div></pre></td></tr></table></figure>
<p>到这里我们就添加完了osd. 感兴趣的同学可以往下看,这里ceph-disk到底帮我们做了什么操作呢?</p>
<p>1.创建 OSD。如果未指定 UUID,OSD 启动时会自动生成一个。下列命令会输出 OSD 号,后续步骤你会用到<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">ceph osd create [&#123;uuid&#125; [&#123;id&#125;]]</div><div class="line"></div><div class="line">[root@node4 ~]# ceph osd create           </div><div class="line">9</div></pre></td></tr></table></figure></p>
<p>如果指定了可选参数 {id} ，那么它将作为 OSD id 。要注意，如果此数字已使用，此命令会出错。<br><b>建议</b>：一般来说,我们不建议指定 {id} 。因为 ID 是按照数组分配的,跳过一些依然会浪费内存；<br>尤其是跳过太多、或者集群很大时，会更明显。若未指定 {id} ,将用最小可用数字。<br>2.在新 OSD 主机上创建数据目录<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">ssh &#123;new-osd-host&#125;</div><div class="line">sudo mkdir /var/lib/ceph/osd/ceph-&#123;osd-number&#125;</div><div class="line"></div><div class="line">[root@node4 ~]# mkdir /var/lib/ceph/osd/ceph-9</div><div class="line">[root@node4 ~]# chown ceph:ceph -R /var/lib/ceph/osd/ceph-9</div></pre></td></tr></table></figure></p>
<p>3.建立分区,可以参考<a href="http://ceph.com/geen-categorie/creating-a-ceph-osd-from-a-designated-disk-partition/" target="_blank" rel="external">这里</a>,关于sgdisk的用法参考<a href="http://hustcat.github.io/sgdisk-basic" target="_blank" rel="external">这里</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line">[root@node4 ~]# sgdisk -n 1:10487808:4194301951 -t 1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -p /dev/sdb</div><div class="line">Disk /dev/sdb: 4194304000 sectors, 2.0 TiB</div><div class="line">Logical sector size: 512 bytes</div><div class="line">Disk identifier (GUID): A996FDA1-5621-45CD-871A-028E30E33027</div><div class="line">Partition table holds up to 128 entries</div><div class="line">First usable sector is 34, last usable sector is 4194303966</div><div class="line">Partitions will be aligned on 2048-sector boundaries</div><div class="line">Total free space is 10489789 sectors (5.0 GiB)</div><div class="line"></div><div class="line">Number  Start (sector)    End (sector)  Size       Code  Name</div><div class="line">   1        10487808      4194301951   1.9 TiB     FFFF  </div><div class="line">The operation has completed successfully.</div><div class="line"></div><div class="line">[root@node4 ~]# sgdisk -n 2:2048:10487807 -t 2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 -p /dev/sdb</div><div class="line">Disk /dev/sdb: 4194304000 sectors, 2.0 TiB</div><div class="line">Logical sector size: 512 bytes</div><div class="line">Disk identifier (GUID): A996FDA1-5621-45CD-871A-028E30E33027</div><div class="line">Partition table holds up to 128 entries</div><div class="line">First usable sector is 34, last usable sector is 4194303966</div><div class="line">Partitions will be aligned on 2048-sector boundaries</div><div class="line">Total free space is 4029 sectors (2.0 MiB)</div><div class="line"></div><div class="line">Number  Start (sector)    End (sector)  Size       Code  Name</div><div class="line">   1        10487808      4194301951   1.9 TiB     FFFF  </div><div class="line">   2            2048        10487807   5.0 GiB     FFFF  </div><div class="line">The operation has completed successfully.</div></pre></td></tr></table></figure></p>
<p>4.给数据盘和日志盘做标记,typecode参考<a href="https://github.com/ceph/ceph/blob/v0.67.4/src/ceph-disk#L65" target="_blank" rel="external">这里</a>,关于磁盘自动挂载的请参考<a href="http://www.zphj1987.com/2016/12/22/Ceph%E6%95%B0%E6%8D%AE%E7%9B%98%E6%80%8E%E6%A0%B7%E5%AE%9E%E7%8E%B0%E8%87%AA%E5%8A%A8%E6%8C%82%E8%BD%BD/" target="_blank" rel="external">这里</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">sgdisk 例子</div><div class="line">#分5120M大小</div><div class="line">/usr/sbin/sgdisk  --new=2:0:5120M --change-name=2:ceph-journal  --partition-guid=2:150f0081-c630-44c9-ad21-7d95613866ea  --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106    --mbrtogpt -- /dev/sdb</div><div class="line">#largest-new 将剩余block全部使用</div><div class="line">/usr/sbin/sgdisk --largest-new=1 --change-name=1:ceph-data --partition-guid=1:db182a1d-f8c6-4660-9c12-0222e2459dd5 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sdb</div><div class="line">#查看udev事件队列，如果所有的events已处理则退出</div><div class="line">/sbin/udevadm settle</div><div class="line"></div><div class="line">[root@node4 ~]# /usr/sbin/sgdisk  --change-name=2:&apos;ceph journal&apos; --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106  -- /dev/sdb</div><div class="line">The operation has completed successfully.</div><div class="line"></div><div class="line">[root@node4 ~]# /usr/sbin/sgdisk  --change-name=1:&apos;ceph data&apos; --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdb</div><div class="line">Warning: The kernel is still using the old partition table.</div><div class="line">The new table will be used at the next reboot.</div><div class="line">The operation has completed successfully.</div><div class="line"></div><div class="line">[root@node4 ceph-9]# mkfs.xfs /dev/sdb1</div><div class="line">[root@node4 ceph-9]# mount /dev/sdb1 /var/lib/ceph/osd/ceph-9</div></pre></td></tr></table></figure></p>
<p>5.初始化 OSD 数据目录<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">ssh &#123;new-osd-host&#125;</div><div class="line">ceph-osd -i &#123;osd-num&#125; --mkfs --mkkey</div><div class="line"></div><div class="line">[root@node4 ~]# ceph-osd -i 9 --mkfs --mkkey </div><div class="line">2017-06-29 16:23:05.161478 7f614226e800 -1 journal FileJournal::_open: disabling aio for non-block journal.  Use journal_force_aio to force use of aio anyway</div><div class="line">2017-06-29 16:23:05.200065 7f614226e800 -1 journal FileJournal::_open: disabling aio for non-block journal.  Use journal_force_aio to force use of aio anyway</div><div class="line">2017-06-29 16:23:05.221105 7f614226e800 -1 filestore(/var/lib/ceph/osd/ceph-9) could not find #-1:7b3f43c4:::osd_superblock:0# in index: (2) No such file or directory</div><div class="line">2017-06-29 16:23:05.237813 7f614226e800 -1 created object store /var/lib/ceph/osd/ceph-9 for osd.9 fsid d6d92de4-2a08-4bd6-a749-6c104c88fc40</div><div class="line">2017-06-29 16:23:05.237987 7f614226e800 -1 auth: error reading file: /var/lib/ceph/osd/ceph-9/keyring: can&apos;t open /var/lib/ceph/osd/ceph-9/keyring: (2) No such file or directory</div><div class="line">2017-06-29 16:23:05.238397 7f614226e800 -1 created new key in keyring /var/lib/ceph/osd/ceph-9/keyring</div><div class="line"></div><div class="line">[root@node4 ceph-9]# cd /var/lib/ceph/osd/ceph-9</div><div class="line">[root@node4 ceph-9]# rm -f journal</div><div class="line">[root@node4 ceph-9]# ll /dev/disk/by-partuuid/</div><div class="line">total 0</div><div class="line">lrwxrwxrwx 1 root root 10 Jun 30 11:35 c50d9217-c928-49a3-be1b-55990facf2e0 -&gt; ../../sdb2</div><div class="line">lrwxrwxrwx 1 root root 10 Jun 30 11:36 ced5d1bb-568c-4317-954f-efc63fa3bcaa -&gt; ../../sdb1</div><div class="line">[root@node4 ceph-9]# ln -s /dev/disk/by-partuuid/c50d9217-c928-49a3-be1b-55990facf2e0 journal</div><div class="line">[root@node4 ceph-9]# chown ceph:ceph -R /var/lib/ceph/osd/ceph-9</div><div class="line">[root@node4 ceph-9]# chown ceph:ceph /var/lib/ceph/osd/ceph-9/journal</div></pre></td></tr></table></figure></p>
<p>在启动 ceph-osd 前，数据目录必须是空的<br>6.注册 OSD 认证密钥,ceph-{osd-num} 路径里的 ceph 值应该是 $cluster-$id,如果你的集群名字不是 ceph,那就用自己集群的名字<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">ceph auth add osd.&#123;osd-num&#125; osd &apos;allow *&apos; mon &apos;allow rwx&apos; -i /var/lib/ceph/osd/ceph-&#123;osd-num&#125;/keyring</div><div class="line"></div><div class="line">[root@node4 ~]# ceph auth add osd.9 osd &apos;allow *&apos; mon &apos;allow profile osd&apos; -i /var/lib/ceph/osd/ceph-9/keyring</div><div class="line">added key for osd.9</div><div class="line"></div><div class="line">#创建client.bootstrap-osd key文件(本节点新建第一个osd时才需要)</div><div class="line">[root@node4 ~]# ceph auth get-or-create client.bootstrap-osd -o /var/lib/ceph/bootstrap-osd/ceph.keyring</div><div class="line">[root@node4 ~]# chmod 600 /var/lib/ceph/bootstrap-osd/ceph.keyring</div><div class="line">[root@node4 ~]# chown ceph:ceph /var/lib/ceph/bootstrap-osd/ceph.keyring</div></pre></td></tr></table></figure></p>
<p>7.把新 OSD 加入 CRUSH Map 中,以便它可以开始接收数据。用 ceph osd crush add 命令把 OSD 加入 CRUSH 分级结构的合适位置。<br>如果你指定了不止一个 bucket，此命令会把它加入你所指定的 bucket 中最具体的一个，并且把此 bucket 挪到你指定的其它 bucket 之内<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">ceph osd crush add &#123;id-or-name&#125; &#123;weight&#125; [&#123;bucket-type&#125;=&#123;bucket-name&#125; ...]</div><div class="line"></div><div class="line">[root@node4 ~]# ceph osd crush add-bucket node4 host</div><div class="line">added bucket node4 type host to crush map</div><div class="line">[root@node4 ~]# ceph osd crush move node4 root=default</div><div class="line">moved item id -5 name &apos;node4&apos; to location &#123;root=default&#125; in crush map</div><div class="line">[root@node4 ~]# ceph osd crush add osd.9 1.0 host=node4</div><div class="line">add item id 9 name &apos;osd.9&apos; weight 1 at location &#123;host=node4&#125; to crush map</div></pre></td></tr></table></figure></p>
<p>你也可以反编译 CRUSH Map、把 OSD 加入设备列表、以 bucket 的形式加入主机（如果它没在 CRUSH Map 里）、以条目形式把设备加入主机、分配权重、重编译并应用它<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">[root@node4 ~]# ceph osd getcrushmap -o crushmap.txt</div><div class="line">[root@node4 ~]# crushtool -d crushmap.txt -o crushmap-decompile</div><div class="line">[root@node4 ~]# vim crushmap-decompile</div><div class="line">删除掉node4相关的信息</div><div class="line">[root@node4 ~]# crushtool -c crushmap-decompile  -o crushmap-compile</div><div class="line">[root@node4 ~]# ceph osd setcrushmap -i crushmap-compile </div><div class="line">set crush map</div></pre></td></tr></table></figure></p>
<p>8.启动 OSD。把 OSD 加入 Ceph 后， OSD 就在配置里了。然而它还没运行，它现在的状态为 down &amp; out 。你必须先启动 OSD 它才能收数据<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">systemctl start ceph-osd@&#123;osd-num&#125;</div><div class="line"></div><div class="line">[root@node4 ~]# systemctl start ceph-osd@9</div></pre></td></tr></table></figure></p>
<p>启动了 OSD ，其状态就变成了 up &amp; in<br>遇到的问题:<br>[root@node4 ~]# systemctl start ceph-osd@9 启动不成功,报错如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">[root@node4 ~]# /usr/bin/ceph-osd -f  --cluster ceph --id 9 --setuser ceph --setgroup ceph</div><div class="line">starting osd.9 at :/0 osd_data /var/lib/ceph/osd/ceph-9 /var/lib/ceph/osd/ceph-9/journal</div><div class="line">2017-06-30 11:49:18.599998 7f3f36afb800 -1 journal FileJournal::open: ondisk fsid 0c8bb770-f16a-4208-91d2-7e659768fbc8 doesn&apos;t match expected 03bba8cd-3765-4364-b727-e4f9269447cc, invalid (someone else&apos;s?) journal</div></pre></td></tr></table></figure></p>
<p>解决方法:Create new journal<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">$ ceph-osd --mkjournal -i &lt;osd num&gt;</div><div class="line"></div><div class="line">[root@node4 osd]# ceph-osd --mkjournal -i 9</div><div class="line">SG_IO: bad/missing sense data, sb[]:  70 00 05 00 00 00 00 0a 00 00 00 00 20 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00</div><div class="line">2017-06-30 11:59:44.328209 7fc180861800 -1 journal check: ondisk fsid 0c8bb770-f16a-4208-91d2-7e659768fbc8 doesn&apos;t match expected 03bba8cd-3765-4364-b727-e4f9269447cc, invalid (someone else&apos;s?) journal</div><div class="line">SG_IO: bad/missing sense data, sb[]:  70 00 05 00 00 00 00 0a 00 00 00 00 20 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00</div><div class="line">2017-06-30 11:59:44.338081 7fc180861800 -1 created new journal /var/lib/ceph/osd/ceph-9/journal for object store /var/lib/ceph/osd/ceph-9</div><div class="line"></div><div class="line">#启动成功</div><div class="line">systemctl start ceph-osd@9</div></pre></td></tr></table></figure></p>
<h5 id="增加-OSD-ceph-deploy"><a href="#增加-OSD-ceph-deploy" class="headerlink" title="增加 OSD(ceph-deploy)"></a><b>增加 OSD(ceph-deploy)</b></h5><p>1.登入 ceph-deploy 工具所在的 Ceph admin 节点，进入工作目录<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">ssh &#123;ceph-deploy-node&#125;</div><div class="line">cd /path/ceph-deploy-work-path</div><div class="line"></div><div class="line">[neo@admin ~]$ cd cluster/</div></pre></td></tr></table></figure></p>
<p>2.列举磁盘。<br>执行下列命令列举一节点上的磁盘：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">ceph-deploy disk list &#123;node-name [node-name]...&#125;</div><div class="line"></div><div class="line">[neo@admin cluster]$  ceph-deploy disk list node4</div></pre></td></tr></table></figure></p>
<p>3.格式化磁盘。<br>用下列命令格式化（删除分区表）磁盘，以用于 Ceph :<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">ceph-deploy disk zap &#123;osd-server-name&#125;:&#123;disk-name&#125;</div><div class="line"></div><div class="line">[neo@admin cluster]$  ceph-deploy disk zap node4:sdb</div></pre></td></tr></table></figure></p>
<p><b>重要</b>： 这会删除磁盘上的所有数据<br>4.准备 OSD<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">ceph-deploy osd prepare &#123;node-name&#125;:&#123;data-disk&#125;[:&#123;journal-disk&#125;]</div><div class="line"></div><div class="line">[neo@admin cluster]$ ceph-deploy osd prepare node4:sdb</div></pre></td></tr></table></figure></p>
<p>prepare 命令只准备 OSD 。在大多数操作系统中，硬盘分区创建后，不用 activate 命令也会自动执行 activate 阶段（通过 Ceph 的 udev 规则）<br>前例假定一个硬盘只会用于一个 OSD 守护进程，以及一个到 SSD 日志分区的路径。<br>我们建议把日志存储于另外的驱动器以最优化性能；你也可以指定一单独的驱动器用于日志（也许比较昂贵）、或者把日志放到 OSD 数据盘（不建议，因为它有损性能,这里放在一起了）。<br><b>注意</b>： 在一个节点运行多个 OSD 守护进程、且多个 OSD 守护进程共享一个日志分区时，你应该考虑整个节点的最小 CRUSH 故障域，<br>因为如果这个 SSD 坏了，所有用其做日志的 OSD 守护进程也会失效<br>5.准备好 OSD 后，可以用下列命令激活它<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">ceph-deploy osd activate &#123;node-name&#125;:&#123;data-disk-partition&#125;[:&#123;journal-disk-partition&#125;]</div><div class="line">[neo@admin cluster]$  ceph-deploy osd activate node4:/dev/sdb1</div></pre></td></tr></table></figure></p>
<p>activate 命令会让 OSD 进入 up 且 in 状态。该命令使用的分区路径是前面 prepare 命令创建的</p>
<h5 id="删除-OSD-手动"><a href="#删除-OSD-手动" class="headerlink" title="删除 OSD(手动)"></a><b>删除 OSD(手动)</b></h5><p>在 Ceph 里，一个 OSD 通常是一台主机上的一个 ceph-osd 守护进程、它运行在一个硬盘之上。<br>如果一台主机上有多个数据盘，你得逐个删除其对应 ceph-osd。<br>通常，操作前应该检查集群容量，看是否快达到上限了，确保删除 OSD 后不会使集群达到 near full 比率<br><b>警告</b>：删除 OSD 时不要让集群达到 full ratio 值，删除 OSD 可能导致集群达到或超过 full ratio 值。</p>
<p>1.调整osd的crush weight<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">ssh &#123;osd-host&#125;</div><div class="line">[root@node4 ~]# ceph osd crush reweight osd.10  0.1</div></pre></td></tr></table></figure></p>
<p>备注：可以分几次调整将crush 的weight 减低到0 ，目的就是让数据慢慢的分布到其他节点上，直到完全迁移到其他osd,<br>这个地方不光调整了osd 的crush weight ，实际上同时调整了host 的 weight ，这样会调整集群的整体的crush 分布，在osd 的crush 为0 后， 再对这个osd的任何删除相关操作都不会影响到集群的数据的分布</p>
<p>2.停止需要剔除的 OSD 进程，让其他的 OSD 知道这个 OSD 不提供服务了。停止 OSD 后，状态变为 down<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">sudo systemctl stop  ceph-osd@id=&#123;osd-num&#125;</div><div class="line"></div><div class="line">[root@node4 ~]# systemctl stop ceph-osd@10</div></pre></td></tr></table></figure></p>
<p>3.将 OSD 标记为 out 状态，这个一步是告诉 mon，这个 OSD 已经不能服务了，需要在其他的 OSD 上进行数据的均衡和恢复了<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">ceph osd out &#123;osd-num&#125;</div><div class="line"></div><div class="line">[root@node4 ~]# ceph osd out 10</div><div class="line">marked out osd.10.</div></pre></td></tr></table></figure></p>
<p>执行完这一步后，会触发数据的恢复过程。此时应该等待数据恢复结束，集群恢复到 HEALTH_OK 状态，再进行下一步操作</p>
<p>4.删除 CRUSH Map 中的对应 OSD 条目，它就不再接收数据了。你也可以反编译 CRUSH Map、删除 device 列表条目、删除对应的 host 桶条目或删除 host 桶（如果它在 CRUSH Map 里，而且你想删除主机），重编译 CRUSH Map 并应用它<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">ceph osd crush remove &#123;name&#125;</div><div class="line"></div><div class="line">[root@node4 ~]# ceph osd crush remove osd.10</div><div class="line">removed item id 10 name &apos;osd.10&apos; from crush map</div></pre></td></tr></table></figure></p>
<p>该步骤会触发数据的重新分布。等待数据重新分布结束，整个集群会恢复到 HEALTH_OK 状态<br>5.删除 OSD 认证密钥：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">ceph auth del osd.&#123;osd-num&#125;</div><div class="line"></div><div class="line">[root@node4 ~]# ceph auth del osd.10</div><div class="line">updated</div></pre></td></tr></table></figure></p>
<p>6.删除 OSD<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">ceph osd rm &#123;osd-num&#125;</div><div class="line"></div><div class="line">[root@node4 ~]# ceph osd rm 10</div><div class="line">removed osd.10</div></pre></td></tr></table></figure></p>
<p>7.卸载 OSD 的挂载点.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">sudo umount /var/lib/ceph/osd/$cluster-&#123;osd-num&#125;</div><div class="line"></div><div class="line">[root@node4 ~]# umount /var/lib/ceph/osd/ceph-10</div></pre></td></tr></table></figure></p>
<p>8.登录到保存 ceph.conf 主拷贝的主机<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">ssh &#123;admin-host&#125;</div><div class="line">cd /etc/ceph</div><div class="line">vim ceph.conf</div></pre></td></tr></table></figure></p>
<p>9.从 ceph.conf 配置文件里删除对应条目<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">[osd.10]</div><div class="line">        host = &#123;hostname&#125;</div></pre></td></tr></table></figure></p>
<p>10.从保存 ceph.conf 主拷贝的主机，把更新过的 ceph.conf 拷贝到集群其他主机的 /etc/ceph 目录下<br>如果在 ceph.conf 中没有定义各 OSD 入口，就不必执行第 8 ~ 10 步</p>
<h5 id="删除-OSD-ceph-deploy"><a href="#删除-OSD-ceph-deploy" class="headerlink" title="删除 OSD(ceph-deploy)"></a><b>删除 OSD(ceph-deploy)</b></h5><p>ref<br><a href="https://lihaijing.gitbooks.io/ceph-handbook/content/Operation/add_rm_osd.html" target="_blank" rel="external">增加/删除 OSD</a><br><a href="https://xiaoquqi.github.io/blog/2015/05/12/ceph-osd-is-full/" target="_blank" rel="external">Ceph集群磁盘没有剩余空间的解决方法</a><br><a href="https://github.com/chenzhongtao/work_summary/blob/master/ceph_doc/Ceph%E5%A6%82%E4%BD%95%E6%B7%BB%E5%8A%A0ssd%E7%A3%81%E7%9B%98%E5%81%9Acache%20tier.txt" target="_blank" rel="external">Ceph如何添加ssd磁盘做cache tier.txt</a><br><a href="http://ceph.com/planet/%E8%AE%B0%E6%9C%80%E8%BF%91%E4%B8%80%E6%AC%A1ceph%E6%95%85%E9%9A%9C%E4%BF%AE%E5%A4%8D/" target="_blank" rel="external">记最近一次ceph故障修复</a><br><a href="https://forest.gitbooks.io/ceph-practice/content/troubleshoot.html" target="_blank" rel="external">故障定位和处理</a><br><a href="http://www.isjian.com/ceph/ceph-cluster-add-or-remove-osd-manual" target="_blank" rel="external">ceph集群中进行osd的手动添加移除</a><br><a href="http://ceph.com/geen-categorie/creating-a-ceph-osd-from-a-designated-disk-partition/" target="_blank" rel="external">Creating a Ceph OSD from a designated disk partition</a><br><a href="http://www.zphj1987.com/2015/11/12/%E5%A6%82%E4%BD%95%E5%88%A0%E9%99%A4%E4%B8%80%E5%8F%B0OSD%E4%B8%BB%E6%9C%BA/" target="_blank" rel="external">如何删除一台OSD主机</a><br><a href="https://ekuric.wordpress.com/2016/01/11/addremove-ceph-osd-object-storage-device/" target="_blank" rel="external">add/remove CEPH OSD – Object Storage Device</a><br><a href="https://www.xncoding.com/2017/03/14/ceph/disk-partition.html" target="_blank" rel="external">Linux磁盘分区总结</a><br><a href="http://bbs.ceph.org.cn/article/36" target="_blank" rel="external">Ceph：SSD日志故障后的OSD恢复</a><br><a href="https://lihaijing.gitbooks.io/ceph-handbook/content/Advance_usage/change_osd_journal.html" target="_blank" rel="external">更换 OSD Journal</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;增加-OSD-手动&quot;&gt;&lt;a href=&quot;#增加-OSD-手动&quot; class=&quot;headerlink&quot; title=&quot;增加 OSD(手动)&quot;&gt;&lt;/a&gt;&lt;b&gt;增加 OSD(手动)&lt;/b&gt;&lt;/h5&gt;&lt;p&gt;首先修改各个节点的/etc/hosts信息,增加新节点信息,并添加
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>How to change MON IP</title>
    <link href="https://t1ger.github.io/2017/06/21/How-to-change-MON-IP/"/>
    <id>https://t1ger.github.io/2017/06/21/How-to-change-MON-IP/</id>
    <published>2017-06-21T09:12:32.000Z</published>
    <updated>2017-06-23T09:47:53.094Z</updated>
    
    <content type="html"><![CDATA[<p>Ceph 客户端和其他 Ceph 守护进程通过 ceph.conf 来发现 monitor。但是 monitor 之间是通过 mon map 而非 ceph.conf 来发现彼此</p>
<h5 id="修改-MON-IP-推荐-方法一"><a href="#修改-MON-IP-推荐-方法一" class="headerlink" title="修改 MON IP (推荐)方法一 "></a><b>修改 MON IP (推荐)方法一 </b></h5><p>仅修改 ceoh.conf 中 mon 的 IP 是不足以确保集群中的其他 monitor 收到更新的。<br>要修改一个 mon 的 IP，你必须先新增一个使用新 IP 的 monitor，确保这个新 mon 成功加入集群并形成法定人数。<br>然后，删除使用旧 IP 的 mon。最后，更新 ceph.conf ，以便客户端和其他守护进程可以知道新 mon 的 IP。<br>比如，假设现有 3 个 monitors：<br>[mon.node1]<br>        host = node1<br>        addr = 192.168.138.141:6789<br>[mon.node2]<br>        host = node2<br>        addr = 192.168.138.142:6789<br>[mon.node3]<br>        host = node3<br>        addr = 192.168.138.143:6789</p>
<p>把 mon.node3 变更为 mon.node4 。增加一个 mon.node4 ，host 设为 node4，IP 地址设为 192.168.138.144。先启动 mon.node4 ，再 删除 mon.node3 ，否则会破坏法定人数。</p>
<h5 id="修改-MON-IP-方法二"><a href="#修改-MON-IP-方法二" class="headerlink" title="修改 MON IP 方法二 "></a><b>修改 MON IP 方法二 </b></h5><p>有时，monitor 需要迁移到一个新的网络中、数据中心的其他位置或另一个数据中心。这时，需要为集群中所有的 monitors 生成一个新的 mon map （指定了新的 MON IP），再注入每一个 monitor 中</p>
<p>还以前面的 mon 配置为例。假定想把 monitor 从 192.168.138.x 网段改为 192.168.139.x 网段，这两个网段直接是不通的。执行下列步骤：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div></pre></td><td class="code"><pre><div class="line">1、获取 mon map</div><div class="line">ceph mon getmap -o &#123;tmp&#125;/&#123;filename&#125;</div><div class="line">2、下面的例子说明了 monmap 的内容。</div><div class="line">monmaptool --print &#123;tmp&#125;/&#123;filename&#125;</div><div class="line">3、删除已有的 monitors</div><div class="line">monmaptool --rm a --rm b --rm c &#123;tmp&#125;/&#123;filename&#125;</div><div class="line"></div><div class="line">monmaptool: monmap file &#123;tmp&#125;/&#123;filename&#125;</div><div class="line">monmaptool: removing a</div><div class="line">monmaptool: removing b</div><div class="line">monmaptool: removing c</div><div class="line">monmaptool: writing epoch 1 to &#123;tmp&#125;/&#123;filename&#125; (0 monitors)</div><div class="line">4、新增 monitor</div><div class="line">monmaptool --add node1 192.168.139.141:6789 --add node2 192.168.139.142:6789 --add node3 192.168.139.143:6789 &#123;tmp&#125;/&#123;filename&#125;</div><div class="line"></div><div class="line">monmaptool: monmap file &#123;tmp&#125;/&#123;filename&#125;</div><div class="line">monmaptool: writing epoch 1 to &#123;tmp&#125;/&#123;filename&#125; (3 monitors)</div><div class="line">5、检查 monmap 的新内容</div><div class="line">$ monmaptool --print &#123;tmp&#125;/&#123;filename&#125;</div><div class="line">monmaptool: monmap file &#123;tmp&#125;/&#123;filename&#125;</div><div class="line"></div><div class="line">此时，我们假定 monitor 已在新位置安装完毕。下面的步骤就是分发新的 monmap 并注入到各新 monitor 中。</div><div class="line">1、停止所有的 monitor 。必须停止 mon 守护进程才能进行 monmap 注入。</div><div class="line">2、注入 monmap。</div><div class="line">ceph-mon -i &#123;mon-id&#125; --inject-monmap &#123;tmp&#125;/&#123;filename&#125;</div><div class="line">3、重启各 monitors </div><div class="line"></div><div class="line"></div><div class="line">[root@admin ~]# ceph mon getmap -o /tmp/map</div><div class="line">got monmap epoch 5</div><div class="line">[root@admin ~]#  monmaptool --print /tmp/map </div><div class="line">monmaptool: monmap file /tmp/map</div><div class="line">epoch 5</div><div class="line">fsid d6d92de4-2a08-4bd6-a749-6c104c88fc40</div><div class="line">last_changed 2017-06-21 12:59:00.947896</div><div class="line">created 2017-05-27 11:43:56.428001</div><div class="line">0: 192.168.138.141:6789/0 mon.node1</div><div class="line">1: 192.168.138.142:6789/0 mon.node2</div><div class="line">2: 192.168.138.143:6789/0 mon.node3</div><div class="line">[root@admin ~]# monmaptool --rm node1 --rm node2 --rm node3 /tmp/map </div><div class="line">monmaptool: removing node1</div><div class="line">monmaptool: removing node2</div><div class="line">monmaptool: removing node3</div><div class="line">monmaptool: writing epoch 5 to tmp/map (0 monitors)</div><div class="line">[root@admin ~]# monmaptool --add node1 192.168.139.141:6789 --add node2 192.168.139.142:6789 --add node3 192.168.139.143:6789 /tmp/map</div><div class="line">monmaptool: monmap file /tmp/map</div><div class="line">monmaptool: writing epoch 5 to /tmp/map (3 monitors)</div><div class="line"></div><div class="line">[root@admin ~]#  monmaptool --print /tmp/map </div><div class="line">epoch 5</div><div class="line">fsid 224e376d-c5fe-4504-96bb-ea6332a19e61</div><div class="line">last_changed 2017-06-21 12:59:00.947896</div><div class="line">created 2017-05-27 11:43:56.428001</div><div class="line">0: 192.168.139.141:6789/0 mon.node1</div><div class="line">1: 192.168.139.142:6789/0 mon.node2</div><div class="line">2: 192.168.139.143:6789/0 mon.node3</div><div class="line"></div><div class="line"></div><div class="line">systemctl stop ceph-mon@node1</div><div class="line">systemctl stop ceph-mon@node2</div><div class="line">systemctl stop ceph-mon@node3</div><div class="line">ceph-mon -i node1 --inject-monmap tmp/map</div><div class="line">ceph-mon -i node2 --inject-monmap tmp/map</div><div class="line">ceph-mon -i node3 --inject-monmap tmp/map</div><div class="line">systemctl restart ceph-mon@node1</div><div class="line">systemctl restart ceph-mon@node2</div><div class="line">systemctl restart ceph-mon@node3</div></pre></td></tr></table></figure></p>
<h5 id="修改-MON-IP-方法三"><a href="#修改-MON-IP-方法三" class="headerlink" title="修改 MON IP 方法三 "></a><b>修改 MON IP 方法三 </b></h5><p>假如我们在机房迁移时候没有导出monmap,我们如何修改ip呢<br>这里我放弃使用导出monmap的方法而选择新建monmap，因为新建可以解决这里MON修改IP后无法启动提取monmap的问题。</p>
<p>假定，MON的IP从192.168.56.x 迁到了172.16.56.x ，我们首先创建一个使用新的IP的monmap，这里还是使用了三个MON：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div></pre></td><td class="code"><pre><div class="line">[root@node1 ~]# cat /etc/ceph/ceph.conf |grep fsid</div><div class="line">fsid = d6d92de4-2a08-4bd6-a749-6c104c88fc40</div><div class="line">[root@node1 ~]# monmaptool -h</div><div class="line"> usage: [--print] [--create [--clobber][--fsid uuid]] [--generate] [--set-initial-members] [--add name 1.2.3.4:567] [--rm name] &lt;mapfilename&gt;</div><div class="line"> </div><div class="line">[root@node1 ~]# monmaptool --create --fsid d6d92de4-2a08-4bd6-a749-6c104c88fc40 --add node1 172.16.56.141 --add node2 172.16.56.142 --add  node3 172.16.56.143 /tmp/monmap</div><div class="line">monmaptool: monmap file /tmp/monmap</div><div class="line">monmaptool: set fsid to d6d92de4-2a08-4bd6-a749-6c104c88fc40</div><div class="line">monmaptool: writing epoch 0 to /tmp/monmap (3 monitors)</div><div class="line"></div><div class="line">[root@ceph-1 ~]# monmaptool --print /tmp/monmap </div><div class="line">monmaptool: monmap file /tmp/monmap</div><div class="line">epoch 0</div><div class="line">fsid d6d92de4-2a08-4bd6-a749-6c104c88fc40</div><div class="line">last_changed 2017-06-21 12:59:00.947896</div><div class="line">created 2017-05-27 11:43:56.428001</div><div class="line">0: 172.16.56.141:6789/0 mon.node1</div><div class="line">1: 172.16.56.142:6789/0 mon.node2</div><div class="line">2: 172.16.56.143:6789/0 mon.node3</div><div class="line"></div><div class="line">通过打印monmap可以看到已经成功添加了三个MON，只是这里的epoch为0，实际的epoch肯定大于0的，</div><div class="line">不用担心，monmaptool的代码里面写死了是0，并且不影响注入到MON的数据库里</div><div class="line"></div><div class="line">[root@node1 ~]# scp /tmp/monmap node2:/tmp/monmap</div><div class="line">[root@node1 ~]# scp /tmp/monmap node3:/tmp/monmap</div><div class="line">[root@node1 ~]# ceph-mon -i node1 --inject-monmap /tmp/monmap </div><div class="line">[root@node2 ~]# ceph-mon -i node2 --inject-monmap /tmp/monmap </div><div class="line">[root@node3 ~]# ceph-mon -i node3 --inject-monmap /tmp/monmap</div><div class="line"></div><div class="line">注意这里是在三个主机上分别注入的，最后修改配置文件，发放到各个节点，开启MON服务</div><div class="line"></div><div class="line">[root@node1 cluster]# vim ceph.conf </div><div class="line">[root@node1 cluster]# cat /root/cluster/ceph.conf |grep mon</div><div class="line">mon_initial_members = node1,node2,node3</div><div class="line">mon_host = 172.16.56.141,172.16.56.142,172.16.56.143</div><div class="line">[root@node1 cluster]# ceph-deploy --overwrite-conf config push node1 node2 node3</div><div class="line">[root@node1 ~]# systemctl start ceph.target</div><div class="line">[root@node2 ~]# systemctl start ceph.target</div><div class="line">[root@node3 ~]# systemctl start ceph.target</div><div class="line">[root@node1 cluster]# ceph -s</div><div class="line">    cluster d6d92de4-2a08-4bd6-a749-6c104c88fc40</div><div class="line">     health HEALTH_WARN</div><div class="line">            clock skew detected on mon.node2, mon.node3</div><div class="line">            Monitor clock skew detected </div><div class="line">     monmap e6: 3 mons at &#123;node1=172.16.56.141:6789/0,node2=172.16.56.142:6789/0,node3=172.16.56.143:6789/0&#125;</div><div class="line">            election epoch 6, quorum 0,1,2 node1,node2,node3</div><div class="line">     osdmap e30: 3 osds: 3 up, 3 in</div><div class="line">      pgmap v48: 64 pgs, 1 pools, 0 bytes data, 0 objects</div><div class="line">            101 MB used, 6125 GB / 6125 GB avail</div><div class="line">                  64 active+clean</div><div class="line"></div><div class="line">这样我们就完成了MON的IP迁移</div></pre></td></tr></table></figure>
<h5 id="修改-MON-IP-方法四"><a href="#修改-MON-IP-方法四" class="headerlink" title="修改 MON IP 方法四"></a><b>修改 MON IP 方法四</b></h5><p>如果三个MON的数据库都被损坏了,我们可以参考<a href="http://www.zphj1987.com/2016/09/20/Ceph%E7%9A%84Mon%E6%95%B0%E6%8D%AE%E9%87%8D%E6%96%B0%E6%9E%84%E5%BB%BA%E5%B7%A5%E5%85%B7/" target="_blank" rel="external">这里</a>重建MON<br>主要使用新工具ceph-monstore-tool来重建丢失的MON数据库。当然，如果每天备份一次MON数据库，就不用担心故障了</p>
<h5 id="Monitor的备份"><a href="#Monitor的备份" class="headerlink" title="Monitor的备份 "></a><b>Monitor的备份 </b></h5><p>备份,重要的事情强调三遍.简单讲基本思路就是，停止一个MON，然后将这个MON的数据库压缩保存到其他路径，再开启MON，文中提到了之所以要停止MON是要保证levelDB数据库的完整性.</p>
<p>当某个集群的所有的MON节点都挂掉之后，我们可以将最新的备份的数据库解压到任意一个节点上，用同样的方法新建monmap，注入，开启MON，推送config,重启OSD就好了</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">systemctl stop ceph-mon@node1</div><div class="line">tar czf /var/backups/ceph-mon-backup_$(date +&apos;%a&apos;).tar.gz /var/lib/ceph/mon</div><div class="line">systemctl start ceph-mon@node1</div><div class="line">#for safety, copy it to other nodes</div><div class="line">scp /var/backups/* someNode:/backup/</div></pre></td></tr></table></figure>
<p>ref</p>
<p><a href="http://www.xuxiaopang.com/2016/10/26/exp-monitor-operation/" target="_blank" rel="external">monitor的增删改备</a><br><a href="https://lihaijing.gitbooks.io/ceph-handbook/content/Operation/modify_mon_ip.html" target="_blank" rel="external">修改 MON IP</a><br><a href="https://github.com/angapov/ceph-systemd" target="_blank" rel="external">Systemd script for CEPH object storage</a></p>
<hr>
<p>您的鼓励是我写作最大的动力</p>
<p>俗话说，投资效率是最好的投资。 如果您感觉我的文章质量不错，读后收获很大，预计能为您提高 10% 的工作效率，不妨小额捐助我一下，让我有动力继续写出更多好文章。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Ceph 客户端和其他 Ceph 守护进程通过 ceph.conf 来发现 monitor。但是 monitor 之间是通过 mon map 而非 ceph.conf 来发现彼此&lt;/p&gt;
&lt;h5 id=&quot;修改-MON-IP-推荐-方法一&quot;&gt;&lt;a href=&quot;#修改-MON-
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>monitor的添加或删除</title>
    <link href="https://t1ger.github.io/2017/06/15/monitor%E7%9A%84%E6%B7%BB%E5%8A%A0%E6%88%96%E5%88%A0%E9%99%A4/"/>
    <id>https://t1ger.github.io/2017/06/15/monitor的添加或删除/</id>
    <published>2017-06-15T11:09:13.000Z</published>
    <updated>2017-06-21T07:07:49.788Z</updated>
    
    <content type="html"><![CDATA[<p>一般来说，在实际运行中，ceph monitor的个数是2n+1(n&gt;=0)个，在线上至少3个，只要正常的节点数&gt;=n+1，ceph的paxos算法能保证系统的正常运行。所以,对于3个节点，同时只能挂掉一个。建议（但不是强制）部署奇数个 monitor ,不建议把监视器和 OSD 置于同一主机上,后续如果要增加，请一次增加 2 个</p>
<p>一般来说，同时挂掉2个节点的概率比较小，但是万一挂掉2个呢？<br>如果ceph的monitor节点超过半数挂掉，paxos算法就无法正常进行仲裁(quorum)，此时，ceph集群会阻塞对集群的操作，直到超过半数的monitor节点恢复</p>
<ul>
<li>如果挂掉的2个节点至少有一个可以恢复，也就是monitor的监控数据还是OK的，那么只需要重启ceph-mon进程即可。所以，对于monitor，最好运行在RAID的机器上。这样，即使机器出现故障，恢复也比较容易</li>
<li>如果挂掉的2个节点的监控数据都损坏了呢？</li>
</ul>
<p>带着这些疑问,后续几篇文章我们来提供了一些常见场景的处理方法，包括增加monitor，移除某个monitor，机房搬迁需要修改IP，备份MON的数据库等</p>
<p>本文讲一下如何对一个已经存在的ceph storage cluster添加或删除一个监控节点.<br>用 ceph-deploy 增加和删除监视器很简单，只要一个命令就可以增加或删除一或多个监视器<br>大致步骤:<br>1.环境准备<br>2.安装软件<br>3.添加节点</p>
<h5 id="添加一个monitor-ceph-deploy"><a href="#添加一个monitor-ceph-deploy" class="headerlink" title="添加一个monitor(ceph-deploy )"></a><b>添加一个monitor(ceph-deploy )</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div></pre></td><td class="code"><pre><div class="line">[neo@admin cluster]$ sudo ceph -s</div><div class="line">    cluster d6d92de4-2a08-4bd6-a749-6c104c88fc40</div><div class="line">     health HEALTH_OK</div><div class="line">     monmap e2: 2 mons at &#123;node1=192.168.138.141:6789/0,node2=192.168.138.142:6789/0&#125;</div><div class="line">            election epoch 62, quorum 0,1 node1,node2</div><div class="line">     osdmap e246: 9 osds: 9 up, 9 in</div><div class="line">            flags sortbitwise,require_jewel_osds</div><div class="line">      pgmap v1167: 256 pgs, 1 pools, 4 bytes data, 1 objects</div><div class="line">            350 MB used, 18377 GB / 18378 GB avail</div><div class="line">                 256 active+clean</div><div class="line">[neo@admin cluster]$ cat ~/cluster/ceph.conf |grep mon     </div><div class="line">mon_initial_members = node1, node2</div><div class="line">mon_host = 192.168.138.141,192.168.138.142</div><div class="line"></div><div class="line">#修改配置文件,添加新的节点</div><div class="line">[neo@admin cluster]$ vi ceph.conf </div><div class="line">[neo@admin cluster]$ cat ~/cluster/ceph.conf |grep mon</div><div class="line">mon_initial_members = node1, node2, node3</div><div class="line">mon_host = 192.168.138.141,192.168.138.142,192.168.138.143</div><div class="line"></div><div class="line">[neo@admin cluster]$ ceph-deploy --overwrite-conf config push  node1 node2</div><div class="line"></div><div class="line">#添加MON，注意如果如果要添加多个MON，需要一个个add</div><div class="line">需要注意,往存在的cluster里添加monitor时，需要修改配置文件ceph.conf在global章节中</div><div class="line">指定public network或者mon.nodeX中指定public addr，配置文件中写成代</div><div class="line">下划线的public_network =也是可以的</div><div class="line"></div><div class="line">[neo@admin cluster]$ ceph-deploy --overwrite-conf mon add node3</div><div class="line">[ceph_deploy.conf][DEBUG ] found configuration file at: /home/neo/.cephdeploy.conf</div><div class="line">[ceph_deploy.cli][INFO  ] Invoked (1.5.37): /usr/bin/ceph-deploy --overwrite-conf mon add node3</div><div class="line">[ceph_deploy.cli][INFO  ] ceph-deploy options:</div><div class="line">[ceph_deploy.cli][INFO  ]  username                      : None</div><div class="line">[ceph_deploy.cli][INFO  ]  verbose                       : False</div><div class="line">[ceph_deploy.cli][INFO  ]  overwrite_conf                : True</div><div class="line">[ceph_deploy.cli][INFO  ]  subcommand                    : add</div><div class="line">[ceph_deploy.cli][INFO  ]  quiet                         : False</div><div class="line">[ceph_deploy.cli][INFO  ]  cd_conf                       : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x164fc68&gt;</div><div class="line">[ceph_deploy.cli][INFO  ]  cluster                       : ceph</div><div class="line">[ceph_deploy.cli][INFO  ]  mon                           : [&apos;node3&apos;]</div><div class="line">[ceph_deploy.cli][INFO  ]  func                          : &lt;function mon at 0x1647320&gt;</div><div class="line">[ceph_deploy.cli][INFO  ]  address                       : None</div><div class="line">[ceph_deploy.cli][INFO  ]  ceph_conf                     : None</div><div class="line">[ceph_deploy.cli][INFO  ]  default_release               : False</div><div class="line">[ceph_deploy.mon][INFO  ] ensuring configuration of new mon host: node3</div><div class="line">[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to node3</div><div class="line">[node3][DEBUG ] connection detected need for sudo</div><div class="line">[node3][DEBUG ] connected to host: node3 </div><div class="line">[node3][DEBUG ] detect platform information from remote host</div><div class="line">[node3][DEBUG ] detect machine type</div><div class="line">[node3][DEBUG ] write cluster configuration to /etc/ceph/&#123;cluster&#125;.conf</div><div class="line">[ceph_deploy.mon][DEBUG ] Adding mon to cluster ceph, host node3</div><div class="line">[ceph_deploy.mon][DEBUG ] using mon address by resolving host: 192.168.138.143</div><div class="line">[ceph_deploy.mon][DEBUG ] detecting platform for host node3 ...</div><div class="line">[node3][DEBUG ] connection detected need for sudo</div><div class="line">[node3][DEBUG ] connected to host: node3 </div><div class="line">[node3][DEBUG ] detect platform information from remote host</div><div class="line">[node3][DEBUG ] detect machine type</div><div class="line">[node3][DEBUG ] find the location of an executable</div><div class="line">[ceph_deploy.mon][INFO  ] distro info: CentOS Linux 7.3.1611 Core</div><div class="line">[node3][DEBUG ] determining if provided host has same hostname in remote</div><div class="line">[node3][DEBUG ] get remote short hostname</div><div class="line">[node3][DEBUG ] adding mon to node3</div><div class="line">[node3][DEBUG ] get remote short hostname</div><div class="line">[node3][DEBUG ] write cluster configuration to /etc/ceph/&#123;cluster&#125;.conf</div><div class="line">[node3][DEBUG ] create the mon path if it does not exist</div><div class="line">[node3][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-node3/done</div><div class="line">[node3][DEBUG ] create a done file to avoid re-doing the mon deployment</div><div class="line">[node3][DEBUG ] create the init path if it does not exist</div><div class="line">[node3][INFO  ] Running command: sudo systemctl enable ceph.target</div><div class="line">[node3][INFO  ] Running command: sudo systemctl enable ceph-mon@node3</div><div class="line">[node3][INFO  ] Running command: sudo systemctl start ceph-mon@node3</div><div class="line">[node3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.node3.asok mon_status</div><div class="line">[node3][WARNIN] monitor node3 does not exist in monmap</div><div class="line">[node3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.node3.asok mon_status</div><div class="line">[node3][DEBUG ] ********************************************************************************</div><div class="line">[node3][DEBUG ] status for monitor: mon.node3</div><div class="line">[node3][DEBUG ] &#123;</div><div class="line">[node3][DEBUG ]   &quot;election_epoch&quot;: 0, </div><div class="line">[node3][DEBUG ]   &quot;extra_probe_peers&quot;: [], </div><div class="line">[node3][DEBUG ]   &quot;monmap&quot;: &#123;</div><div class="line">[node3][DEBUG ]     &quot;created&quot;: &quot;2017-05-27 11:43:56.428001&quot;, </div><div class="line">[node3][DEBUG ]     &quot;epoch&quot;: 2, </div><div class="line">[node3][DEBUG ]     &quot;fsid&quot;: &quot;d6d92de4-2a08-4bd6-a749-6c104c88fc40&quot;, </div><div class="line">[node3][DEBUG ]     &quot;modified&quot;: &quot;2017-06-20 11:52:25.801159&quot;, </div><div class="line">[node3][DEBUG ]     &quot;mons&quot;: [</div><div class="line">[node3][DEBUG ]       &#123;</div><div class="line">[node3][DEBUG ]         &quot;addr&quot;: &quot;192.168.138.141:6789/0&quot;, </div><div class="line">[node3][DEBUG ]         &quot;name&quot;: &quot;node1&quot;, </div><div class="line">[node3][DEBUG ]         &quot;rank&quot;: 0</div><div class="line">[node3][DEBUG ]       &#125;, </div><div class="line">[node3][DEBUG ]       &#123;</div><div class="line">[node3][DEBUG ]         &quot;addr&quot;: &quot;192.168.138.142:6789/0&quot;, </div><div class="line">[node3][DEBUG ]         &quot;name&quot;: &quot;node2&quot;, </div><div class="line">[node3][DEBUG ]         &quot;rank&quot;: 1</div><div class="line">[node3][DEBUG ]       &#125;</div><div class="line">[node3][DEBUG ]     ]</div><div class="line">[node3][DEBUG ]   &#125;, </div><div class="line">[node3][DEBUG ]   &quot;name&quot;: &quot;node3&quot;, </div><div class="line">[node3][DEBUG ]   &quot;outside_quorum&quot;: [], </div><div class="line">[node3][DEBUG ]   &quot;quorum&quot;: [], </div><div class="line">[node3][DEBUG ]   &quot;rank&quot;: -1, </div><div class="line">[node3][DEBUG ]   &quot;state&quot;: &quot;synchronizing&quot;, </div><div class="line">[node3][DEBUG ]   &quot;sync&quot;: &#123;</div><div class="line">[node3][DEBUG ]     &quot;sync_cookie&quot;: 1040187393, </div><div class="line">[node3][DEBUG ]     &quot;sync_provider&quot;: &quot;mon.0 192.168.138.141:6789/0&quot;, </div><div class="line">[node3][DEBUG ]     &quot;sync_start_version&quot;: 2944</div><div class="line">[node3][DEBUG ]   &#125;, </div><div class="line">[node3][DEBUG ]   &quot;sync_provider&quot;: []</div><div class="line">[node3][DEBUG ] &#125;</div><div class="line">[node3][DEBUG ] ********************************************************************************</div><div class="line">[node3][INFO  ] monitor: mon.node3 is currently at the state of synchronizing</div></pre></td></tr></table></figure>
<p>错误1:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">[node3][ERROR ] admin_socket: exception getting command descriptions: [Errno 2] No such file or directory</div><div class="line">[node3][WARNIN] monitor: mon.node3, might not be running yet</div><div class="line">[node3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.node3.asok mon_status</div><div class="line">[node3][ERROR ] admin_socket: exception getting command descriptions: [Errno 2] No such file or directory</div><div class="line">[node3][WARNIN] monitor node3 does not exist in monmap</div><div class="line">[node3][WARNIN] neither `public_addr` nor `public_network` keys are defined for monitors</div><div class="line">[node3][WARNIN] monitors may not be able to form quorum</div></pre></td></tr></table></figure></p>
<p>原因: 未配置public network</p>
<h5 id="添加一个monitor-手动"><a href="#添加一个monitor-手动" class="headerlink" title="添加一个monitor(手动)"></a><b>添加一个monitor(手动)</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line">添加之前查看当前节点 ceph mon stat</div><div class="line"></div><div class="line">1、在目标节点上，新建 mon 的默认目录。&#123;mon-id&#125; 一般取为节点的 hostname 。</div><div class="line">ssh &#123;new-mon-host&#125;</div><div class="line">sudo mkdir /var/lib/ceph/mon/ceph-&#123;mon-id&#125;</div><div class="line">2、创建一个临时目录（和第 1 步中的目录不同，添加 mon 完毕后需要删除该临时目录），来存放新增 mon 所需的各种文件，</div><div class="line">mkdir &#123;tmp&#125;</div><div class="line">3、获取 mon 的 keyring 文件，保存在临时目录下。</div><div class="line">ceph auth get mon. -o &#123;tmp&#125;/&#123;key-filename&#125;</div><div class="line">4、获取集群的 mon map 并保存到临时目录下。</div><div class="line">ceph mon getmap -o &#123;tmp&#125;/&#123;map-filename&#125;</div><div class="line">5.Optional. 更新所有mon节点的配置文件，添加新节点的IP地址到ceph.conf [global]字段的mon_host</div><div class="line">[mon.node3] </div><div class="line">        host                  = node3</div><div class="line">        mon addr              = 192.168.138.143:6789</div><div class="line">		</div><div class="line">6、格式化在第 1 步中建立的 mon 数据目录。需要指定 mon map 文件的路径（获取法定人数的信息和集群的 fsid ）和 keyring 文件的路径。</div><div class="line">sudo ceph-mon -i &#123;mon-id&#125; --mkfs --monmap &#123;tmp&#125;/&#123;map-filename&#125; --keyring &#123;tmp&#125;/&#123;key-filename&#125;</div><div class="line">7、启动节点上的 mon 进程，它会自动加入集群。守护进程需要知道绑定到哪个 IP 地址，可以通过 --public-addr &#123;ip:port&#125; 选择指定，或在 ceph.conf 文件中进行配置 mon addr。</div><div class="line">ceph-mon -i &#123;mon-id&#125; --public-addr &#123;ip:port&#125;</div><div class="line"></div><div class="line">[root@node3 ~]# mkdir /var/lib/ceph/mon/ceph-node3/ -p</div><div class="line">[root@node3 ~]# mkdir /var/lib/ceph/tmp -p</div><div class="line">[root@node3 ~]# cd /var/lib/ceph/mon/ceph-node3/</div><div class="line">[root@node3 ceph-node3]# ceph auth get mon. -o ../../tmp/key-node3</div><div class="line">exported keyring for mon.</div><div class="line">[root@node3 ceph-node3]# ceph mon getmap -o ../../tmp/map-node3</div><div class="line">2017-06-21 12:56:30.078870 7fd654086700  0 -- :/2208778235 &gt;&gt; 192.168.138.143:6789/0 pipe(0x7fd65805cc80 sd=3 :0 s=1 pgs=0 cs=0 l=1 c=0x7fd65805df40).fault</div><div class="line">got monmap epoch 4</div><div class="line">[root@node3 ceph-node3]# ceph-mon  -i node3 --mkfs --monmap ../../tmp/map-node3  --keyring ../../tmp/key-node3 </div><div class="line">ceph-mon: set fsid to d6d92de4-2a08-4bd6-a749-6c104c88fc40</div><div class="line">ceph-mon: created monfs at /var/lib/ceph/mon/ceph-node3 for mon.node3</div><div class="line"></div><div class="line">#启动节点上的 mon 进程，它会自动加入集群,此步骤可略</div><div class="line">[root@node3 ceph-node3]# ceph mon add node3 192.169.138.143:6789</div><div class="line">[root@node3 ceph-node3]# ceph-mon -i node3 --public-addr 192.169.138.143:6789</div></pre></td></tr></table></figure>
<h5 id="删除一个monitor-ceph-deploy"><a href="#删除一个monitor-ceph-deploy" class="headerlink" title="删除一个monitor(ceph-deploy )"></a><b>删除一个monitor(ceph-deploy )</b></h5><p>这里我们删除node3节点的mon,修改部署目录的配置文件，去除node3及其IP,再推送到三个节点<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div></pre></td><td class="code"><pre><div class="line">[neo@admin cluster]$ cat ~/cluster/ceph.conf |grep mon     </div><div class="line">mon_initial_members = node1, node2, node3</div><div class="line">mon_host = 192.168.138.141,192.168.138.142,192.168.138.143</div><div class="line"></div><div class="line">[neo@admin cluster]$ ceph-deploy --overwrite-conf config push node1 node2 node3</div><div class="line">[neo@admin cluster]$ ceph-deploy mon destroy node3</div><div class="line">[ceph_deploy.conf][DEBUG ] found configuration file at: /home/neo/.cephdeploy.conf</div><div class="line">[ceph_deploy.cli][INFO  ] Invoked (1.5.37): /usr/bin/ceph-deploy mon destroy node3</div><div class="line">[ceph_deploy.cli][INFO  ] ceph-deploy options:</div><div class="line">[ceph_deploy.cli][INFO  ]  username                      : None</div><div class="line">[ceph_deploy.cli][INFO  ]  verbose                       : False</div><div class="line">[ceph_deploy.cli][INFO  ]  overwrite_conf                : False</div><div class="line">[ceph_deploy.cli][INFO  ]  subcommand                    : destroy</div><div class="line">[ceph_deploy.cli][INFO  ]  quiet                         : False</div><div class="line">[ceph_deploy.cli][INFO  ]  cd_conf                       : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x1481c68&gt;</div><div class="line">[ceph_deploy.cli][INFO  ]  cluster                       : ceph</div><div class="line">[ceph_deploy.cli][INFO  ]  mon                           : [&apos;node3&apos;]</div><div class="line">[ceph_deploy.cli][INFO  ]  func                          : &lt;function mon at 0x1479320&gt;</div><div class="line">[ceph_deploy.cli][INFO  ]  ceph_conf                     : None</div><div class="line">[ceph_deploy.cli][INFO  ]  default_release               : False</div><div class="line">[ceph_deploy.mon][DEBUG ] Removing mon from node3</div><div class="line">[node3][DEBUG ] connection detected need for sudo</div><div class="line">[node3][DEBUG ] connected to host: node3 </div><div class="line">[node3][DEBUG ] detect platform information from remote host</div><div class="line">[node3][DEBUG ] detect machine type</div><div class="line">[node3][DEBUG ] find the location of an executable</div><div class="line">[node3][DEBUG ] get remote short hostname</div><div class="line">[node3][INFO  ] Running command: sudo ceph --cluster=ceph -n mon. -k /var/lib/ceph/mon/ceph-node3/keyring mon remove node3</div><div class="line">[node3][WARNIN] removing mon.node3 at 192.168.138.143:6789/0, there will be 2 monitors</div><div class="line">[node3][INFO  ] polling the daemon to verify it stopped</div><div class="line">[node3][INFO  ] Running command: sudo systemctl stop ceph-mon@node3.service</div><div class="line">[node3][INFO  ] Running command: sudo mkdir -p /var/lib/ceph/mon-removed</div><div class="line">[node3][DEBUG ] move old monitor data</div><div class="line"></div><div class="line">[neo@admin cluster]$ sudo ceph -s</div><div class="line">    cluster d6d92de4-2a08-4bd6-a749-6c104c88fc40</div><div class="line">     health HEALTH_OK</div><div class="line">     monmap e2: 2 mons at &#123;node1=192.168.138.141:6789/0,node2=192.168.138.142:6789/0&#125;</div><div class="line">            election epoch 62, quorum 0,1 node1,node2</div><div class="line">     osdmap e246: 9 osds: 9 up, 9 in</div><div class="line">            flags sortbitwise,require_jewel_osds</div><div class="line">      pgmap v1167: 256 pgs, 1 pools, 4 bytes data, 1 objects</div><div class="line">            350 MB used, 18377 GB / 18378 GB avail</div><div class="line">                 256 active+clean</div><div class="line">				 </div><div class="line">备注:</div><div class="line">ceph-deploy删除MON的时候调用的指令是ceph mon remove node3,删除的MON的文件夹被移到了/var/lib/ceph/mon-removed</div></pre></td></tr></table></figure></p>
<p>注意： 确保你删除某个 Mon 后，其余 Mon 仍能达成一致。如果不可能，删除它之前可能需要先增加一个</p>
<h5 id="删除一个monitor-手动"><a href="#删除一个monitor-手动" class="headerlink" title="删除一个monitor(手动)"></a><b>删除一个monitor(手动)</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">1、停止 mon 进程。</div><div class="line">[neo@admin cluster]$ sudo systemctl stop ceph-mon@node3</div><div class="line">2、从集群中删除 monitor。</div><div class="line">[neo@admin cluster]$ sudo ceph mon remove node3</div><div class="line">removing mon.node3 at 192.168.138.143:6789/0, there will be 2 monitors</div><div class="line">[neo@admin cluster]$ sudo ceph -s</div><div class="line">    cluster d6d92de4-2a08-4bd6-a749-6c104c88fc40</div><div class="line">     health HEALTH_OK</div><div class="line">     monmap e4: 2 mons at &#123;node1=192.168.138.141:6789/0,node2=192.168.138.142:6789/0&#125;</div><div class="line">            election epoch 74, quorum 0,1 node1,node2</div><div class="line">     osdmap e264: 9 osds: 9 up, 9 in</div><div class="line">            flags sortbitwise,require_jewel_osds</div><div class="line">      pgmap v1218: 256 pgs, 1 pools, 4 bytes data, 1 objects</div><div class="line">            354 MB used, 18377 GB / 18378 GB avail</div><div class="line">                 256 active+clean</div><div class="line"></div><div class="line">3、从 ceph.conf 中移除 mon 的入口部分（如果有）。</div></pre></td></tr></table></figure>
<h5 id="删除-Monitor（从不健康的集群中）"><a href="#删除-Monitor（从不健康的集群中）" class="headerlink" title="删除 Monitor（从不健康的集群中）"></a><b>删除 Monitor（从不健康的集群中）</b></h5><p>从一个不健康的集群（比如集群中的 monitor 无法达成法定人数）中删除 ceph-mon 守护进程。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">1、停止集群中所有的 ceph-mon 守护进程。</div><div class="line">ssh &#123;mon-host&#125;</div><div class="line">systemctl stop ceph-mon@mon-host</div><div class="line"># and repeat for all mons</div><div class="line">2、确认存活的 mon 并登录该节点。</div><div class="line">ssh &#123;mon-host&#125;</div><div class="line">3、提取 mon map。</div><div class="line">ceph-mon -i &#123;mon-id&#125; --extract-monmap &#123;map-path&#125;</div><div class="line"># in most cases, that&apos;s</div><div class="line">ceph-mon -i `hostname` --extract-monmap /tmp/monmap</div><div class="line">4、删除未存活或有问题的的 monitor。比如，有 3 个 monitors，mon.node1 、mon.node2 和 mon.node3，现在仅有 mon.node1 存活，执行下列步骤：</div><div class="line">monmaptool &#123;map-path&#125; --rm &#123;mon-id&#125;</div><div class="line"># for example,</div><div class="line">monmaptool /tmp/monmap --rm node2</div><div class="line">monmaptool /tmp/monmap --rm node3</div><div class="line">5、向存活的 monitor(s) 注入修改后的 mon map。比如，把 mon map 注入 mon.node1，执行下列步骤：</div><div class="line">ceph-mon -i &#123;mon-id&#125; --inject-monmap &#123;map-path&#125;</div><div class="line"># for example,</div><div class="line">ceph-mon -i a --inject-monmap /tmp/monmap</div><div class="line">6、启动存活的 monitor。</div><div class="line">7、确认 monitor 是否达到法定人数（ ceph -s ）。</div><div class="line">8、你可能需要把已删除的 monitor 的数据目录 /var/lib/ceph/mon 归档到一个安全的位置。或者，如果你确定剩下的 monitor 是健康的且数量足够，也可以直接删除数据目录。</div></pre></td></tr></table></figure>
<h5 id="挂掉的2个节点的监控数据的恢复"><a href="#挂掉的2个节点的监控数据的恢复" class="headerlink" title="挂掉的2个节点的监控数据的恢复"></a><b>挂掉的2个节点的监控数据的恢复</b></h5><p>前边我们说如果挂掉的2个节点的监控数据都损坏了呢？恢复方法请参考<a href="http://www.cnblogs.com/hustcat/p/3925971.html" target="_blank" rel="external">这里</a></p>
<p>ref<br><a href="http://www.xuxiaopang.com/2016/10/26/exp-monitor-operation/" target="_blank" rel="external">monitor的增删改备</a><br><a href="http://blog.csdn.net/scaleqiao/article/details/50513655" target="_blank" rel="external">Ceph Monitor挂了之后对集群的影响</a><br><a href="https://lihaijing.gitbooks.io/ceph-handbook/content/Operation/add_rm_mon.html" target="_blank" rel="external">增加/删除 Monitor</a><br><a href="https://github.com/thesues/cephdoc/blob/master/ceph-deploy-cn.markdown" target="_blank" rel="external">thesues/cephdoc</a><br><a href="http://blog.sina.com.cn/s/blog_8ea8e9d50102xhbq.html" target="_blank" rel="external">Ceph集群</a></p>
<hr>
<p>您的鼓励是我写作最大的动力</p>
<p>俗话说，投资效率是最好的投资。 如果您感觉我的文章质量不错，读后收获很大，预计能为您提高 10% 的工作效率，不妨小额捐助我一下，让我有动力继续写出更多好文章。    </p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;一般来说，在实际运行中，ceph monitor的个数是2n+1(n&amp;gt;=0)个，在线上至少3个，只要正常的节点数&amp;gt;=n+1，ceph的paxos算法能保证系统的正常运行。所以,对于3个节点，同时只能挂掉一个。建议（但不是强制）部署奇数个 monitor ,不建议
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>ceph-Monitor clock skew detected</title>
    <link href="https://t1ger.github.io/2017/06/13/ceph-Monitor-clock-skew-detected/"/>
    <id>https://t1ger.github.io/2017/06/13/ceph-Monitor-clock-skew-detected/</id>
    <published>2017-06-13T07:00:09.000Z</published>
    <updated>2017-06-13T06:59:44.990Z</updated>
    
    <content type="html"><![CDATA[<p>ceph异常警告:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">[root@admin ~]# ceph -s</div><div class="line">    cluster d6d92de4-2a08-4bd6-a749-6c104c88fc40</div><div class="line">     health HEALTH_WARN</div><div class="line">            clock skew detected on mon.node2</div><div class="line">            Monitor clock skew detected </div><div class="line">     monmap e1: 3 mons at &#123;node1=192.168.138.141:6789/0,node2=192.168.138.142:6789/0,node3=192.168.138.143:6789/0&#125;</div><div class="line">            election epoch 46, quorum 0,1,2 node1,node2,node3</div><div class="line">     osdmap e218: 9 osds: 9 up, 9 in</div><div class="line">            flags sortbitwise,require_jewel_osds</div><div class="line">      pgmap v1007: 256 pgs, 1 pools, 4 bytes data, 1 objects</div><div class="line">            345 MB used, 18377 GB / 18378 GB avail</div><div class="line">                 256 active+clean</div><div class="line">				 			 </div><div class="line">[root@admin ~]# ceph health detail</div><div class="line">HEALTH_WARN clock skew detected on mon.node2, mon.node3; Monitor clock skew detected </div><div class="line">mon.node2 addr 192.168.138.142:6789/0 clock skew 0.434161s &gt; max 0.05s (latency 0.00740637s)</div><div class="line">mon.node3 addr 192.168.138.143:6789/0 clock skew 0.687451s &gt; max 0.05s (latency 0.00722567s)</div></pre></td></tr></table></figure></p>
<p>解决方法:</p>
<ul>
<li><p>方法一:配置ntp server<br>本来以为配置ntp server了,时间应该就一致了,原来ceph默认容忍的时间偏差不到1秒,随意只能用本地的ntp server了<br>下面来说说配置ntp server(If Iptables is running, allow NTP port. NTP uses 123/UDP.)<br>chrony_server</p>
<p>  1.install chrony</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">yum -y install chrony</div></pre></td></tr></table></figure>
<p>  2.config chrony</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">[root@admin ~]# cat /etc/chrony.conf </div><div class="line"># These servers were defined in the installation:</div><div class="line">server 3.centos.pool.ntp.org iburst</div><div class="line">server 0.centos.pool.ntp.org iburst</div><div class="line">server cn.pool.ntp.org iburst</div><div class="line">server 1.centos.pool.ntp.org iburst</div><div class="line">server 2.centos.pool.ntp.org iburst</div><div class="line"># Serve time even if not synchronized to any NTP server.</div><div class="line">local stratum 10</div><div class="line">....</div><div class="line">allow 192.168/16</div></pre></td></tr></table></figure>
<p>  3.run chrony</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">systemctl enable chronyd</div><div class="line">systemctl start chronyd</div></pre></td></tr></table></figure>
<p>  4.timedatectl</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">## 查看时间</div><div class="line">timedatectl</div><div class="line">## 开启ntp时间同步</div><div class="line">timedatectl set-ntp true</div></pre></td></tr></table></figure>
<p>  5.chronyc</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">## 查看ntp_servers状态</div><div class="line">chronyc sources -v</div><div class="line">## 查看ntp_sync状态</div><div class="line">chronyc sourcestats -v</div><div class="line">## 查看ntp_servers 是否在线</div><div class="line">chronyc activity -v</div><div class="line">## 查看ntp时间详细信息</div><div class="line">chronyc tracking -v</div></pre></td></tr></table></figure>
<p>  chrony_client<br>  1.install chrony</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">yum -y install chrony</div></pre></td></tr></table></figure>
<p>  2.config chrony</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[root@node3 ~]# cat /etc/chrony.conf</div><div class="line"># These servers were defined in the installation:</div><div class="line">server 192.168.138.140 iburst</div></pre></td></tr></table></figure>
<p>  3.run chrony</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">systemctl enable chronyd</div><div class="line">systemctl start chronyd</div></pre></td></tr></table></figure>
<p>  4.timedatectl</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">## 查看时间</div><div class="line">timedatectl</div><div class="line">## 开启ntp时间同步</div><div class="line">timedatectl set-ntp true</div></pre></td></tr></table></figure>
<p>  5.chronyc</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">## 查看ntp_servers状态</div><div class="line">chronyc sources -v</div><div class="line">## 查看ntp_sync状态</div><div class="line">chronyc sourcestats -v</div><div class="line">## 查看ntp_servers 是否在线</div><div class="line">chronyc activity -v</div><div class="line">## 查看ntp时间详细信息</div><div class="line">chronyc tracking -v</div><div class="line"></div><div class="line">[root@node3 ~]# chronyc tracking</div><div class="line">Reference ID    : 192.168.138.140 (192.168.138.140)</div><div class="line">Stratum         : 5</div><div class="line">Ref time (UTC)  : Tue Jun 13 05:52:48 2017</div><div class="line">System time     : 0.000494327 seconds fast of NTP time</div><div class="line">Last offset     : +0.000704829 seconds</div><div class="line">RMS offset      : 0.001765276 seconds</div><div class="line">Frequency       : 71.667 ppm slow</div><div class="line">Residual freq   : +0.269 ppm</div><div class="line">Skew            : 6.979 ppm</div><div class="line">Root delay      : 0.324675 seconds</div><div class="line">Root dispersion : 0.039305 seconds</div><div class="line">Update interval : 64.8 seconds</div><div class="line">Leap status     : Normal</div></pre></td></tr></table></figure>
<p>  备注:chrony手动校时</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">chronyc -a makestep</div></pre></td></tr></table></figure>
</li>
<li><p>方法二:调整ceph参数避免<br>1.在admin结点上，修改ceph.conf，添加：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">mon_clock_drift_allowed = 5</div><div class="line">mon_clock_drift_warn_backoff = 30</div></pre></td></tr></table></figure>
<p>  详细参数参考<a href="http://docs.ceph.com/docs/hammer/rados/configuration/mon-config-ref/#clock" target="_blank" rel="external">这里</a></p>
<p>  2.执行下面命令，node1等是monitor结点的名称</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ceph-deploy --overwrite-conf admin node1 node2 node3</div></pre></td></tr></table></figure>
<p>  3.重启monitor</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">systemctl restart ceph-mon@node1</div><div class="line">systemctl restart ceph-mon@node2</div><div class="line">systemctl restart ceph-mon@node3</div></pre></td></tr></table></figure>
<p>  4.验证</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">[root@admin ~]# ceph -s</div><div class="line">    cluster d6d92de4-2a08-4bd6-a749-6c104c88fc40</div><div class="line">     health HEALTH_OK</div><div class="line">     monmap e1: 3 mons at &#123;node1=192.168.138.141:6789/0,node2=192.168.138.142:6789/0,node3=192.168.138.143:6789/0&#125;</div><div class="line">            election epoch 48, quorum 0,1,2 node1,node2,node3</div><div class="line">     osdmap e218: 9 osds: 9 up, 9 in</div><div class="line">            flags sortbitwise,require_jewel_osds</div><div class="line">      pgmap v1007: 256 pgs, 1 pools, 4 bytes data, 1 objects</div><div class="line">            345 MB used, 18377 GB / 18378 GB avail</div><div class="line">                 256 active+clean</div></pre></td></tr></table></figure>
</li>
</ul>
<p>ref<br><a href="https://www.zfl9.com/chrony.html" target="_blank" rel="external">chrony时间同步</a><br><a href="https://dywang.csie.cyut.edu.tw/dywang/rhel7/node70.html" target="_blank" rel="external">chronyd 使用</a><br><a href="https://www.server-world.info/en/note?os=CentOS_6&amp;p=ntp&amp;f=3" target="_blank" rel="external">Chrony : Configure NTP Server</a><br><a href="https://my.oschina.net/u/2475751/blog/704375" target="_blank" rel="external">ceph: HEALTH_WARN: Monitor clock skew detected</a></p>
<hr>
<p>您的鼓励是我写作最大的动力</p>
<p>俗话说，投资效率是最好的投资。 如果您感觉我的文章质量不错，读后收获很大，预计能为您提高 10% 的工作效率，不妨小额捐助我一下，让我有动力继续写出更多好文章。    </p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;ceph异常警告:&lt;br&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div cl
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>ceph-crush algorithm(二)</title>
    <link href="https://t1ger.github.io/2017/06/06/ceph-crush%20algorithm(%E4%BA%8C)/"/>
    <id>https://t1ger.github.io/2017/06/06/ceph-crush algorithm(二)/</id>
    <published>2017-06-06T08:23:40.000Z</published>
    <updated>2017-06-13T09:07:09.977Z</updated>
    
    <content type="html"><![CDATA[<h5 id="源码文件"><a href="#源码文件" class="headerlink" title="源码文件"></a><b>源码文件</b></h5><p>crush目录下的源码文件如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">CrushCompiler.cc	</div><div class="line">CrushCompiler.h	</div><div class="line">CrushLocation.cc	</div><div class="line">CrushLocation.h	</div><div class="line">CrushTester.cc	</div><div class="line">CrushTester.h	</div><div class="line">CrushTreeDumper.h	</div><div class="line">CrushWrapper.cc	</div><div class="line">CrushWrapper.h	</div><div class="line">CrushWrapper.i	</div><div class="line">builder.c	</div><div class="line">builder.h	</div><div class="line">crush.c	</div><div class="line">crush.h	</div><div class="line">crush_compat.h	</div><div class="line">crush_ln_table.h	</div><div class="line">grammar.h	</div><div class="line">hash.c	</div><div class="line">hash.h	</div><div class="line">mapper.c	</div><div class="line">mapper.h	</div><div class="line">old_sample.txt	</div><div class="line">sample.txt	</div><div class="line">types.h</div></pre></td></tr></table></figure></p>
<h5 id="CRUSH-maps"><a href="#CRUSH-maps" class="headerlink" title="CRUSH maps"></a><b>CRUSH maps</b></h5><p>crush maps组成: osd list,Bucket list ,rule list</p>
<p>Device<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">device 0 osd.0</div><div class="line">device 1 osd.1</div><div class="line">device 2 osd.2</div><div class="line">device 3 osd.3</div><div class="line">device 4 osd.4</div><div class="line">device 5 osd.5</div><div class="line">device 6 osd.6</div><div class="line">device 7 osd.7</div></pre></td></tr></table></figure></p>
<p>Bucket<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line">host ceph-osd-ssd-server-1 &#123;</div><div class="line">           id -1</div><div class="line">           alg straw</div><div class="line">           hash 0</div><div class="line">           item osd.0 weight 1.00</div><div class="line">           item osd.1 weight 1.00</div><div class="line">   &#125;</div><div class="line">   host ceph-osd-ssd-server-2 &#123;</div><div class="line">           id -2</div><div class="line">           alg straw</div><div class="line">           hash 0</div><div class="line">           item osd.2 weight 1.00</div><div class="line">           item osd.3 weight 1.00</div><div class="line">   &#125;</div><div class="line">  host ceph-osd-platter-server-1 &#123;</div><div class="line">           id -3</div><div class="line">           alg straw</div><div class="line">           hash 0</div><div class="line">           item osd.4 weight 1.00</div><div class="line">           item osd.5 weight 1.00</div><div class="line">   &#125;</div><div class="line">   host ceph-osd-platter-server-2 &#123;</div><div class="line">           id -4</div><div class="line">           alg straw</div><div class="line">           hash 0</div><div class="line">           item osd.6 weight 1.00</div><div class="line">           item osd.7 weight 1.00</div><div class="line">   &#125;</div><div class="line">   root platter &#123;</div><div class="line">           id -5</div><div class="line">           alg straw</div><div class="line">           hash 0</div><div class="line">           item ceph-osd-platter-server-1 weight 2.00</div><div class="line">           item ceph-osd-platter-server-2 weight 2.00</div><div class="line">   &#125;</div><div class="line">   root ssd &#123;</div><div class="line">           id -6</div><div class="line">           alg straw</div><div class="line">           hash 0</div><div class="line">           item ceph-osd-ssd-server-1 weight 2.00</div><div class="line">           item ceph-osd-ssd-server-2 weight 2.00</div><div class="line">   &#125;</div></pre></td></tr></table></figure></p>
<p>Rule<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div></pre></td><td class="code"><pre><div class="line">rule data &#123;</div><div class="line">              ruleset 0</div><div class="line">              type replicated</div><div class="line">              min_size 2</div><div class="line">              max_size 2</div><div class="line">              step take platter</div><div class="line">              step chooseleaf firstn 0 type host</div><div class="line">              step emit</div><div class="line">      &#125;</div><div class="line">      rule metadata &#123;</div><div class="line">              ruleset 1</div><div class="line">              type replicated</div><div class="line">              min_size 0</div><div class="line">              max_size 10</div><div class="line">              step take platter</div><div class="line">              step chooseleaf firstn 0 type host</div><div class="line">              step emit</div><div class="line">      &#125;</div><div class="line">      rule rbd &#123;</div><div class="line">              ruleset 2</div><div class="line">              type replicated</div><div class="line">              min_size 0</div><div class="line">              max_size 10</div><div class="line">              step take platter</div><div class="line">              step chooseleaf firstn 0 type host</div><div class="line">              step emit</div><div class="line">      &#125;</div><div class="line">      rule platter &#123;</div><div class="line">              ruleset 3</div><div class="line">              type replicated</div><div class="line">              min_size 0</div><div class="line">              max_size 10</div><div class="line">              step take platter</div><div class="line">              step chooseleaf firstn 0 type host</div><div class="line">              step emit</div><div class="line">      &#125;</div><div class="line">      rule ssd &#123;</div><div class="line">              ruleset 4</div><div class="line">              type replicated</div><div class="line">              min_size 0</div><div class="line">              max_size 4</div><div class="line">              step take ssd</div><div class="line">              step chooseleaf firstn 0 type host</div><div class="line">              step emit</div><div class="line">      &#125;</div><div class="line">      rule ssd-primary &#123;</div><div class="line">              ruleset 5</div><div class="line">              type replicated</div><div class="line">              min_size 5</div><div class="line">              max_size 10</div><div class="line">              step take ssd</div><div class="line">              step chooseleaf firstn 1 type host</div><div class="line">              step emit</div><div class="line">              step take platter</div><div class="line">              step chooseleaf firstn -1 type host</div><div class="line">              step emit</div><div class="line">      &#125;</div></pre></td></tr></table></figure></p>
<h5 id="Do-Rule"><a href="#Do-Rule" class="headerlink" title="Do Rule"></a><b>Do Rule</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">void do_rule(int rule, int x, vector&lt;int&gt;&amp; out, int maxout,</div><div class="line">	       const WeightVector&amp; weight,</div><div class="line">	       uint64_t choose_args_index) const &#123;</div><div class="line">    int rawout[maxout];</div><div class="line">    char work[crush_work_size(crush, maxout)];</div><div class="line">    crush_init_workspace(crush, work);</div><div class="line">    crush_choose_arg_map arg_map = choose_args_get(choose_args_index);</div><div class="line">    int numrep = crush_do_rule(crush, rule, x, rawout, maxout, &amp;weight[0],</div><div class="line">			       weight.size(), work, arg_map.args);</div></pre></td></tr></table></figure>
<p>@rule: 使用的crush_rule在crush_map的rules列表中所在index<br>@x: 输入Hash ID，object_id或者pg_id)<br>@out: 输出Device ID列表<br>@maxout: 在输出Device ID的个数，副本的个数<br>@weight: 输出Device列表对应的权重</p>
<p>具体工作调用crush_do_rule完成</p>
<p>Mapper.c<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">/**</div><div class="line"> * crush_do_rule - calculate a mapping with the given input and rule</div><div class="line"> * @map: the crush_map</div><div class="line"> * @ruleno: the rule id</div><div class="line"> * @x: hash input</div><div class="line"> * @result: pointer to result vector</div><div class="line"> * @result_max: maximum result size</div><div class="line"> * @weight: weight vector (for map leaves)</div><div class="line"> * @weight_max: size of weight vector</div><div class="line"> * @cwin: Pointer to at least map-&gt;working_size bytes of memory or NULL.</div><div class="line"> */</div><div class="line">int crush_do_rule(const struct crush_map *map,</div><div class="line">		  int ruleno, int x, int *result, int result_max,</div><div class="line">		  const __u32 *weight, int weight_max,</div><div class="line">		  void *cwin, const struct crush_choose_arg *choose_args)</div></pre></td></tr></table></figure></p>
<p>值得说的变量emit 通常用在规则的结束,同时可以被用在在形相同规则下选择不同的树.更多详细信息看<a href="http://docs.ceph.com/docs/master/rados/operations/crush-map/#crush-map-rules" target="_blank" rel="external">这里</a></p>
<p>ref<br><a href="http://www.shalandis.com/original/2016/05/19/CEPH-CRUSH-algorithm-source-code-analysis/" target="_blank" rel="external">CEPH CRUSH algorithm source code analysis</a><br><a href="http://way4ever.com/?p=123" target="_blank" rel="external">ceph的CRUSH算法的源码分析</a><br><a href="http://www.xuxiaopang.com/2016/11/08/easy-ceph-CRUSH/" target="_blank" rel="external">大话Ceph–CRUSH那点事儿</a><br><a href="http://blog.csdn.net/scaleqiao/article/details/51165575" target="_blank" rel="external">Ceph源码目录架构</a></p>
<hr>
<p>您的鼓励是我写作最大的动力</p>
<p>俗话说，投资效率是最好的投资。 如果您感觉我的文章质量不错，读后收获很大，预计能为您提高 10% 的工作效率，不妨小额捐助我一下，让我有动力继续写出更多好文章。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;源码文件&quot;&gt;&lt;a href=&quot;#源码文件&quot; class=&quot;headerlink&quot; title=&quot;源码文件&quot;&gt;&lt;/a&gt;&lt;b&gt;源码文件&lt;/b&gt;&lt;/h5&gt;&lt;p&gt;crush目录下的源码文件如下:&lt;br&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;ta
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>ceph-crush algorithm(一)</title>
    <link href="https://t1ger.github.io/2017/06/02/ceph-crush%20algorithm(%E4%B8%80)/"/>
    <id>https://t1ger.github.io/2017/06/02/ceph-crush algorithm(一)/</id>
    <published>2017-06-02T10:28:17.000Z</published>
    <updated>2017-06-06T09:37:53.304Z</updated>
    
    <content type="html"><![CDATA[<h5 id="CRUSH简介"><a href="#CRUSH简介" class="headerlink" title="CRUSH简介"></a><b>CRUSH简介</b></h5><p>CRUSH是ceph的一个模块，主要解决可控、可扩展、去中心化的数据副本分布问题.<br>CRUSH全称Controlled Replication Under Scalable Hashing，是一种数据分发算法，类似于哈希和一致性哈希。<br>哈希的问题在于数据增长时不能动态加Bucket，一致性哈希的问题在于加Bucket时数据迁移量比较大，<br>其他数据分发算法依赖中心的Metadata服务器来存储元数据效率较低，CRUSH则是通过计算、接受多维参数的来解决动态数据分发的场景</p>
<p>CRUSH实现了一种伪随机数据分布算法，它能够在层级结构的存储集群中有效的分布对象的副本,它的参数是object id或object group id，并返回一组存储设备(用于保存object副本)<br>CRUSH需要cluster map(描述存储集群的层级结构)、和副本分布策略(rule)</p>
<h5 id="算法基础"><a href="#算法基础" class="headerlink" title="算法基础"></a><b>算法基础</b></h5><p>在学习CRUSH之前，需要了解以下的内容。<br>CRUSH算法通过每个设备的权重来计算数据对象的分布。对象分布是由cluster map和data distribution policy决定的。<br>cluster map描述了可用存储资源和层级结构(比如有多少个机架，每个机架上有多少个服务器，每个服务器上有多少个磁盘)。<br>data distribution policy由placement rules组成。rule决定了每个数据对象有多少个副本，这些副本存储的限制条件(比如3个副本放在不同的机架中)。<br>每个rule就是一系列操作，take操作就是就是选一个bucket，select操作就是选择n个类型是t的项，emit操作就是提交最后的返回结果。<br>select要考虑的东西主要包括是否冲突、是否有失败和负载问题.</p>
<p>CRUSH算法还通过输入一个整数x，输出则是一个包含n个目标的列表R，例如三备份的话输出可能是[1, 3, 5]。<br>(osd0, osd1, osd2 … osdn) = CRUSH(x)<br>CRUSH利用多参数HASH函数，HASH函数中的参数包括x，使得从x到OSD集合是确定性的和独立的。<br>CRUSH只使用了cluster map、placement rules、x。CRUSH是伪随机算法，相似输入的结果之间没有相关性。</p>
<ul>
<li><p>Cluster map<br>Cluster map由device和bucket组成，它们都有id和权重值。Bucket可以包含任意数量item。item可以都是的devices或者都是buckets。<br>管理员控制存储设备的权重。权重和存储设备的容量有关。Bucket的权重被定义为它所包含所有item的权重之和。<br>CRUSH基于4种不同的bucket type，每种有不同的选择算法。</p>
</li>
<li><p>副本分布<br>副本在存储设备上的分布影响数据的安全。cluster map反应了存储系统的物理结构。CRUSH placement policies决定把对象副本分布在不同的区域(某个区域发生故障时并不会影响其他区域)。每个rule包含一系列操作(用在层级结构上)<br>这些操作包括：<br>1.take(a) ：选择一个item，一般是bucket，并返回bucket所包含的所有item。这些item是后续操作的参数，这些item组成向量i。<br>2.select(n, t)：迭代操作每个item(向量i中的item)，对于每个item(向量i中的item)向下遍历(遍历这个item所包含的item)，都返回n个不同的item(type为t的item)，并把这些item都放到向量i中。select函数会调用c(r, x)函数，这个函数会在每个bucket中伪随机选择一个item。<br>3.emit：把向量i放到result中。</p>
<p>存储设备有一个确定的类型。每个bucket都有type属性值，用于区分不同的bucket类型(比如”row”、”rack”、”host”等，type可以自定义)。rules可以包含多个take和emit语句块，这样就允许从不同的存储池中选择副本的storage target</p>
</li>
<li><p>冲突、故障、超载<br>select(n, t)操作会循环选择第 r=1,…,n 个副本，r作为选择参数。在这个过程中，假如选择到的item遇到三种情况(冲突，故障，超载)时，CRUSH会拒绝选择这个item，并使用r’(r’和r、出错次数、firstn参数有关)作为选择参数重新选择item。<br>1.冲突：这个item已经在向量i中，已被选择。<br>2.故障：设备发生故障，不能被选择。<br>3.超载：设备使用容量超过警戒线，没有剩余空间保存数据对象。<br>故障设备和超载设备会在cluster map上标记(还留在系统中)，这样避免了不必要的数据迁移。</p>
</li>
<li><p>MAP改变和数据迁移<br>当添加移除存储设备，或有存储设备发生故障时(cluster map发生改变时)，存储系统中的数据会发生迁移。好的数据分布算法可以最小化数据迁移大小。</p>
</li>
</ul>
<h5 id="CRUSH总结"><a href="#CRUSH总结" class="headerlink" title="CRUSH总结"></a><b>CRUSH总结</b></h5><ul>
<li><p>算法总结<br>CRUSH与一致性哈希最大的区别在于接受的参数多了cluster map和placement rules，这样就可以根据目前cluster的状态动态调整数据位置，同时通过算法得到一致的结果</p>
</li>
<li><p>算法补充<br>前面介绍了bucket根据不同场景有四种类型，分别是Uniform、List、Tree和Straw，他们对应运行数据和数据迁移量有不同的tradeoff，目前大家都在用Straw因此不太需要关注其他。<br>目前erasing code可以大大减小三备份的数据量，但除了会导致数据恢复慢，部分ceph支持的功能也是不能直接用的，而且功能仍在开发中不建议使用。</p>
<p>  有兴趣的读者可以拜读下Sega本人的博士论文作品:<br>长论文包含了RADOS、CRUSH等所有内容的介绍，但篇幅相当长，如果感兴趣可以阅读，标题为《CEPH: RELIABLE, SCALABLE, AND HIGH-PERFORMANCE DISTRIBUTED STORAGE》，地址 <a href="http://ceph.com/papers/weil-thesis.pdf" target="_blank" rel="external">http://ceph.com/papers/weil-thesis.pdf</a> </p>
<p>  CRUSH论文标题为《CRUSH: Controlled, Scalable, Decentralized Placement of Replicated Data》，地址 <a href="http://ceph.com/papers/weil-crush-sc06.pdf" target="_blank" rel="external">http://ceph.com/papers/weil-crush-sc06.pdf</a> ，介绍了CRUSH的设计与实现细节</p>
<p>  RADOS沦为标题为《RADOS: A Scalable, Reliable Storage Service for Petabyte-scale Storage Clusters》，地址为 <a href="http://ceph.com/papers/weil-rados-pdsw07.pdf" target="_blank" rel="external">http://ceph.com/papers/weil-rados-pdsw07.pdf</a> ，介绍了RADOS的设计与实现细节</p>
<p>  CephFS论文标题为《Ceph: A Scalable, High-Performance Distributed File System》，地址为 <a href="http://ceph.com/papers/weil-ceph-osdi06.pdf" target="_blank" rel="external">http://ceph.com/papers/weil-ceph-osdi06.pdf</a> ，介绍了Ceph的基本架构和Ceph的设计与实现细节</p>
</li>
</ul>
<p>ref<br><a href="http://way4ever.com/?p=122" target="_blank" rel="external">ceph的CRUSH数据分布算法介绍</a><br><a href="https://tobegit3hub1.gitbooks.io/ceph_from_scratch/content/architecture/crush.html" target="_blank" rel="external">CRUSH详解</a><br><a href="http://www.xuxiaopang.com/2016/11/08/easy-ceph-CRUSH/" target="_blank" rel="external">大话Ceph–CRUSH那点事儿</a></p>
<hr>
<p>您的鼓励是我写作最大的动力</p>
<p>俗话说，投资效率是最好的投资。 如果您感觉我的文章质量不错，读后收获很大，预计能为您提高 10% 的工作效率，不妨小额捐助我一下，让我有动力继续写出更多好文章。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;CRUSH简介&quot;&gt;&lt;a href=&quot;#CRUSH简介&quot; class=&quot;headerlink&quot; title=&quot;CRUSH简介&quot;&gt;&lt;/a&gt;&lt;b&gt;CRUSH简介&lt;/b&gt;&lt;/h5&gt;&lt;p&gt;CRUSH是ceph的一个模块，主要解决可控、可扩展、去中心化的数据副本分布问题.&lt;b
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>ceph硬件推荐</title>
    <link href="https://t1ger.github.io/2017/06/01/ceph%E7%A1%AC%E4%BB%B6%E6%8E%A8%E8%8D%90/"/>
    <id>https://t1ger.github.io/2017/06/01/ceph硬件推荐/</id>
    <published>2017-06-01T11:41:45.000Z</published>
    <updated>2017-06-02T05:42:40.066Z</updated>
    
    <content type="html"><![CDATA[<h4 id="cpu和内存"><a href="#cpu和内存" class="headerlink" title="cpu和内存"></a><b>cpu和内存</b></h4><ul>
<li>CPU<br>metadata servers：属于CPU敏感型的角色服务器<br>OSDs：负责运行RADOS service，使用CRUSH算法计算数据分布，复制数据，并维护它们自己的拷贝，所以CPU也需要较好的<br>Monitors：监控服务器芝士简单的维护一份集群mapping的主拷贝，所以并非是CPU敏感的</li>
<li>内存<br>metadata servers和monitors需要快速响应它们存储的数据，所以它们需要不少的内存<br>每个daemon实例最少1GB<br>OSDs不需要太多的内存做日常的操作，500MB即可，但是在recovery阶段<br>每1TB的存储，需要消耗约1GB的内存，所以说，内存是越多越好</li>
</ul>
<h4 id="硬盘"><a href="#硬盘" class="headerlink" title="硬盘"></a><b>硬盘</b></h4><p>这里说下固态盘</p>
<ul>
<li>固态硬盘: SSD 和硬盘相比每 GB 成本通常要高 10 倍以上，但访问时间至少比硬盘快 100 倍.SSD 没有可移动机械部件，所以不存在和硬盘一样的局限性。但 SSD 也有局限性，评估SSD 时，顺序读写性能很重要，在为多个 OSD 存储日志时，有着 400MB/s 顺序读写吞吐量的 SSD 其性能远高于 120MB/s 的.<br>备注:我们建议发掘 SSD 的用法来提升性能。然而在大量投入 SSD 前，我们强烈建议核实 SSD 的性能指标，并在测试环境下衡量性能<br>可接受的 IOPS 指标对选择用于 Ceph 的 SSD 还不够，用于日志和 SSD 时还有几个重要考量：<br>写密集语义： 记日志涉及写密集语义，所以你要确保选用的 SSD 写入性能和硬盘相当或好于硬盘。廉价 SSD 可能在加速访问的同时引入写延时，有时候高性能硬盘的写入速度可以和便宜 SSD 相媲美<br>顺序写入： 在一个 SSD 上为多个 OSD 存储多个日志时也必须考虑 SSD 的顺序写入极限，因为它们要同时处理多个 OSD 日志的写入请求。<br>分区对齐： 采用了 SSD 的一个常见问题是人们喜欢分区，却常常忽略了分区对齐，这会导致 SSD 的数据传输速率慢很多，所以请确保分区对齐了</li>
<li>其他注意事项<br>你可以在同一主机上运行多个 OSD ，但要确保 OSD 硬盘总吞吐量不超过为客户端提供读写服务所需的网络带宽；还要考虑集群在每台主机上所存储的数据占总体的百分比，如果一台主机所占百分比太大而它挂了，就可能导致诸如超过 full ratio 的问题，此问题会使 Ceph 中止运作以防数据丢失。<br>如果每台主机运行多个 OSD ，也得保证内核是最新的。参阅<a href="http://docs.ceph.org.cn/start/os-recommendations/" target="_blank" rel="external">操作系统推荐</a>里关于 glibc 和 syncfs(2) 的部分，确保硬件性能可达期望值。<br>OSD 数量较多（如 20 个以上）的主机会派生出大量线程，尤其是在恢复和重均衡期间。很多 Linux 内核默认的最大线程数较小（如 32k 个），如果您遇到了这类问题，可以把 kernel.pid_max 值调高些。理论最大值是 4194303 。例如把下列这行加入 /etc/sysctl.conf 文件：<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">kernel.pid_max = 4194303</div></pre></td></tr></table></figure>
</li>
</ul>
<p><b>结论: OSDs建议最小1TB,Ceph 最佳实践指示，应该分别在单独的硬盘运行操作系统、 OSD 数据和 OSD 日志</b></p>
<h4 id="网络"><a href="#网络" class="headerlink" title="网络"></a><b>网络</b></h4><p>建议每台机器最少两个千兆网卡，现在大多数机械硬盘都能达到大概 100MB/s 的吞吐量，网卡应该能处理所有 OSD 硬盘总吞吐量，所以推荐最少两个千兆网卡，分别用于公网（前端）和集群网络（后端）。集群网络（最好别连接到国际互联网）用于处理由数据复制产生的额外负载，而且可防止拒绝服务攻击，拒绝服务攻击会干扰数据归置组，使之在 OSD 数据复制时不能回到 active + clean 状态。请考虑部署万兆网卡。通过 1Gbps 网络复制 1TB 数据耗时 3 小时，而 3TB （典型配置）需要 9 小时，相比之下，如果使用 10Gbps 复制时间可分别缩减到 20 分钟和 1 小时。在一个 PB 级集群中， OSD 磁盘失败是常态，而非异常；在性价比合理的的前提下，系统管理员想让 PG 尽快从 degraded （降级）状态恢复到 active + clean 状态。另外，一些部署工具（如 Dell 的 Crowbar ）部署了 5 个不同的网络，但使用了 VLAN 以提高网络和硬件可管理性。 VLAN 使用 802.1q 协议，还需要采用支持 VLAN 功能的网卡和交换机，增加的硬件成本可用节省的运营（网络安装、维护）成本抵消。使用 VLAN 来处理集群和计算栈（如 OpenStack 、 CloudStack 等等）之间的 VM 流量时，采用 10G 网卡仍然值得。每个网络的机架路由器到核心路由器应该有更大的带宽，如 40Gbps 到 100Gbps 。</p>
<p>服务器应配置底板管理控制器（ Baseboard Management Controller, BMC ），管理和部署工具也应该大规模使用 BMC ，所以请考虑带外网络管理的成本/效益平衡，此程序管理着 SSH 访问、 VM 映像上传、操作系统安装、端口管理、等等，会徒增网络负载。运营 3 个网络有点过分，但是每条流量路径都指示了部署一个大型数据集群前要仔细考虑的潜能力、吞吐量、性能瓶颈</p>
<p><b>结论:每台机器最少是两块前兆网卡（物理交换机也需要隔离）,最少是10Gbps在机架</b></p>
<h4 id="故障域"><a href="#故障域" class="headerlink" title="故障域"></a><b>故障域</b></h4><p>故障域指任何导致不能访问一个或多个 OSD 的故障，可以是主机上停止的进程、硬盘故障、操作系统崩溃、有问题的网卡、损坏的电源、断网、断电等等。规划硬件需求时，要在多个需求间寻求平衡点，像付出很多努力减少故障域带来的成本削减、隔离每个潜在故障域增加的成本。</p>
<h4 id="最低硬件推荐"><a href="#最低硬件推荐" class="headerlink" title="最低硬件推荐"></a><b>最低硬件推荐</b></h4><ul>
<li><p>Dell 实例: Ceph 集群项目(2012年)使用了 2 个相当强悍的 OSD 硬件配置，和稍逊的监视器配置</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">|Configuration	|Criteria      	|Minimum Recommended</div><div class="line">|---		|---		|---</div><div class="line">|Dell PE R510  	|Processor	|2x 64-bit quad-core Xeon CPUs</div><div class="line">|		|RAM		|16 GB</div><div class="line">|		|Volume Storage	|8x 2TB drives. 1 OS, 7 Storage</div><div class="line">|		|Client Network	|2x 1GB Ethernet NICs</div><div class="line">|		|OSD Network	|2x 1GB Ethernet NICs</div><div class="line">|		|Mgmt. Network	|2x 1GB Ethernet NICs</div><div class="line">|Dell PE R515   |Processor	|1x hex-core Opteron CPU</div><div class="line">|		|RAM		|16 GB</div><div class="line">|		|Volume Storage	|12x 3TB drives. Storage</div><div class="line">|		|OS Storage	|1x 500GB drive. Operating System.</div><div class="line">|		|Client Network	|2x 1GB Ethernet NICs</div><div class="line">|		|OSD Network	|2x 1GB Ethernet NICs</div><div class="line">|		|Mgmt. Network	|2x 1GB Ethernet NICs</div></pre></td></tr></table></figure>
</li>
<li><p>低配(RedHat提供)<br>1.osd</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">|Criteria	|Minimum Recommended|</div><div class="line">|: --:		|:–		|</div><div class="line">|Processor	|1x AMD64 and Intel 64|</div><div class="line">|RAM 		|2 GB of RAM per deamon|</div><div class="line">|Volume Storage	|1x storage drive per daemon|</div><div class="line">|Journal	|1x SSD partition per daemon (optional)|</div><div class="line">|Network	|2x 1GB Ethernet NICs|</div></pre></td></tr></table></figure>
<p>  2.mon</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">| Criteria	| Minimum Recommended|</div><div class="line">| : --:		| :–		|</div><div class="line">|Processor	| 1x AMD64 and Intel 64|</div><div class="line">|RAM 		| 1 GB of RAM per deamon|</div><div class="line">|Disk Space	| 10 GB per daemon|</div><div class="line">| Network	| 2x 1GB Ethernet NICs|</div></pre></td></tr></table></figure>
</li>
<li><p>高配(Intel提供)<br>1.osd –Good</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">|Criteria	|Minimum Recommended|</div><div class="line">|: --:		|:–		|</div><div class="line">|Processor	|Inter(R) Xeon(R)CPU E5-2650v3|</div><div class="line">|RAM 		|64GB |</div><div class="line">|Volume Storage	|1x1.6TB P3700+12x4TB HDDs(1:12 ratio) P3700 as Journal and caching|</div><div class="line">|Caching software	|Intel(R) CAS3.0,option: Intel(R) RSTe/MD4.3|</div><div class="line">|Network	|10Gbe|</div></pre></td></tr></table></figure>
<p>  2.osd –Better</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">|Criteria	|Minimum Recommended|</div><div class="line">|: --:		|:–		|</div><div class="line">|Processor	|Inter(R) Xeon(R)CPU E5-2690|</div><div class="line">|RAM 		|128GB |</div><div class="line">|Volume Storage	|1xIntel(R) DC P3700(800G)+4xIntel(R) DC S3510 1.6TB|</div><div class="line">|Network	|Duel 10Gbe|</div></pre></td></tr></table></figure>
<p>  3.osd –Best</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">|Criteria	|Minimum Recommended|</div><div class="line">|: --:		|:–		|</div><div class="line">|Processor	|Inter(R) Xeon(R)CPU E5-2699v3|</div><div class="line">|RAM 		|&gt;=128GB |</div><div class="line">|Volume Storage	|4 to 6xIntel(R)DC P3700 2TB|</div><div class="line">|Network	|2x 40GbE,4xdual 10GbE|</div></pre></td></tr></table></figure>
</li>
<li><p>vpsee生产实例</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">| Hostname  | IP Address    | Role  |                                           Hardware Info |</div><div class="line">|-----------+---------------+-------|---------------------------------------------------------|</div><div class="line">| ceph-adm  | 192.168.2.100 | adm   |                             2 Cores, 4GB RAM, 20GB DISK |</div><div class="line">| ceph-mon1 | 192.168.2.101 | mon   |                         24 Cores，64GB RAM, 2x750GB SAS |</div><div class="line">| ceph-mon2 | 192.168.2.102 | mon   |                         24 Cores，64GB RAM, 2x750GB SAS |</div><div class="line">| ceph-mon3 | 192.168.2.103 | mon   |                         24 Cores，64GB RAM, 2x750GB SAS |</div><div class="line">| ceph-osd1 | 192.168.2.121 | osd   | 12 Cores，64GB RAM, 10x4TB SAS，2x400GB SSD，2x80GB SSD |</div><div class="line">| ceph-osd2 | 192.168.2.122 | osd   | 12 Cores，64GB RAM, 10x4TB SAS，2x400GB SSD，2x80GB SSD |</div></pre></td></tr></table></figure>
</li>
<li><p>ADM 服务器硬件配置比较随意，用1台低配置的虚拟机就可以了，只是用来操作和管理 Ceph；</p>
</li>
<li>MON 服务器2块硬盘做成 RAID1，用来安装操作系统；</li>
<li>OSD 服务器上用10块 4TB 硬盘做 Ceph 存储，每个 osd 对应1块硬盘，每个 osd 需要1个 Journal，所以10块硬盘需要10个 Journal，我们用2块大容量 SSD 硬盘做 journal，每个 SSD 等分成5个区，这样每个区分别对应一个 osd 硬盘的 journal，剩下的2块小容量 SSD 装操作系统，采用 RAID1.</li>
</ul>
<p>ref<br><a href="http://docs.ceph.org.cn/start/hardware-recommendations/#id9" target="_blank" rel="external">硬件推荐</a><br><a href="https://github.com/lemonhall/node_note/blob/master/ceph%E7%9A%84%E7%A1%AC%E4%BB%B6%E9%85%8D%E7%BD%AE.txt" target="_blank" rel="external">ceph的硬件配置.txt</a><br><a href="http://www.vpsee.com/2015/07/install-ceph-on-centos-7/" target="_blank" rel="external">在 CentOS 7.1 上安装分布式存储系统 Ceph</a><br><a href="http://www.xuxiaopang.com/2016/10/10/ceph-full-install-el7-jewel/" target="_blank" rel="external">CEPH部署完整版(el7+jewel)</a><br><a href="http://www.xuxiaopang.com/2016/11/11/doc-ceph-table/" target="_blank" rel="external">Ceph常用表格汇总</a></p>
<hr>
<p>您的鼓励是我写作最大的动力</p>
<p>俗话说，投资效率是最好的投资。 如果您感觉我的文章质量不错，读后收获很大，预计能为您提高 10% 的工作效率，不妨小额捐助我一下，让我有动力继续写出更多好文章。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;cpu和内存&quot;&gt;&lt;a href=&quot;#cpu和内存&quot; class=&quot;headerlink&quot; title=&quot;cpu和内存&quot;&gt;&lt;/a&gt;&lt;b&gt;cpu和内存&lt;/b&gt;&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;CPU&lt;br&gt;metadata servers：属于CPU敏感型的角色服务器&lt;br
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>ceph维护命令手册</title>
    <link href="https://t1ger.github.io/2017/06/01/ceph%E7%BB%B4%E6%8A%A4%E5%91%BD%E4%BB%A4%E6%89%8B%E5%86%8C/"/>
    <id>https://t1ger.github.io/2017/06/01/ceph维护命令手册/</id>
    <published>2017-06-01T09:09:48.000Z</published>
    <updated>2017-07-03T08:23:31.324Z</updated>
    
    <content type="html"><![CDATA[<p>本文操作都是centos7环境</p>
<h5 id="crush-map-管理方法"><a href="#crush-map-管理方法" class="headerlink" title="crush map 管理方法"></a><b>crush map 管理方法</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">#获得默认 crushmap (加密)</div><div class="line">ceph osd getcrushmap -o crushmap.dump</div><div class="line"></div><div class="line">#转换 crushmap 格式 (加密 -&gt; 明文格式)</div><div class="line">crushtool -d crushmap.dump -o crushmap.txt</div><div class="line"></div><div class="line">#转换 crushmap 格式(明文 -&gt; 加密格式)</div><div class="line">crushtool -c crushmap.txt -o crushmap.done</div><div class="line"></div><div class="line">#重新使用新 crushmap</div><div class="line">ceph osd setcrushmap -i crushmap.done</div></pre></td></tr></table></figure>
<p>这里要说下,对crush map进行定义:<br>1 物理主机划分<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line">root default &#123;</div><div class="line">    id -1           # do not change unnecessarily</div><div class="line">    # weight 264.000</div><div class="line">    alg straw</div><div class="line">    hash 0  # rjenkins1</div><div class="line">    item 240.30.128.33 weight 12.000</div><div class="line">    item 240.30.128.32 weight 12.000</div><div class="line">    item 240.30.128.215 weight 12.000</div><div class="line">    item 240.30.128.209 weight 12.000</div><div class="line">    item 240.30.128.213 weight 12.000</div><div class="line">    item 240.30.128.214 weight 12.000</div><div class="line">    item 240.30.128.212 weight 12.000</div><div class="line">    item 240.30.128.211 weight 12.000</div><div class="line">    item 240.30.128.210 weight 12.000</div><div class="line">    item 240.30.128.208 weight 12.000</div><div class="line">    item 240.30.128.207 weight 12.000</div><div class="line">    item 240.30.128.63 weight 12.000</div><div class="line">    item 240.30.128.34 weight 12.000</div><div class="line">    item 240.30.128.35 weight 12.000</div><div class="line">    item 240.30.128.36 weight 12.000</div><div class="line">    item 240.30.128.37 weight 12.000</div><div class="line">    item 240.30.128.39 weight 12.000</div><div class="line">    item 240.30.128.38 weight 12.000</div><div class="line">    item 240.30.128.58 weight 12.000</div><div class="line">    item 240.30.128.59 weight 12.000</div><div class="line">    item 240.30.128.60 weight 12.000</div><div class="line">    item 240.30.128.29 weight 12.000</div><div class="line">&#125;</div><div class="line"></div><div class="line">root registry &#123;</div><div class="line">    id -26</div><div class="line">    # weight 36.000</div><div class="line">    alg straw</div><div class="line">    item 240.30.128.206 weight 12.000</div><div class="line">    item 240.30.128.40 weight 12.000</div><div class="line">    item 240.30.128.30 weight 12.000</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>说明:上面划分了两个物理区域</p>
<ul>
<li>root 区域, 包含了 264TB 空间</li>
<li>registry 区域,  包含了 36TB 空间<br>需要注意的问题:<br>建议在存放数据前就对物理池进行规划, 否则会出现大量数据迁移现象, 或者会出现 osd full 现象</li>
</ul>
<p>2.规则划分<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">rule replicated_ruleset &#123;</div><div class="line">    ruleset 0</div><div class="line">    type replicated</div><div class="line">    min_size 1</div><div class="line">    max_size 10</div><div class="line">    step take default</div><div class="line">    step chooseleaf firstn 0 type host</div><div class="line">    step emit</div><div class="line">&#125;</div><div class="line"></div><div class="line">rule registry_ruleset &#123;</div><div class="line">    ruleset 1</div><div class="line">    type replicated</div><div class="line">    min_size 2</div><div class="line">    max_size 3</div><div class="line">    step take registry</div><div class="line">    step chooseleaf firstn 0 type host</div><div class="line">    step emit</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h5 id="pool-创建-删除方法"><a href="#pool-创建-删除方法" class="headerlink" title="pool 创建, 删除方法"></a><b>pool 创建, 删除方法</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">#创建</div><div class="line"> ceph osd  pool  create  volumes 10240 10240</div><div class="line"> ceph osd  pool  create  paas 2048 2048</div><div class="line">#删除</div><div class="line"> ceph osd pool delete paas paas --yes-i-really-really-mean-it</div><div class="line">#查询</div><div class="line">[root@node1 ~]# ceph osd dump | grep replica</div><div class="line">#指定</div><div class="line"> ceph osd pool set paas crush_ruleset 1</div></pre></td></tr></table></figure>
<h5 id="OSD机器重启"><a href="#OSD机器重启" class="headerlink" title="OSD机器重启"></a><b>OSD机器重启</b></h5><p>1.设置noout 避免在异常情况下触发集群数据重新平衡<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ceph osd set noout</div></pre></td></tr></table></figure></p>
<p>2.关闭OSD机器上所有OSD进程<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">ceph osd down x    #把这个机器上的OSD都设置为down状态</div><div class="line">systemctl stop ceph-osd@* #用systemctl重启OSD进程</div></pre></td></tr></table></figure></p>
<p>3.重启OSD机器<br>4.恢复noout 设置<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ceph osd unset noout</div></pre></td></tr></table></figure></p>
<h5 id="扩容"><a href="#扩容" class="headerlink" title="扩容"></a><b>扩容</b></h5><ul>
<li>PB级的集群的容量超过50%，就要考虑扩容了。 假如OSD主机的磁盘容量为48TB（12 4TB），则需要backfill的数据为24TB（48TB 50%） ，假设网卡为10Gb，则新加一个OSD时，集群大约需要19200s（24TB/(10Gb/8)） 约3小时完成backfill，而backfill后台数据填充将会涉及大量的IO读和网络传输，必将影响生产业务运行。 如果集群容量到80%再扩容会导致更长的backfill时间，近8个小时。</li>
<li>OSD对应的磁盘利用率如果超过50%，也需要尽快扩容。<br>在业务闲时,向集群中增加OSD主机。</li>
</ul>
<h5 id="升级Ceph软件版本"><a href="#升级Ceph软件版本" class="headerlink" title="升级Ceph软件版本"></a><b>升级Ceph软件版本</b></h5><p>1.在MON和OSD机器上升级安装指定的ceph版本的软件包<br>2.逐个重启MON进程<br>3.设置noout 避免在异常情况下触发集群数据重新平衡<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ceph osd set noout</div></pre></td></tr></table></figure></p>
<p>4.逐个重启OSD进程<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">ceph osd down x        #提前mark down， 减少slow request</div><div class="line">systemctl restart ceph-osd@x #用systemctl重启OSD进程</div></pre></td></tr></table></figure></p>
<p>5.恢复noout 设置<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ceph osd unset noout</div></pre></td></tr></table></figure></p>
<h5 id="ceph的启动和停止"><a href="#ceph的启动和停止" class="headerlink" title="ceph的启动和停止"></a><b>ceph的启动和停止</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"># 列出节点上所有的units</div><div class="line">sudo systemctl status ceph\*.service ceph\*.target</div><div class="line"># 启动所有的守护进程</div><div class="line">sudo systemctl start ceph.target</div><div class="line"># 停止所有守护进程</div><div class="line">sudo systemctl stop ceph\*.service ceph\*.target</div><div class="line"></div><div class="line"># 按类型启动所有守护进程</div><div class="line">sudo systemctl start ceph-osd.target</div><div class="line">sudo systemctl start ceph-mon.target</div><div class="line">sudo systemctl start ceph-mds.target</div><div class="line"></div><div class="line"># 停止ceph节点上某一类守护进程</div><div class="line">sudo systemctl stop ceph-mon\*.service ceph-mon.target</div><div class="line">sudo systemctl stop ceph-osd\*.service ceph-osd.target</div><div class="line">sudo systemctl stop ceph-mds\*.service ceph-mds.target</div><div class="line"></div><div class="line"># 启动某节点上一个特定的守护进程例程</div><div class="line">sudo systemctl start ceph-osd@&#123;id&#125;</div><div class="line">sudo systemctl start ceph-mon@&#123;hostname&#125;</div><div class="line">sudo systemctl start ceph-mds@&#123;hostname&#125;</div><div class="line"></div><div class="line"># 停止某节点上一个特定的守护进程例程</div><div class="line">sudo systemctl stop ceph-osd@&#123;id&#125;</div><div class="line">sudo systemctl stop ceph-mon@&#123;hostname&#125;</div><div class="line">sudo systemctl stop ceph-mds@&#123;hostname&#125;</div></pre></td></tr></table></figure>
<p>ref<br><a href="https://forest.gitbooks.io/ceph-practice/content/operation.html" target="_blank" rel="external">维护操作</a><br><a href="https://forest.gitbooks.io/ceph-practice/content/troubleshoot.html" target="_blank" rel="external">故障定位和处理</a><br><a href="https://yq.aliyun.com/articles/70814" target="_blank" rel="external">ceph - crush map 与 pool</a></p>
<hr>
<p>您的鼓励是我写作最大的动力</p>
<p>俗话说，投资效率是最好的投资。 如果您感觉我的文章质量不错，读后收获很大，预计能为您提高 10% 的工作效率，不妨小额捐助我一下，让我有动力继续写出更多好文章。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文操作都是centos7环境&lt;/p&gt;
&lt;h5 id=&quot;crush-map-管理方法&quot;&gt;&lt;a href=&quot;#crush-map-管理方法&quot; class=&quot;headerlink&quot; title=&quot;crush map 管理方法&quot;&gt;&lt;/a&gt;&lt;b&gt;crush map 管理方法&lt;/b&gt;
    
    </summary>
    
    
  </entry>
  
</feed>
