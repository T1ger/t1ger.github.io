<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>t1ger的茶馆</title>
  <subtitle>头顶有光终是幻，足下生云未是仙</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://t1ger.github.io/"/>
  <updated>2018-11-22T09:44:29.757Z</updated>
  <id>https://t1ger.github.io/</id>
  
  <author>
    <name>t1ger</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>How to install ModSecurity with Nginx</title>
    <link href="https://t1ger.github.io/2018/11/22/How-to-install-ModSecurity-with-Nginx/"/>
    <id>https://t1ger.github.io/2018/11/22/How-to-install-ModSecurity-with-Nginx/</id>
    <published>2018-11-22T08:45:00.000Z</published>
    <updated>2018-11-22T09:44:29.757Z</updated>
    
    <content type="html"><![CDATA[<p>本文不打算写安装过程，具体安装参考<a href="https://github.com/SpiderLabs/ModSecurity/wiki/Compilation-recipes-for-v3.x" target="_blank" rel="noopener">这里</a><br>安装环境： centos6.8 </p>
<p>安装前准备,需要安装gcc4.8 和 更新autoconf，参考<a href="https://gist.github.com/tkuchiki/543e277a2f7221a7833a" target="_blank" rel="noopener">这里</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost opt]# curl -L -O http://ftp.gnu.org/gnu/autoconf/autoconf-2.69.tar.gz</span><br><span class="line">[root@localhost opt]# tar zxf autoconf-2.69.tar.gz</span><br><span class="line">[root@localhost opt]# cd autoconf-2.69</span><br><span class="line">[root@localhost opt]# yum install -y openssl-devel</span><br><span class="line">[root@localhost opt]# ./configure</span><br><span class="line">[root@localhost opt]# make &amp;&amp; make install</span><br><span class="line"></span><br><span class="line">[root@localhost opt]#  wget http://people.centos.org/tru/devtools-2/devtools-2.repo -O /etc/yum.repos.d/devtools-2.repo</span><br><span class="line">[root@localhost opt]#  yum install -y devtoolset-2-gcc-c++ devtoolset-2-binutils</span><br><span class="line">[root@localhost opt]#  source /opt/rh/devtoolset-2/enable</span><br></pre></td></tr></table></figure></p>
<p>规则下载<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cd /opt</span><br><span class="line">git clone https://github.com/SpiderLabs/owasp-modsecurity-crs.git</span><br><span class="line">cd owasp-modsecurity-crs/</span><br><span class="line">cp crs-setup.conf.example /usr/local/openresty/nginx/conf/crs-setup.conf</span><br><span class="line">cd rules</span><br><span class="line">cp REQUEST-900-EXCLUSION-RULES-BEFORE-CRS.conf.example REQUEST-900-EXCLUSION-RULES-BEFORE-CRS.conf</span><br><span class="line">cp RESPONSE-999-EXCLUSION-RULES-AFTER-CRS.conf.example RESPONSE-999-EXCLUSION-RULES-AFTER-CRS.conf</span><br></pre></td></tr></table></figure></p>
<p>添加modsecurity配置<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">cp /opt/ModSecurity/modsecurity.conf-recommended /usr/local/openresty/nginx/conf/modsecurity.conf</span><br><span class="line">cp /opt/ModSecurity/unicode.mapping /usr/local/openresty/nginx/conf/unicode.mapping</span><br><span class="line">cp cp *.data /usr/local/openresty/nginx/conf/owasp/rules</span><br><span class="line">cp cp *.conf /usr/local/openresty/nginx/conf/owasp/rules</span><br><span class="line">vi modsecurity.conf</span><br><span class="line">最后添加</span><br><span class="line">Include /usr/local/openresty/nginx/conf/crs-setup.conf</span><br><span class="line">Include /usr/local/openresty/nginx/conf/owasp/rules/REQUEST-900-EXCLUSION-RULES-BEFORE-CRS.conf</span><br><span class="line">Include /usr/local/openresty/nginx/conf/owasp/rules/REQUEST-901-INITIALIZATION.conf</span><br><span class="line">Include /usr/local/openresty/nginx/conf/owasp/rules/REQUEST-905-COMMON-EXCEPTIONS.conf</span><br><span class="line">Include /usr/local/openresty/nginx/conf/owasp/rules/REQUEST-910-IP-REPUTATION.conf</span><br><span class="line">Include /usr/local/openresty/nginx/conf/owasp/rules/REQUEST-911-METHOD-ENFORCEMENT.conf</span><br><span class="line">Include /usr/local/openresty/nginx/conf/owasp/rules/REQUEST-912-DOS-PROTECTION.conf</span><br><span class="line">Include /usr/local/openresty/nginx/conf/owasp/rules/REQUEST-913-SCANNER-DETECTION.conf</span><br><span class="line">Include /usr/local/openresty/nginx/conf/owasp/rules/REQUEST-920-PROTOCOL-ENFORCEMENT.conf</span><br><span class="line">Include /usr/local/openresty/nginx/conf/owasp/rules/REQUEST-921-PROTOCOL-ATTACK.conf</span><br><span class="line">Include /usr/local/openresty/nginx/conf/owasp/rules/REQUEST-930-APPLICATION-ATTACK-LFI.conf</span><br><span class="line">Include /usr/local/openresty/nginx/conf/owasp/rules/REQUEST-931-APPLICATION-ATTACK-RFI.conf</span><br><span class="line">Include /usr/local/openresty/nginx/conf/owasp/rules/REQUEST-932-APPLICATION-ATTACK-RCE.conf</span><br><span class="line">Include /usr/local/openresty/nginx/conf/owasp/rules/REQUEST-933-APPLICATION-ATTACK-PHP.conf</span><br><span class="line">Include /usr/local/openresty/nginx/conf/owasp/rules/REQUEST-941-APPLICATION-ATTACK-XSS.conf</span><br><span class="line">Include /usr/local/openresty/nginx/conf/owasp/rules/REQUEST-942-APPLICATION-ATTACK-SQLI.conf</span><br><span class="line">Include /usr/local/openresty/nginx/conf/owasp/rules/REQUEST-943-APPLICATION-ATTACK-SESSION-FIXATION.conf</span><br><span class="line">Include /usr/local/openresty/nginx/conf/owasp/rules/REQUEST-949-BLOCKING-EVALUATION.conf</span><br><span class="line">Include /usr/local/openresty/nginx/conf/owasp/rules/RESPONSE-950-DATA-LEAKAGES.conf</span><br><span class="line">Include /usr/local/openresty/nginx/conf/owasp/rules/RESPONSE-951-DATA-LEAKAGES-SQL.conf</span><br><span class="line">Include /usr/local/openresty/nginx/conf/owasp/rules/RESPONSE-952-DATA-LEAKAGES-JAVA.conf</span><br><span class="line">Include /usr/local/openresty/nginx/conf/owasp/rules/RESPONSE-953-DATA-LEAKAGES-PHP.conf</span><br><span class="line">Include /usr/local/openresty/nginx/conf/owasp/rules/RESPONSE-954-DATA-LEAKAGES-IIS.conf</span><br><span class="line">Include /usr/local/openresty/nginx/conf/owasp/rules/RESPONSE-959-BLOCKING-EVALUATION.conf</span><br><span class="line">Include /usr/local/openresty/nginx/conf/owasp/rules/RESPONSE-980-CORRELATION.conf</span><br><span class="line">Include /usr/local/openresty/nginx/conf/owasp/rules/RESPONSE-999-EXCLUSION-RULES-AFTER-CRS.conf</span><br><span class="line">#禁用某个规则方法</span><br><span class="line">#SecRuleRemoveById 949110</span><br><span class="line">#SecRuleRemoveById 920420</span><br><span class="line">#SecRuleRemoveById 931100</span><br></pre></td></tr></table></figure></p>
<p>默认只是检测，不拦截，可以修改配置，将<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">SecRuleEngine DetectionOnly改为</span><br><span class="line">SecRuleEngine On</span><br></pre></td></tr></table></figure></p>
<p>编辑crs-setup.conf文件<br>默认ModSecurity不会阻挡恶意连接，只会记录在Log里。修改SecDefaultAction选项，默认开启阻挡<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ sed -ie &apos;s/SecDefaultAction &quot;phase:1,log,auditlog,pass&quot;/#SecDefaultAction &quot;phase:1,log,auditlog,pass&quot;/g&apos; crs-setup.conf</span><br><span class="line">$ sed -ie &apos;s/SecDefaultAction &quot;phase:2,log,auditlog,pass&quot;/#SecDefaultAction &quot;phase:2,log,auditlog,pass&quot;/g&apos; crs-setup.conf</span><br><span class="line">$ sed -ie &apos;s/#.*SecDefaultAction &quot;phase:1,log,auditlog,deny,status:403&quot;/SecDefaultAction &quot;phase:1,log,auditlog,deny,status:403&quot;/g&apos; crs-setup.conf</span><br><span class="line">$ sed -ie &apos;s/# SecDefaultAction &quot;phase:2,log,auditlog,deny,status:403&quot;/SecDefaultAction &quot;phase:2,log,auditlog,deny,status:403&quot;/g&apos; crs-setup.conf</span><br></pre></td></tr></table></figure></p>
<p>nginx中增加</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">modsecurity on;</span><br><span class="line">modsecurity_rules_file /usr/local/openresty/nginx/conf/modsecurity.conf;</span><br></pre></td></tr></table></figure>
<p>重启nginx<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service nginx restart</span><br></pre></td></tr></table></figure></p>
<p>打开modsecurity检测日志<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tail -f /var/log/modsec_audit.log</span><br></pre></td></tr></table></figure></p>
<p>测试Modsecurity</p>
<p>在浏览器中访问默认首页，会看到Nginx默认的欢迎页<br>这时我们在网址后面自己加上正常参数，例如： <a href="http://localhost/?id=1" target="_blank" rel="noopener">http://localhost/?id=1</a> 。同样会看到Nginx默认的欢迎页：<br>接下来，我们在前面正常参数的基础上再加上 AND 1=1 ,整个请求变成： <a href="http://localhost/?id=1" target="_blank" rel="noopener">http://localhost/?id=1</a> AND 1=1<br>就会看到Nginx返回403 Forbidden的信息了，说明Modsecurity成功拦截了此请求。</p>
<p>出现的错误：</p>
<ol>
<li>未更新autoconf,更新即可<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ModSecurity]# sh build.sh </span><br><span class="line">libtoolize: putting auxiliary files in `.&apos;.</span><br><span class="line">libtoolize: copying file `./ltmain.sh&apos;</span><br><span class="line">libtoolize: putting macros in AC_CONFIG_MACRO_DIR, `build&apos;.</span><br><span class="line">libtoolize: copying file `build/libtool.m4&apos;</span><br><span class="line">libtoolize: copying file `build/ltoptions.m4&apos;</span><br><span class="line">libtoolize: copying file `build/ltsugar.m4&apos;</span><br><span class="line">libtoolize: copying file `build/ltversion.m4&apos;</span><br><span class="line">libtoolize: copying file `build/lt~obsolete.m4&apos;</span><br><span class="line">/usr/bin/m4:configure.ac:209: bad expression in eval: m4_esyscmd_s(cat headers/modsecurity/modsecurity.h | grep &quot;define MODSECURITY_MAJOR &quot; | awk &#123;&apos;print &apos;&#125; | sed &apos;s/\&quot;//g&apos;) + m4_esyscmd_s(cat headers/modsecurity/modsecurity.h | grep &quot;define MODSECURITY_MINOR &quot; | awk &#123;&apos;print &apos;&#125; | sed &apos;s/\&quot;//g&apos;)</span><br><span class="line">autom4te: /usr/bin/m4 failed with exit status: 1</span><br><span class="line">aclocal: autom4te failed with exit status: 1</span><br><span class="line">autoreconf: aclocal failed with exit status: 1</span><br><span class="line">/usr/bin/m4:configure.ac:209: bad expression in eval: m4_esyscmd_s(cat headers/modsecurity/modsecurity.h | grep &quot;define MODSECURITY_MAJOR &quot; | awk &#123;&apos;print &apos;&#125; | sed &apos;s/\&quot;//g&apos;) + m4_esyscmd_s(cat headers/modsecurity/modsecurity.h | grep &quot;define MODSECURITY_MINOR &quot; | awk &#123;&apos;print &apos;&#125; | sed &apos;s/\&quot;//g&apos;)</span><br><span class="line">autom4te: /usr/bin/m4 failed with exit status: 1</span><br><span class="line">autoheader: &apos;/usr/bin/autom4te&apos; failed with exit status: 1</span><br><span class="line">/usr/bin/m4:configure.ac:209: bad expression in eval: m4_esyscmd_s(cat headers/modsecurity/modsecurity.h | grep &quot;define MODSECURITY_MAJOR &quot; | awk &#123;&apos;print &apos;&#125; | sed &apos;s/\&quot;//g&apos;) + m4_esyscmd_s(cat headers/modsecurity/modsecurity.h | grep &quot;define MODSECURITY_MINOR &quot; | awk &#123;&apos;print &apos;&#125; | sed &apos;s/\&quot;//g&apos;)</span><br><span class="line">autom4te: /usr/bin/m4 failed with exit status: 1</span><br><span class="line">automake: autoconf failed with exit status: 1</span><br><span class="line">/usr/bin/m4:configure.ac:209: bad expression in eval: m4_esyscmd_s(cat headers/modsecurity/modsecurity.h | grep &quot;define MODSECURITY_MAJOR &quot; | awk &#123;&apos;print &apos;&#125; | sed &apos;s/\&quot;//g&apos;) + m4_esyscmd_s(cat headers/modsecurity/modsecurity.h | grep &quot;define MODSECURITY_MINOR &quot; | awk &#123;&apos;print &apos;&#125; | sed &apos;s/\&quot;//g&apos;)</span><br><span class="line">autom4te: /usr/bin/m4 failed with exit status: 1</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>ref</p>
<p><a href="https://docs.nginx.com/nginx-waf/admin-guide/nginx-plus-modsecurity-waf-owasp-crs/" target="_blank" rel="noopener">Using the OWASP CRS with NGINX WAF</a><br><a href="https://fossies.org/linux/owasp-modsecurity-crs/INSTALL" target="_blank" rel="noopener">owasp-modsecurity-crs-3.0.2/INSTALL</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文不打算写安装过程，具体安装参考&lt;a href=&quot;https://github.com/SpiderLabs/ModSecurity/wiki/Compilation-recipes-for-v3.x&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;这里&lt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>How to config Kafka SASL zookeeper authentication</title>
    <link href="https://t1ger.github.io/2018/11/21/How-to-config-Kafka-SASL-zookeeper-authentication/"/>
    <id>https://t1ger.github.io/2018/11/21/How-to-config-Kafka-SASL-zookeeper-authentication/</id>
    <published>2018-11-21T09:06:00.000Z</published>
    <updated>2018-11-22T09:56:59.367Z</updated>
    
    <content type="html"><![CDATA[<p>单节点zookeeper配置<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost kafka]# cat /etc/zookeeper/zoo.cfg </span><br><span class="line"># The number of milliseconds of each tick</span><br><span class="line">tickTime=2000</span><br><span class="line"># The number of ticks that the initial </span><br><span class="line"># synchronization phase can take</span><br><span class="line">initLimit=10</span><br><span class="line"># The number of ticks that can pass between </span><br><span class="line"># sending a request and getting an acknowledgement</span><br><span class="line">syncLimit=5</span><br><span class="line"># the directory where the snapshot is stored.</span><br><span class="line">dataDir=/var/lib/zookeeper/data</span><br><span class="line"># the port at which the clients will connect</span><br><span class="line">clientPort=2181</span><br><span class="line"></span><br><span class="line">authProvider.1=org.apache.zookeeper.server.auth.SASLAuthenticationProvider</span><br><span class="line">requireClientAuthScheme=sasl</span><br><span class="line">jaasLoginRenew=3600000</span><br></pre></td></tr></table></figure></p>
<p>JAAS配置文件指定身份认证插件，并提供登录到zookeeper的登录名与密码。我创建了一个配置文件kafka_zoo_jaas.conf<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Server &#123;</span><br><span class="line">  org.apache.kafka.common.security.plain.PlainLoginModule required</span><br><span class="line">    username=&quot;admin&quot;</span><br><span class="line">    password=&quot;admin&quot;</span><br><span class="line">    user_admin=&quot;admin&quot;;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p>
<p>拷贝第三方Jar包到ZooKeeper目录,可以从<a href="https://mvnrepository.com/" target="_blank" rel="noopener">https://mvnrepository.com/</a> 查看需要的包依赖<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">kafka-clients-2.0.1.jar </span><br><span class="line">lz4-java-1.4.jar</span><br><span class="line">org.osgi.core-4.3.0.jar</span><br><span class="line">slf4j-api-1.7.25.jar</span><br><span class="line">snappy-java-1.1.4.jar</span><br></pre></td></tr></table></figure></p>
<p> /usr/local/kafka_2.12-1.1.1/config 目录下的   server.properties 文件 进行修改</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">listeners=SASL_PLAINTEXT://172.16.10.112:9092</span><br><span class="line">security.inter.broker.protocol=SASL_PLAINTEXT  </span><br><span class="line">sasl.enabled.mechanisms=PLAIN </span><br><span class="line">sasl.mechanism.inter.broker.protocol=PLAIN  </span><br><span class="line">authorizer.class.name=kafka.security.auth.SimpleAclAuthorizer</span><br><span class="line">allow.everyone.if.no.acl.found=true</span><br><span class="line">auto.create.topics.enable=false</span><br><span class="line">super.users=User:admin</span><br><span class="line">delete.topic.enable=true</span><br></pre></td></tr></table></figure>
<p>在config目录添加kafka_server_jaas.conf<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">KafkaServer &#123;</span><br><span class="line">    org.apache.kafka.common.security.plain.PlainLoginModule required</span><br><span class="line">    username=&quot;admin&quot;</span><br><span class="line">    password=&quot;admin&quot;</span><br><span class="line">    user_admin=&quot;admin&quot;</span><br><span class="line">    user_kafka=&quot;kafka&quot;;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Client &#123;</span><br><span class="line">    org.apache.kafka.common.security.plain.PlainLoginModule required</span><br><span class="line">        username=&quot;admin&quot;</span><br><span class="line">        password=&quot;admin&quot;;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">KafkaClient &#123;</span><br><span class="line">    org.apache.kafka.common.security.plain.PlainLoginModule required</span><br><span class="line">            username=&quot;kafka&quot;</span><br><span class="line">            password=&quot;kafka&quot;;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p>
<p>kafka-run-class.sh添加下面配置</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">KAFKA_SASL_OPTS=&quot;-Djava.security.auth.login.config=/opt/kafka/config/kafka_server_jaas.conf&quot;</span><br><span class="line"></span><br><span class="line"># Launch mode</span><br><span class="line">if [ &quot;x$DAEMON_MODE&quot; = &quot;xtrue&quot; ]; then</span><br><span class="line">  nohup $JAVA $KAFKA_HEAP_OPTS $KAFKA_JVM_PERFORMANCE_OPTS $KAFKA_GC_LOG_OPTS $KAFKA_SASL_OPTS $KAFKA_JMX_OPTS $KAFKA_LOG4J_OPTS -cp $CLASSPATH $KAFKA_OPTS &quot;$@&quot; &gt; &quot;$CONSOLE_OUTPUT_FILE&quot; 2&gt;&amp;1 &lt; /dev/null &amp;</span><br><span class="line">else</span><br><span class="line">  exec $JAVA $KAFKA_HEAP_OPTS $KAFKA_JVM_PERFORMANCE_OPTS $KAFKA_GC_LOG_OPTS $KAFKA_SASL_OPTS $KAFKA_JMX_OPTS $KAFKA_LOG4J_OPTS -cp $CLASSPATH $KAFKA_OPTS &quot;$@&quot;</span><br><span class="line">fi</span><br></pre></td></tr></table></figure>
<p>kafka已经开启了SASL/PLAIN权限认证,producer和consumer不做配置是无法连接kafka的<br>在config目录下创建kafka_client_jaas.conf<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">KafkaClient &#123;</span><br><span class="line">        org.apache.kafka.common.security.plain.PlainLoginModule required</span><br><span class="line">        username=&quot;kafka&quot;</span><br><span class="line">        password=&quot;kafka&quot;;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p>
<p>在config下的producer.properties和consumer.properties添加下面配置：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">security.protocol=SASL_PLAINTEXT</span><br><span class="line">sasl.mechanism=PLAIN</span><br></pre></td></tr></table></figure></p>
<p>在bin下的kafka-console-producer.sh和kafka-console-consumer.sh下添加下面配置：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">if [ &quot;x$KAFKA_OPTS&quot; ]; then</span><br><span class="line"> export KAFKA_OPTS=&quot;-Djava.security.auth.login.config=/opt/kafka/config/kafka_client_jaas.conf&quot;</span><br><span class="line">fi</span><br></pre></td></tr></table></figure></p>
<p>为你需要使用的用户授权<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/kafka-acls.sh --authorizer-properties zookeeper.connect=localhost:2181/kafka --add --allow-principal User:alice --operation Read --operation Write --topic sean-security</span><br></pre></td></tr></table></figure></p>
<p>查询已经授权的用户</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/kafka-acls.sh --authorizer-properties zookeeper.connect=localhost:2181/kafka --list --topic sean-security</span><br></pre></td></tr></table></figure>
<p>配置完成后，测试下,注意如果有开机自启,需要修改相应脚本,加载KAFKA_OPTS 选项<br>创建topic<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./kafka-topics.sh  --zookeeper localhost:2181/kafka --create --topic Test  --partitions 1 --replication-factor 1</span><br></pre></td></tr></table></figure></p>
<p>producer发送消息：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">./kafka-console-producer.sh --broker-list 172.16.10.100:9092 --topic test</span><br><span class="line">--producer.config /opt/kafk/config/producer.properties</span><br></pre></td></tr></table></figure></p>
<p>consumer消费消息：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">./kafka-console-consumer.sh --bootstrap-server 172.16.10.100:9092 --topic test --from-beginning</span><br><span class="line"> --consumer.config /opt/kafk/config/consumer.properties</span><br></pre></td></tr></table></figure></p>
<p>如果是在程序里面只需要在kafka配置信息里添加如下配置：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">System.setProperty(&quot;java.security.auth.login.config&quot;, &quot;kafka_client_jaas.conf&quot;); // 环境变量添加，需要输入配置文件的路径</span><br><span class="line">propsMap.put(&quot;security.protocol&quot;,&quot;SASL_PLAINTEXT&quot;);</span><br><span class="line">propsMap.put(&quot;sasl.mechanism&quot;, &quot;PLAIN&quot;);</span><br></pre></td></tr></table></figure></p>
<p>无论是producer还是consumer，都需要配置认证信息，其中kafka_client_jaas.conf的内容如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">KafkaClient &#123;</span><br><span class="line">        org.apache.kafka.common.security.plain.PlainLoginModule required</span><br><span class="line">        username=&quot;kafka&quot;</span><br><span class="line">        password=&quot;kafka&quot;;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p>
<p>ref<br><a href="http://kafka.apache.org/documentation/" target="_blank" rel="noopener">Documentation</a><br><a href="https://blog.csdn.net/u012040869/article/details/82589002?utm_source=blogkpcl4" target="_blank" rel="noopener">为Kafka 添加用户名 密码权限，即SASL/PLAIN</a><br><a href="http://www.2bowl.info/kafka%E7%9A%84saslplain%E8%AE%A4%E8%AF%81%E9%85%8D%E7%BD%AE%E8%AF%B4%E6%98%8E/" target="_blank" rel="noopener">Kafka的SASL/PLAIN认证配置说明</a><br><a href="https://stackoverflow.com/questions/43469962/kafka-sasl-zookeeper-authentication" target="_blank" rel="noopener">Kafka SASL zookeeper authentication</a><br><a href="https://blog.csdn.net/u012842205/article/details/73188534?utm_source=blogxgwz0" target="_blank" rel="noopener">单机节点Kafka配置SASL用户名密码认证</a><br><a href="https://blog.csdn.net/u010416101/article/details/79562214" target="_blank" rel="noopener">Kafka SASL配置 &amp; Demo测试</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;单节点zookeeper配置&lt;br&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>How to install Cabot on CentOS</title>
    <link href="https://t1ger.github.io/2018/11/14/How-to-install-Cabot-on-CentOS/"/>
    <id>https://t1ger.github.io/2018/11/14/How-to-install-Cabot-on-CentOS/</id>
    <published>2018-11-14T09:56:36.000Z</published>
    <updated>2018-11-15T06:11:12.188Z</updated>
    
    <content type="html"><![CDATA[<p>Cabot环境搭建</p>
<p>初始化mysql数据库<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ mysql -u root -p -e &quot;CREATE USER cabot@localhost IDENTIFIED BY &apos;cabot&apos;&quot;;</span><br><span class="line">$ mysql -u root -p -e &quot;CREATE DATABASE cabot&quot;;</span><br><span class="line">$ mysql -u root -p -e &quot;GRANT ALL PRIVILEGES ON \`cabot\`.* TO \`cabot\`@localhost&quot;;</span><br></pre></td></tr></table></figure></p>
<p>下载cabot</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/arachnys/cabot.git</span><br></pre></td></tr></table></figure>
<p>安装依赖软件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ sudo yum install ruby</span><br><span class="line">$ sudo gem install foreman</span><br><span class="line">$ sudo pip install fabric</span><br><span class="line">$ sudo yum install python-devel</span><br><span class="line">$ sudo yum install openldap-devel</span><br><span class="line">$ sudo pip install -r requirements.txt </span><br><span class="line">$ sudo pip install -r requirements-plugins.txt </span><br><span class="line">$ sudo pip install -r requirements-dev.txt</span><br><span class="line">$ sudo pip install MySQL-python</span><br><span class="line">$ sudo yum install nodejs</span><br><span class="line">$ sudo npm install -g less</span><br><span class="line">$ sudo npm install -g coffee-script</span><br></pre></td></tr></table></figure></p>
<p>修改配置文件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">$ cp production.env.example production.env</span><br><span class="line">$ vi production.env</span><br><span class="line">DEBUG=t</span><br><span class="line">DATABASE_URL=mysql://cabot:cabot@localhost:3306/cabot</span><br><span class="line">DJANGO_SETTINGS_MODULE=cabot.settings</span><br><span class="line">LOG_FILE=log</span><br><span class="line">PORT=5008</span><br><span class="line"></span><br><span class="line"># Local time zone for this installation. Choices can be found here:</span><br><span class="line"># http://en.wikipedia.org/wiki/List_of_tz_zones_by_name</span><br><span class="line">TIME_ZONE=Asia/Shanghai</span><br><span class="line"></span><br><span class="line"># Django settings</span><br><span class="line">CELERY_BROKER_URL=redis://localhost:6379/1</span><br><span class="line">DJANGO_SECRET_KEY=2FL6ORhHwr5eX34pP9mMugnIOd3jzVuT45f7w430Mt5PnEwbcJgma0q8zUXNZ68A</span><br><span class="line"></span><br><span class="line"># Hostname of your Graphite server instance</span><br><span class="line">GRAPHITE_API=http://*.*.*.*:12346/</span><br><span class="line">GRAPHITE_USER=username</span><br><span class="line">GRAPHITE_PASS=password</span><br></pre></td></tr></table></figure></p>
<p>修改启动文件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">$ vi .foreman</span><br><span class="line"></span><br><span class="line"># vi: set ft=yaml :</span><br><span class="line">procfile: Procfile</span><br><span class="line">env: conf/production.env</span><br><span class="line"></span><br><span class="line">$ vi gunicorn.conf</span><br><span class="line"></span><br><span class="line"># -*- mode: python -*-</span><br><span class="line"># vi: set ft=python :</span><br><span class="line">import os</span><br><span class="line">bind = &apos;0.0.0.0:%s&apos; % os.environ[&apos;PORT&apos;]</span><br><span class="line">workers = 3</span><br><span class="line"></span><br><span class="line">$ vi Procfile</span><br><span class="line">web:       gunicorn cabot.wsgi:application --config gunicorn.conf</span><br><span class="line">celery:    celery worker -B -A cabot --loglevel=INFO --concurrency=16 -Ofair</span><br><span class="line">beat:      celery beat -A cabot --loglevel=INFO</span><br></pre></td></tr></table></figure></p>
<p>初始化数据库<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sh setup_dev.sh</span><br></pre></td></tr></table></figure></p>
<p>启动web程序<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ foreman start web</span><br></pre></td></tr></table></figure></p>
<p>启动celery<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">foreman start celery</span><br></pre></td></tr></table></figure></p>
<p>登录管理页面(第一次登录需要设置管理员账号)</p>
<p>使用supervisor管理Cabot,Celery进程<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">$ sudo vi /etc/supervisord.d/cabot.conf</span><br><span class="line"></span><br><span class="line">[program:web]</span><br><span class="line">command=foreman start web</span><br><span class="line">autorstart=true</span><br><span class="line">autorestart=true</span><br><span class="line">redirect_stderr=true</span><br><span class="line">stopsignal=TERM</span><br><span class="line">stdout_logfile=/var/log/web.log</span><br><span class="line">directory=/opt/cabot</span><br><span class="line"></span><br><span class="line">[program:celery]</span><br><span class="line">command=foreman start celery</span><br><span class="line">autorstart=true</span><br><span class="line">autorestart=true</span><br><span class="line">redirect_stderr=true</span><br><span class="line">stopsignal=TERM</span><br><span class="line">stdout_logfile=/var/log/celery.log</span><br><span class="line">directory=/opt/cabot</span><br></pre></td></tr></table></figure></p>
<p>使用supervisor管理程序<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo supervisorctl reload</span><br></pre></td></tr></table></figure></p>
<p>遇到的错误<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">1. &quot;Missing staticfiles manifest entry for &apos;%s&apos;&quot; % clean_name</span><br><span class="line">#解决方法</span><br><span class="line">python manage.py collectstatic</span><br><span class="line"></span><br><span class="line">2. CommandError: An error occurred during rendering /usr/local/whistle/webapps/cabot/cabot/templates/cabotapp/statuscheck_report.html:</span><br><span class="line"> /bin/sh: lessc: command not found</span><br><span class="line">#解决方法</span><br><span class="line">pip install nodeenv</span><br><span class="line">nodeenv nodeenv</span><br><span class="line">source nodeenv/bin/activate</span><br><span class="line">npm install -g less</span><br></pre></td></tr></table></figure></p>
<p>ref<br><a href="https://my.oschina.net/cdsc/blog/1480219" target="_blank" rel="noopener">Cabot Alert Mysql环境搭建</a><br><a href="https://github.com/ialbert/biostar-central/issues/193" target="_blank" rel="noopener">Cannot run Biostars out of the box: “lessc: command not found”</a><br><a href="https://stackoverflow.com/questions/44160666/valueerror-missing-staticfiles-manifest-entry-for-favicon-ico" target="_blank" rel="noopener">ValueError: Missing staticfiles manifest entry for ‘favicon.ico’</a></p>
<hr>
<p>您的鼓励是我写作最大的动力</p>
<p>俗话说，投资效率是最好的投资。 如果您感觉我的文章质量不错，读后收获很大，预计能为您提高 10% 的工作效率，不妨小额捐助我一下，让我有动力继续写出更多好文章。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Cabot环境搭建&lt;/p&gt;
&lt;p&gt;初始化mysql数据库&lt;br&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span c
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>mysql audit plugin</title>
    <link href="https://t1ger.github.io/2018/11/05/mysql-audit-plugin/"/>
    <id>https://t1ger.github.io/2018/11/05/mysql-audit-plugin/</id>
    <published>2018-11-05T09:40:06.000Z</published>
    <updated>2018-11-05T09:59:45.781Z</updated>
    
    <content type="html"><![CDATA[<p>此文转载于<a href="https://blog.csdn.net/heizistudio/article/details/50954294" target="_blank" rel="noopener">ora600</a>,略作调整.</p>
<p>下载地址如下<br><a href="http://pan.baidu.com/s/1dFGFCrv" target="_blank" rel="noopener">http://pan.baidu.com/s/1dFGFCrv</a></p>
<p>mysql5.6.X.tar.gz到mysql-5.7.8-rc.tar.gz是一个版本—-audit5_6_21.so<br>mysql5.7.1.tar.gz—mysql5.7.9.tar.gz是一个版本—-audit5_7_9.so<br>mysql5.7.10–mysql5.7.22是一个版本</p>
<p>一、查找插件所在位置<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; show variables like &apos;%plugin_dir%&apos;;</span><br><span class="line">+---------------+------------------------------+</span><br><span class="line">| Variable_name | Value                        |</span><br><span class="line">+---------------+------------------------------+</span><br><span class="line">| plugin_dir    | /usr/local/mysql/lib/plugin/ |</span><br><span class="line">+---------------+------------------------------+</span><br><span class="line">1 row in set (0.00 sec)</span><br><span class="line">---------------------</span><br></pre></td></tr></table></figure></p>
<p>二、将audit_版本号.so插件下载后放到plugin_dir位置<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mv audit_版本号.so  audit.so</span><br></pre></td></tr></table></figure></p>
<p>三、加载插件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; install plugin audit  SONAME &apos;audit.so&apos;;</span><br></pre></td></tr></table></figure></p>
<p>四、卸载插件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; uninstall plugin audit;</span><br></pre></td></tr></table></figure></p>
<p>使用插件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; show variables like &apos;%audit%&apos;;</span><br><span class="line">+----------------+----------------------+</span><br><span class="line">| Variable_name  | Value                |</span><br><span class="line">+----------------+----------------------+</span><br><span class="line">| audit_logfile  | /tmp/mysql_audit.log |</span><br><span class="line">| audit_myswitch | OFF                  |</span><br><span class="line">| audit_num      | 0                    |</span><br><span class="line">| audit_sql      | all_sql              |</span><br><span class="line">| audit_user     | all_user             |</span><br><span class="line">+----------------+----------------------+</span><br><span class="line">5 rows in set (0.01 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; set global audit_logfile=&apos;/tmp/mysql_audit_1.log&apos;;----只读变量，审计仅指定在/tmp/mysql_audit.log文件，保障权限可以写</span><br><span class="line">ERROR 1238 (HY000): Variable &apos;audit_logfile&apos; is a read only variable</span><br><span class="line"></span><br><span class="line">set global audit_sql=&apos;delete;select;drop&apos;;   -----这些审计关键字用;分开</span><br><span class="line">set global audit_user=&apos;user2;user3&apos;;         ----审计用户用;隔开</span><br><span class="line">set global audit_num =0;                          ----审计sql影响的最少行数，默认为0</span><br><span class="line">set global audit_myswitch=on|off|ON|OFF|1|0;       -----开启关闭审计</span><br></pre></td></tr></table></figure></p>
<p>查看日志linux下tailf /tmp/mysql_audit.log</p>
<p>ref<br><a href="https://blog.csdn.net/heizistudio/article/details/50954294" target="_blank" rel="noopener">mysql审计插件(运维不在背锅)</a><br><a href="https://dev.mysql.com/doc/refman/5.5/en/writing-audit-plugins.html" target="_blank" rel="noopener">Writing Audit Plugins</a><br><a href="https://github.com/mcafee/mysql-audit/wiki/Configuration" target="_blank" rel="noopener">mcafee/mysql-audit</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;此文转载于&lt;a href=&quot;https://blog.csdn.net/heizistudio/article/details/50954294&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;ora600&lt;/a&gt;,略作调整.&lt;/p&gt;
&lt;p&gt;下载地址如下&lt;b
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Spark On YARN</title>
    <link href="https://t1ger.github.io/2018/10/30/Spark-On-YARN/"/>
    <id>https://t1ger.github.io/2018/10/30/Spark-On-YARN/</id>
    <published>2018-10-30T07:27:47.000Z</published>
    <updated>2018-10-30T10:02:56.495Z</updated>
    
    <content type="html"><![CDATA[<h5 id="Deployment-Modes"><a href="#Deployment-Modes" class="headerlink" title="Deployment Modes"></a><b>Deployment Modes</b></h5><p>在YARN中，每个应用实例有一个ApplicationMaster进程，这是应用实例开启的第一个容器。 ResourceManager向ApplicationMaster申请资源，<br>在资源分配后，应用实例会通知NodeManagers去启动容器。ApplicationMasters 评估每个客户端的需要:进程启动的应用可以被中断，持续协调管理进程。</p>
<h5 id="Cluster-Deployment-Mode"><a href="#Cluster-Deployment-Mode" class="headerlink" title="Cluster Deployment Mode"></a><b>Cluster Deployment Mode</b></h5><p>在集群模式，Spark driver 运行在ApplicationMaster集群主机里边，在Yarn容器里的进程负责驱动应用和向YARN请求资源。<br>集群模式不适合交互<br><img src="https://www.cloudera.com/documentation/enterprise/latest/images/xspark-yarn-cluster.png.pagespeed.ic.f4CfMwda2i.webp" alt="cluster mode"></p>
<h5 id="Cluster-Deployment-Mode-1"><a href="#Cluster-Deployment-Mode-1" class="headerlink" title="Cluster Deployment Mode"></a><b>Cluster Deployment Mode</b></h5><p>在客户端模式，Spark driver运行在提交job的主机上,ApplicationMaster响应来自于Yarn容器的请求，在容器启动后，客户端和容器协调完成任务调度<br><img src="https://www.cloudera.com/documentation/enterprise/latest/images/xspark-yarn-client.png.pagespeed.ic.Nm0CUtnR01.webp" alt="client mode"></p>
<h5 id="Configuring-the-Environment"><a href="#Configuring-the-Environment" class="headerlink" title="Configuring the Environment"></a><b>Configuring the Environment</b></h5><p>Spark 需要配置 HADOOP_CONF_DIR or YARN_CONF_DIR 环境变量指向包含客户端目录的配置文件，这些配置文件用于写入HDFS和连接YARN ResourceManager.如果使用Cloudera Manager的部署客户端配置，这些变量会自动配置好.<br>否则在提交job会出现如下错误<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Exception in thread &quot;main&quot; java.lang.Exception: When running with master &apos;yarn&apos; either HADOOP_CONF_DIR or YARN_CONF_DIR must be set in the environment.</span><br><span class="line">    at org.apache.spark.deploy.SparkSubmitArguments.validateSubmitArguments(SparkSubmitArguments.scala:251)</span><br><span class="line">    at org.apache.spark.deploy.SparkSubmitArguments.validateArguments(SparkSubmitArguments.scala:228)</span><br><span class="line">    at org.apache.spark.deploy.SparkSubmitArguments.&lt;init&gt;(SparkSubmitArguments.scala:109)</span><br><span class="line">    at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:114)</span><br></pre></td></tr></table></figure></p>
<h5 id="Running-a-Spark-Shell-Application-on-YARN"><a href="#Running-a-Spark-Shell-Application-on-YARN" class="headerlink" title="Running a Spark Shell Application on YARN"></a><b>Running a Spark Shell Application on YARN</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#launch a Spark application in cluster mode</span><br><span class="line">[root@cdh2 admin]# cat job.sh </span><br><span class="line">sudo -uhdfs  spark-submit --class org.apache.spark.examples.SparkPi \</span><br><span class="line">    --verbose \</span><br><span class="line">    --master yarn \</span><br><span class="line">    --deploy-mode cluster\</span><br><span class="line">    --num-executors 3 \</span><br><span class="line">    --driver-memory 2g \</span><br><span class="line">    --executor-memory 512m \</span><br><span class="line">    --executor-cores 1 \</span><br><span class="line">    --queue thequeue \</span><br><span class="line">    /opt/cloudera/parcels/CDH-6.0.1-1.cdh6.0.1.p0.590678/lib/spark/examples/jars/spark-examples_2.11-2.2.0-cdh6.0.1.jar \</span><br><span class="line">    10 </span><br><span class="line"></span><br><span class="line">#run spark-shell in client mode:	</span><br><span class="line"> ./bin/spark-shell --master yarn --deploy-mode client</span><br></pre></td></tr></table></figure>
<h5 id="Spark-On-YARN相关的配置参数"><a href="#Spark-On-YARN相关的配置参数" class="headerlink" title="Spark On YARN相关的配置参数"></a><b>Spark On YARN相关的配置参数</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">spark.driver.memory ：默认值512m</span><br><span class="line">spark.executor.memory ：默认值512m</span><br><span class="line">spark.yarn.am.memory ：默认值512m</span><br><span class="line">spark.yarn.executor.memoryOverhead ：值为 executorMemory * 0.07, with minimum of 384</span><br><span class="line">spark.yarn.driver.memoryOverhead ：值为 driverMemory * 0.07, with minimum of 384</span><br><span class="line">spark.yarn.am.memoryOverhead ：值为 AM memory * 0.07, with minimum of 384</span><br><span class="line"></span><br><span class="line">#--executor-memory/spark.executor.memory 控制 executor 的堆的大小，但是 JVM 本身也会占用一定的堆空间，比如内部的 String 或者直接 byte buffer， spark.yarn.XXX.memoryOverhead 属性决定向 YARN 请求的每个 executor 或dirver或am 的额外堆内存大小，默认值为 max(384, 0.07 * spark.executor.memory )</span><br><span class="line">#在 executor 执行的时候配置过大的 memory 经常会导致过长的GC延时，64G是推荐的一个 executor 内存大小的上限。</span><br><span class="line">#HDFS client 在大量并发线程时存在性能问题。大概的估计是每个 executor 中最多5个并行的 task 就可以占满写入带宽</span><br></pre></td></tr></table></figure>
<p>YARN中有几个关键参数，参考YARN的内存和CPU配置<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">yarn.app.mapreduce.am.resource.mb ：AM能够申请的最大内存，默认值为1536MB</span><br><span class="line">yarn.nodemanager.resource.memory-mb ：nodemanager能够申请的最大内存，默认值为8192MB</span><br><span class="line">yarn.scheduler.minimum-allocation-mb ：调度时一个container能够申请的最小资源，默认值为1024MB</span><br><span class="line">yarn.scheduler.maximum-allocation-mb ：调度时一个container能够申请的最大资源，默认值为8192MB</span><br></pre></td></tr></table></figure></p>
<p>设置AM申请的内存值，要么使用cluster模式，要么在client模式中，是有 –conf 手动设置 spark.yarn.am.memory 属性，例如：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">sudo -uhdfs  spark-submit --class org.apache.spark.examples.SparkPi \</span><br><span class="line">    --verbose \</span><br><span class="line">    --master yarn \</span><br><span class="line">    --deploy-mode cluster\</span><br><span class="line">    --num-executors 3 \</span><br><span class="line">    --driver-memory 2g \</span><br><span class="line">    --executor-memory 512m \</span><br><span class="line">    --executor-cores 1 \</span><br><span class="line">	--conf spark.yarn.am.memory=1024m \rr</span><br><span class="line">    --queue thequeue \</span><br><span class="line">    /opt/cloudera/parcels/CDH-6.0.1-1.cdh6.0.1.p0.590678/lib/spark/examples/jars/spark-examples_2.11-2.2.0-cdh6.0.1.jar \</span><br><span class="line">    10</span><br></pre></td></tr></table></figure></p>
<p>ref<br><a href="https://www.cloudera.com/documentation/enterprise/latest/topics/cdh_ig_running_spark_on_yarn.html" target="_blank" rel="noopener">Running Spark Applications on YARN</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;Deployment-Modes&quot;&gt;&lt;a href=&quot;#Deployment-Modes&quot; class=&quot;headerlink&quot; title=&quot;Deployment Modes&quot;&gt;&lt;/a&gt;&lt;b&gt;Deployment Modes&lt;/b&gt;&lt;/h5&gt;&lt;p&gt;在YARN中，
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>how to cleanup keys without exipiration in Redis</title>
    <link href="https://t1ger.github.io/2018/10/24/how-to-cleanup-keys-without-exipiration-in-Redis/"/>
    <id>https://t1ger.github.io/2018/10/24/how-to-cleanup-keys-without-exipiration-in-Redis/</id>
    <published>2018-10-24T04:02:18.000Z</published>
    <updated>2018-10-24T03:29:42.634Z</updated>
    
    <content type="html"><![CDATA[<p>在排查redis内存一直增长问题的时候，我们首先想到的是如何找出有多少key没有设置失效时间</p>
<p>我们可以通过 info keyspace 命令查看：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">127.0.0.1:6379&gt; info keyspace</span><br><span class="line"># Keyspace</span><br><span class="line">db0:keys=473688,expires=1645,avg_ttl=1625894</span><br></pre></td></tr></table></figure></p>
<p>keys是总共的key, expires是要失效的key,ave_ttl是这些key的平均失效时间，单位是ms</p>
<p>注意，我们不能再生产环境使用keys命令，如果没有开启RDB,需要通过执行bgsave 来获得rdb文件，之后拷贝到测试环境</p>
<p>我们可以使用docker来加载<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker run --name redis_dump -d -v `pwd`:/data -p 6379 redis:3.2</span><br><span class="line"></span><br><span class="line">docker exec -ti redis_dump bash</span><br></pre></td></tr></table></figure></p>
<p>现在我们来统计未设置失效时间的key吧<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">redis-cli keys &quot;*&quot; &gt; keys</span><br><span class="line">cat keys | xargs -n 1 -L 1 redis-cli ttl &gt; ttl</span><br><span class="line">paste -d &quot; &quot; keys ttl | grep .*-1$ | cut -d &quot; &quot; -f 1 &gt; without_ttl</span><br><span class="line"></span><br><span class="line"># We can create a script for deleting the keys </span><br><span class="line">cat without_ttl | awk &apos;&#123;print &quot;redis-cli del &quot;$1&#125;&apos; &gt; redis.sh</span><br></pre></td></tr></table></figure></p>
<p>再来一个大红包吧，怎么找出最大的key呢<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]# redis-cli --bigkeys</span><br><span class="line"></span><br><span class="line"># Scanning the entire keyspace to find biggest keys as well as</span><br><span class="line"># average sizes per key type.  You can use -i 0.1 to sleep 0.1 sec</span><br><span class="line"># per 100 SCAN commands (not usually needed).</span><br><span class="line"></span><br><span class="line">[00.00%] Biggest string found so far &apos;courseinfo_201703827_2018-10-29&apos; with 14189 bytes</span><br><span class="line">[00.00%] Biggest string found so far &apos;courseinfo_3160205116_2017-09-04&apos; with 16717 bytes</span><br><span class="line">[00.01%] Biggest string found so far &apos;courseinfo_201707708_2018-12-03&apos; with 20835 bytes</span><br><span class="line">[00.02%] Biggest string found so far &apos;courseinfo_3170204119_2017-10-02&apos; with 23037 bytes</span><br><span class="line">[00.07%] Biggest string found so far &apos;courseinfo_3160204209_2017-09-18&apos; with 29201 bytes</span><br><span class="line">[00.38%] Biggest string found so far &apos;courseinfo_201504330342_2018-05-21&apos; with 138565 bytes</span><br><span class="line">[00.77%] Biggest string found so far &apos;courseinfo_201606060919_2018-03-26&apos; with 201381 bytes</span><br><span class="line">[00.98%] Biggest string found so far &apos;courseinfo_201501310518_2018-05-14&apos; with 215428 bytes</span><br><span class="line">[01.07%] Biggest string found so far &apos;courseinfo_201706061709_2018-06-11&apos; with 223421 bytes</span><br><span class="line">[04.43%] Biggest string found so far &apos;courseinfo_201706061633_2018-03-05&apos; with 249319 bytes</span><br><span class="line">[05.33%] Biggest string found so far &apos;courseinfo_201706062917_2018-03-05&apos; with 278392 bytes</span><br><span class="line">[06.03%] Biggest string found so far &apos;courseinfo_201707030224_2018-03-19&apos; with 281098 bytes</span><br><span class="line">[19.89%] Biggest string found so far &apos;courseinfo_201706062917_2018-03-19&apos; with 296014 bytes</span><br><span class="line"></span><br><span class="line">-------- summary -------</span><br><span class="line"></span><br><span class="line">Sampled 472908 keys in the keyspace!</span><br><span class="line">Total key length in bytes is 17874174 (avg len 37.80)</span><br><span class="line"></span><br><span class="line">Biggest string found &apos;courseinfo_201706062917_2018-03-19&apos; has 296014 bytes</span><br><span class="line"></span><br><span class="line">472908 strings with 2259279653 bytes (100.00% of keys, avg size 4777.42)</span><br><span class="line">0 lists with 0 items (00.00% of keys, avg size 0.00)</span><br><span class="line">0 sets with 0 members (00.00% of keys, avg size 0.00)</span><br><span class="line">0 hashs with 0 fields (00.00% of keys, avg size 0.00)</span><br><span class="line">0 zsets with 0 members (00.00% of keys, avg size 0.00)</span><br></pre></td></tr></table></figure></p>
<p>很快我们就会发现我们的问题了，courseinfo_201706062917_2018-03-19 就是我们要找的</p>
<p>ref<br><a href="https://jmaitrehenry.ca/2017/11/22/found-keys-without-expiration-in-redis/" target="_blank" rel="noopener">Found and cleanup keys without expiration in Redis</a></p>
<hr>
<p>您的鼓励是我写作最大的动力</p>
<p>俗话说，投资效率是最好的投资。 如果您感觉我的文章质量不错，读后收获很大，预计能为您提高 10% 的工作效率，不妨小额捐助我一下，让我有动力继续写出更多好文章。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在排查redis内存一直增长问题的时候，我们首先想到的是如何找出有多少key没有设置失效时间&lt;/p&gt;
&lt;p&gt;我们可以通过 info keyspace 命令查看：&lt;br&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>How to install CDH6.0 Cluster on Centos7.5</title>
    <link href="https://t1ger.github.io/2018/09/03/How-to-install-CDH6-0-Cluster-on-Centos7-5/"/>
    <id>https://t1ger.github.io/2018/09/03/How-to-install-CDH6-0-Cluster-on-Centos7-5/</id>
    <published>2018-09-03T02:21:26.000Z</published>
    <updated>2018-09-03T10:22:33.977Z</updated>
    
    <content type="html"><![CDATA[<h5 id="Cloudera-简介"><a href="#Cloudera-简介" class="headerlink" title="Cloudera 简介"></a><b>Cloudera 简介</b></h5><ul>
<li>Cloudera 官网：<a href="https://www.cloudera.com" target="_blank" rel="noopener">https://www.cloudera.com</a></li>
<li>Cloudera 官方文档： <a href="https://www.cloudera.com/documentation/enterprise/latest.html" target="_blank" rel="noopener">https://www.cloudera.com/documentation/enterprise/latest.html</a></li>
</ul>
<h5 id="安装Cloudera-Manager和CDH"><a href="#安装Cloudera-Manager和CDH" class="headerlink" title="安装Cloudera Manager和CDH"></a><b>安装Cloudera Manager和CDH</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">系统环境：CentOS7.5 </span><br><span class="line">软件环境：Oracle JDK、Cloudera Manager Server 和 Agent 、数据库、CDH各组件</span><br></pre></td></tr></table></figure>
<ul>
<li><p>系统初始化<br>关闭防火墙 禁用selinux,服务器之间免密，时间保持同步</p>
</li>
<li><p>Cloudera安装,官方文档参考<a href="https://www.cloudera.com/documentation/enterprise/6/6.0/topics/install_cm_cdh.html" target="_blank" rel="noopener">这里</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">#Configure a Repository</span><br><span class="line">wget https://archive.cloudera.com/cm6/6.0.0/redhat7/yum/cloudera-manager.repo -P /etc/yum.repos.d/</span><br><span class="line">sudo rpm --import https://archive.cloudera.com/cm6/6.0.0/redhat7/yum/RPM-GPG-KEY-cloudera</span><br><span class="line"></span><br><span class="line">#Installing the JDK</span><br><span class="line">sudo yum install oracle-j2sdk1.8</span><br><span class="line"></span><br><span class="line">#Install Cloudera Manager Packages</span><br><span class="line">sudo yum install cloudera-manager-daemons cloudera-manager-agent cloudera-manager-server</span><br><span class="line"></span><br><span class="line">#Install Databases</span><br><span class="line">wget http://repo.mysql.com/mysql-community-release-el7-5.noarch.rpm</span><br><span class="line">sudo rpm -ivh mysql-community-release-el7-5.noarch.rpm</span><br><span class="line">sudo yum update</span><br><span class="line">sudo yum install mysql-server</span><br><span class="line">sudo systemctl start mysqld</span><br><span class="line"></span><br><span class="line">#Set up the Cloudera Manager Database</span><br><span class="line">1. /opt/cloudera/cm/schema/scm_prepare_database.sh \</span><br><span class="line">[options] &lt;databaseType&gt; &lt;databaseName&gt; &lt;databaseUser&gt; &lt;password&gt;</span><br><span class="line">2. If it exists, remove the embedded PostgreSQL properties file:</span><br><span class="line">sudo rm /etc/cloudera-scm-server/db.mgmt.properties</span><br><span class="line"></span><br><span class="line">Example：</span><br><span class="line">sudo /opt/cloudera/cm/schema/scm_prepare_database.sh mysql scm scm</span><br><span class="line"></span><br><span class="line">#Install CDH and Other Software</span><br><span class="line">sudo systemctl start cloudera-scm-server</span><br><span class="line"></span><br><span class="line">we go to browser http://&lt;server_host&gt;:7180,login admin/admin</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建必需的数据库，可以参考<a href="https://www.cloudera.com/documentation/enterprise/latest/topics/install_cm_mariadb.html" target="_blank" rel="noopener">这里</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">create database metastore DEFAULT CHARACTER SET utf8;</span><br><span class="line">grant all on metastore.* TO &apos;hive&apos;@&apos;%&apos; IDENTIFIED BY &apos;hive&apos;;</span><br><span class="line"></span><br><span class="line">create database amon DEFAULT CHARACTER SET utf8;</span><br><span class="line">grant all on amon.* TO &apos;amon&apos;@&apos;%&apos; IDENTIFIED BY &apos;amon&apos;;</span><br><span class="line"></span><br><span class="line">create database hue DEFAULT CHARACTER SET utf8;</span><br><span class="line">grant all on hue.* TO &apos;hue&apos;@&apos;%&apos; IDENTIFIED BY &apos;hue&apos;;</span><br><span class="line"></span><br><span class="line">create database rman DEFAULT CHARACTER SET utf8;</span><br><span class="line">grant all on rman.* TO &apos;rman&apos;@&apos;%&apos; IDENTIFIED BY &apos;rman&apos;;</span><br><span class="line"></span><br><span class="line">create database navms DEFAULT CHARACTER SET utf8;</span><br><span class="line">grant all on navms.* TO &apos;navms&apos;@&apos;%&apos; IDENTIFIED BY &apos;navms&apos;;</span><br><span class="line"></span><br><span class="line">create database nas DEFAULT CHARACTER SET utf8;</span><br><span class="line">grant all on nas.* TO &apos;nas&apos;@&apos;%&apos; IDENTIFIED BY &apos;nas&apos;;</span><br><span class="line"></span><br><span class="line">create database oos DEFAULT CHARACTER SET utf8;</span><br><span class="line">grant all on oos.* TO &apos;oos&apos;@&apos;%&apos; IDENTIFIED BY &apos;oos&apos;;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h5 id="卸载重装CM服务"><a href="#卸载重装CM服务" class="headerlink" title="卸载重装CM服务"></a><b>卸载重装CM服务</b></h5><p>如果，第一次没有安装成功，那这部分就对你有帮助了<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># 安装CDH manager的服务器上面执行</span><br><span class="line">yum remove cloudera-manager-server -y</span><br><span class="line"></span><br><span class="line"># 在所有的服务器执行下面操作</span><br><span class="line">systemctl stop  cloudera-scm-agent</span><br><span class="line">yum remove cloudera-manager-agennt-y</span><br><span class="line">ps -ef | grep cmf | grep -v grep | awk &apos;&#123;print $2&#125;&apos; | xargs kill -9</span><br><span class="line">find / -name clouder* | xargs rm -rf </span><br><span class="line">find / -name cmf* | xargs rm -rf</span><br></pre></td></tr></table></figure></p>
<h5 id="Custom-Installation-Solutions"><a href="#Custom-Installation-Solutions" class="headerlink" title="Custom Installation Solutions"></a><b>Custom Installation Solutions</b></h5><p>如果在线安装很慢，我们可以通过以下方式来加速安装</p>
<ul>
<li><p>Creating a Permanent Internal Repository(Option)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">#Setting Up a Web Server</span><br><span class="line">sudo yum install httpd</span><br><span class="line">sudo systemctl start httpd</span><br><span class="line"></span><br><span class="line">#Edit the Apache HTTP Server configuration file (/etc/httpd/conf/httpd.conf by default) to add or edit the following line in the &lt;IfModule mime_module&gt; section:</span><br><span class="line">AddType application/x-gzip .gz .tgz .parcel</span><br><span class="line"></span><br><span class="line">#Downloading and Publishing the Package Repository</span><br><span class="line"></span><br><span class="line">#Cloudera Manager 6</span><br><span class="line">sudo mkdir -p /var/www/html/cloudera-repos</span><br><span class="line">sudo wget --recursive --no-parent --no-host-directories https://archive.cloudera.com/cm6/6.0.0/redhat7/ -P /var/www/html/cloudera-repos</span><br><span class="line">sudo chmod -R ugo+rX /var/www/html/cloudera-repos/cm6</span><br><span class="line"></span><br><span class="line">#CDH 6</span><br><span class="line">sudo mkdir -p /var/www/html/cloudera-repos</span><br><span class="line">sudo wget --recursive --no-parent --no-host-directories https://archive.cloudera.com/cdh6/6.0.0/redhat7/ -P /var/www/html/cloudera-repos</span><br><span class="line"></span><br><span class="line">sudo wget --recursive --no-parent --no-host-directories https://archive.cloudera.com/gplextras6/6.0.0/redhat7/ -P /var/www/html/cloudera-repos</span><br><span class="line"></span><br><span class="line">sudo chmod -R ugo+rX /var/www/html/cloudera-repos/cdh6</span><br><span class="line">sudo chmod -R ugo+rX /var/www/html/cloudera-repos/gplextras6</span><br></pre></td></tr></table></figure>
</li>
<li><p>Creating a Temporary Internal Repository(Option)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># Download the repository you need following the instructions in Downloading and Publishing the Package Repository.</span><br><span class="line">cd /var/www/html</span><br><span class="line">python -m SimpleHTTPServer 8900</span><br><span class="line"></span><br><span class="line">#Visit the Repository URL http://&lt;web_server&gt;:8900/cloudera-repos/</span><br></pre></td></tr></table></figure>
</li>
<li><p>Configuring Hosts to Use the Internal Repository</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># Create /etc/yum.repos.d/cloudera-repo.repo files on cluster hosts with the following content, </span><br><span class="line">where &lt;web_server&gt; is the hostname of the web server:</span><br><span class="line"></span><br><span class="line">[cloudera-repo]</span><br><span class="line">name=cloudera-repo</span><br><span class="line">baseurl=http://&lt;web_server&gt;/cm/5</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=0</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h5 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a><b>遇到的问题</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># jdbc驱动未找到</span><br><span class="line">mkdir -p /usr/share/java</span><br><span class="line">rz mysql-connector-java-5.1.45-bin.jar</span><br><span class="line">ln -s mysql-connector-java-5.1.45-bin.jar mysql-connector-java.jar</span><br><span class="line">systemctl restart cloudera-scm-server.service</span><br><span class="line"></span><br><span class="line"># host命令为找到</span><br><span class="line">yum install bind-utils -y</span><br></pre></td></tr></table></figure>
<p>ref<br><a href="https://gist.github.com/lilongen/b179b3868d2c2839ca7303b7605ce16b" target="_blank" rel="noopener">deploy-cm-cdh-on-centos7</a><br><a href="https://www.cloudera.com/documentation/enterprise/6/6.0/topics/cm_ig_create_local_package_repo.html#download_publish_package_repo" target="_blank" rel="noopener">Custom Installation Solutions</a><br><a href="https://www.cloudera.com/documentation/enterprise/6/latest/topics/cm_ig_create_local_parcel_repo.html#" target="_blank" rel="noopener">Using an Internally Hosted Remote Parcel Repository</a></p>
<hr>
<p>您的鼓励是我写作最大的动力</p>
<p>俗话说，投资效率是最好的投资。 如果您感觉我的文章质量不错，读后收获很大，预计能为您提高 10% 的工作效率，不妨小额捐助我一下，让我有动力继续写出更多好文章。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;Cloudera-简介&quot;&gt;&lt;a href=&quot;#Cloudera-简介&quot; class=&quot;headerlink&quot; title=&quot;Cloudera 简介&quot;&gt;&lt;/a&gt;&lt;b&gt;Cloudera 简介&lt;/b&gt;&lt;/h5&gt;&lt;ul&gt;
&lt;li&gt;Cloudera 官网：&lt;a href=&quot;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Elasticsearch6.3的新特性</title>
    <link href="https://t1ger.github.io/2018/06/15/Elasticsearch6-3%E7%9A%84%E6%96%B0%E7%89%B9%E6%80%A7/"/>
    <id>https://t1ger.github.io/2018/06/15/Elasticsearch6-3的新特性/</id>
    <published>2018-06-15T03:02:17.000Z</published>
    <updated>2018-06-20T06:57:05.131Z</updated>
    
    <content type="html"><![CDATA[<p>Elasticsearch 官方推出了6.3.0，今天，让我们看一下 Elasticsearch6.3.0给我们带来的新特性吧，如果想看官网的同学可以参考<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/release-notes-6.3.0.html" target="_blank" rel="noopener">这里</a></p>
<ul>
<li>Elasticsearch6.3默认包含了X-Pack,X-Pack包括APM,Canvas</li>
<li>支持SQL </li>
<li>支持Java 10</li>
<li>汇总统计</li>
<li>安全更新</li>
</ul>
<h5 id="支持SQL"><a href="#支持SQL" class="headerlink" title="支持SQL"></a><b>支持SQL</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line">#创建index并添加数据</span><br><span class="line">[root@localhost ~]# curl -X PUT &quot;localhost:9200/library/book/_bulk?refresh&quot; -H &apos;Content-Type: application/json&apos; -d&apos;</span><br><span class="line">&gt; &#123;&quot;index&quot;:&#123;&quot;_id&quot;: &quot;Leviathan Wakes&quot;&#125;&#125;</span><br><span class="line">&gt; &#123;&quot;name&quot;: &quot;Leviathan Wakes&quot;, &quot;author&quot;: &quot;James S.A. Corey&quot;, &quot;release_date&quot;: &quot;2011-06-02&quot;, &quot;page_count&quot;: 561&#125;</span><br><span class="line">&gt; &#123;&quot;index&quot;:&#123;&quot;_id&quot;: &quot;Hyperion&quot;&#125;&#125;</span><br><span class="line">&gt; &#123;&quot;name&quot;: &quot;Hyperion&quot;, &quot;author&quot;: &quot;Dan Simmons&quot;, &quot;release_date&quot;: &quot;1989-05-26&quot;, &quot;page_count&quot;: 482&#125;</span><br><span class="line">&gt; &#123;&quot;index&quot;:&#123;&quot;_id&quot;: &quot;Dune&quot;&#125;&#125;</span><br><span class="line">&gt; &#123;&quot;name&quot;: &quot;Dune&quot;, &quot;author&quot;: &quot;Frank Herbert&quot;, &quot;release_date&quot;: &quot;1965-06-01&quot;, &quot;page_count&quot;: 604&#125;</span><br><span class="line">&gt; &apos;</span><br><span class="line">&#123;&quot;took&quot;:426,&quot;errors&quot;:false,&quot;items&quot;:[&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;library&quot;,&quot;_type&quot;:&quot;book&quot;,&quot;_id&quot;:&quot;Leviathan Wakes&quot;,&quot;_version&quot;:1,&quot;result&quot;:&quot;created&quot;,&quot;forced_refresh&quot;:true,&quot;_shards&quot;:&#123;&quot;total&quot;:2,&quot;successful&quot;:1,&quot;failed&quot;:0&#125;,&quot;_seq_no&quot;:0,&quot;_primary_term&quot;:1,&quot;status&quot;:201&#125;&#125;,&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;library&quot;,&quot;_type&quot;:&quot;book&quot;,&quot;_id&quot;:&quot;Hyperion&quot;,&quot;_version&quot;:1,&quot;result&quot;:&quot;created&quot;,&quot;forced_refresh&quot;:true,&quot;_shards&quot;:&#123;&quot;total&quot;:2,&quot;successful&quot;:1,&quot;failed&quot;:0&#125;,&quot;_seq_no&quot;:0,&quot;_primary_term&quot;:1,&quot;status&quot;:201&#125;&#125;,&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;library&quot;,&quot;_type&quot;:&quot;book&quot;,&quot;_id&quot;:&quot;Dune&quot;,&quot;_version&quot;:1,&quot;result&quot;:&quot;created&quot;,&quot;forced_refresh&quot;:true,&quot;_shards&quot;:&#123;&quot;total&quot;:2,&quot;successful&quot;:1,&quot;failed&quot;:0&#125;,&quot;_seq_no&quot;:1,&quot;_primary_term&quot;:1,&quot;status&quot;:201&#125;&#125;]&#125;</span><br><span class="line"></span><br><span class="line">#通过SQL REST API执行SQL</span><br><span class="line">[root@localhost ~]# curl -X POST &quot;localhost:9200/_xpack/sql?format=txt&quot; -H &apos;Content-Type: application/json&apos; -d&apos;</span><br><span class="line">&gt; &#123;</span><br><span class="line">&gt;     &quot;query&quot;: &quot;SELECT * FROM library WHERE release_date &lt; \u00272000-01-01\u0027&quot;</span><br><span class="line">&gt; &#125;</span><br><span class="line">&gt; &apos;</span><br><span class="line">    author     |     name      |  page_count   |      release_date      </span><br><span class="line">---------------+---------------+---------------+------------------------</span><br><span class="line">Dan Simmons    |Hyperion       |482            |1989-05-26T00:00:00.000Z</span><br><span class="line">Frank Herbert  |Dune           |604            |1965-06-01T00:00:00.000Z</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#支持SQL REST API</span><br><span class="line">[root@localhost ~]# curl -X POST &quot;localhost:9200/_xpack/sql/translate&quot; -H &apos;Content-Type: application/json&apos; -d&apos;</span><br><span class="line">&gt; &#123;</span><br><span class="line">&gt;     &quot;query&quot;: &quot;SELECT * FROM library ORDER BY page_count DESC&quot;,</span><br><span class="line">&gt;     &quot;fetch_size&quot;: 10</span><br><span class="line">&gt; &#125;</span><br><span class="line">&gt; &apos;</span><br><span class="line">&#123;&quot;size&quot;:10,&quot;_source&quot;:&#123;&quot;includes&quot;:[&quot;author&quot;,&quot;name&quot;],&quot;excludes&quot;:[]&#125;,&quot;docvalue_fields&quot;:[&quot;page_count&quot;,&quot;release_date&quot;],&quot;sort&quot;:[&#123;&quot;page_count&quot;:&#123;&quot;order&quot;:&quot;desc&quot;&#125;&#125;]&#125;</span><br><span class="line"></span><br><span class="line">#支持SQL CLI</span><br><span class="line">[root@localhost ~]# /usr/share/elasticsearch/bin/elasticsearch-sql-cli</span><br><span class="line">     .sssssss.`                     .sssssss.</span><br><span class="line">  .:sXXXXXXXXXXo`                `ohXXXXXXXXXho.</span><br><span class="line"> .yXXXXXXXXXXXXXXo`            `oXXXXXXXXXXXXXXX-</span><br><span class="line">.XXXXXXXXXXXXXXXXXXo`        `oXXXXXXXXXXXXXXXXXX.</span><br><span class="line">.XXXXXXXXXXXXXXXXXXXXo.    .oXXXXXXXXXXXXXXXXXXXXh</span><br><span class="line">.XXXXXXXXXXXXXXXXXXXXXXo``oXXXXXXXXXXXXXXXXXXXXXXy</span><br><span class="line">`yXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX.</span><br><span class="line"> `oXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXo`</span><br><span class="line">   `oXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXo`</span><br><span class="line">     `oXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXo`</span><br><span class="line">       `oXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXo`</span><br><span class="line">         `oXXXXXXXXXXXXXXXXXXXXXXXXXXXXo`</span><br><span class="line">           .XXXXXXXXXXXXXXXXXXXXXXXXXo`</span><br><span class="line">         .oXXXXXXXXXXXXXXXXXXXXXXXXo`</span><br><span class="line">       `oXXXXXXXXXXXXXXXXXXXXXXXXo`   `odo`</span><br><span class="line">     `oXXXXXXXXXXXXXXXXXXXXXXXXo`   `oXXXXXo`</span><br><span class="line">   `oXXXXXXXXXXXXXXXXXXXXXXXXo`   `oXXXXXXXXXo`</span><br><span class="line"> `oXXXXXXXXXXXXXXXXXXXXXXXXo`   `oXXXXXXXXXXXXXo`</span><br><span class="line">`yXXXXXXXXXXXXXXXXXXXXXXXo`    oXXXXXXXXXXXXXXXXX.</span><br><span class="line">.XXXXXXXXXXXXXXXXXXXXXXo`   `oXXXXXXXXXXXXXXXXXXXy</span><br><span class="line">.XXXXXXXXXXXXXXXXXXXXo`     /XXXXXXXXXXXXXXXXXXXXX</span><br><span class="line">.XXXXXXXXXXXXXXXXXXo`        `oXXXXXXXXXXXXXXXXXX-</span><br><span class="line"> -XXXXXXXXXXXXXXXo`            `oXXXXXXXXXXXXXXXo`</span><br><span class="line">  .oXXXXXXXXXXXo`                `oXXXXXXXXXXXo.</span><br><span class="line">    `.sshXXyso`        SQL         `.sshXhss.`</span><br><span class="line"></span><br><span class="line">sql&gt; SELECT * FROM library WHERE release_date &lt; &apos;2000-01-01&apos;;</span><br><span class="line">    author     |     name      |  page_count   |      release_date      </span><br><span class="line">---------------+---------------+---------------+------------------------</span><br><span class="line">Dan Simmons    |Hyperion       |482            |1989-05-26T00:00:00.000Z</span><br><span class="line">Frank Herbert  |Dune           |604            |1965-06-01T00:00:00.000Z</span><br><span class="line"></span><br><span class="line">#支持SQL JDBC</span><br><span class="line"></span><br><span class="line">String address = &quot;jdbc:es://&quot; + elasticsearchAddress;     </span><br><span class="line">Properties connectionProperties = connectionProperties(); </span><br><span class="line">Connection connection = DriverManager.getConnection(address, connectionProperties);</span><br><span class="line">try (Statement statement = connection.createStatement();</span><br><span class="line">        ResultSet results = statement.executeQuery(</span><br><span class="line">            &quot;SELECT name, page_count FROM library ORDER BY page_count DESC LIMIT 1&quot;)) &#123;</span><br><span class="line">    assertTrue(results.next());</span><br><span class="line">    assertEquals(&quot;Don Quixote&quot;, results.getString(1));</span><br><span class="line">    assertEquals(1072, results.getInt(2));</span><br><span class="line">    SQLException e = expectThrows(SQLException.class, () -&gt; results.getInt(1));</span><br><span class="line">    assertTrue(e.getMessage(), e.getMessage().contains(&quot;unable to convert column 1 to an int&quot;));</span><br><span class="line">    assertFalse(results.next());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="支持Java-10"><a href="#支持Java-10" class="headerlink" title="支持Java 10"></a><b>支持Java 10</b></h5><p>Elasticsearch 支持Java9 和Java10，保持和Java快速发布周期一致。但是官方建议大多数用户使用java8，有兴趣的同学可以看<a href="https://jaxenter.com/no-more-public-updates-java-8-143703.html" target="_blank" rel="noopener">这里</a></p>
<h5 id="汇总统计"><a href="#汇总统计" class="headerlink" title="汇总统计"></a><b>汇总统计</b></h5><p>用户可以建立汇总统计job，job会汇聚统计最近搜索的更新数据。这个功能和SQl功能一样，都是实验性质的功能。</p>
<h5 id="安全更新"><a href="#安全更新" class="headerlink" title="安全更新"></a><b>安全更新</b></h5><p>XPackExtension  扩展机制被移除了，引入SPI扩展机制。</p>
<h5 id="Bug-fixes"><a href="#Bug-fixes" class="headerlink" title="Bug fixes"></a><b>Bug fixes</b></h5><p>具体可以参考<a href="https://www.elastic.co/guide/en/logstash/6.3/logstash-6-3-0.html" target="_blank" rel="noopener">这里</a></p>
<p>说了这么多，也许有朋友问了，怎么才能升级到Elasticsearch6.3呢，官方建议是：、<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">1. 6.x到6.y - 可以通过一次升级一个节点来执行</span><br><span class="line">2. 5.x至6.x - 需要完全重启群集</span><br><span class="line">3. 2.x至6.x - 不支持</span><br><span class="line"></span><br><span class="line">[root@localhost ~]# curl -XGET &apos;http://localhost:9200/&apos;</span><br><span class="line">&#123;</span><br><span class="line">  &quot;name&quot; : &quot;99NPxaU&quot;,</span><br><span class="line">  &quot;cluster_name&quot; : &quot;elasticsearch&quot;,</span><br><span class="line">  &quot;cluster_uuid&quot; : &quot;gW7bp0I3RNKvZI50SAsjeg&quot;,</span><br><span class="line">  &quot;version&quot; : &#123;</span><br><span class="line">    &quot;number&quot; : &quot;6.3.0&quot;,</span><br><span class="line">    &quot;build_flavor&quot; : &quot;default&quot;,</span><br><span class="line">    &quot;build_type&quot; : &quot;rpm&quot;,</span><br><span class="line">    &quot;build_hash&quot; : &quot;424e937&quot;,</span><br><span class="line">    &quot;build_date&quot; : &quot;2018-06-11T23:38:03.357887Z&quot;,</span><br><span class="line">    &quot;build_snapshot&quot; : false,</span><br><span class="line">    &quot;lucene_version&quot; : &quot;7.3.1&quot;,</span><br><span class="line">    &quot;minimum_wire_compatibility_version&quot; : &quot;5.6.0&quot;,</span><br><span class="line">    &quot;minimum_index_compatibility_version&quot; : &quot;5.0.0&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;tagline&quot; : &quot;You Know, for Search&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<hr>
<p>您的鼓励是我写作最大的动力</p>
<p>俗话说，投资效率是最好的投资。 如果您感觉我的文章质量不错，读后收获很大，预计能为您提高 10% 的工作效率，不妨小额捐助我一下，让我有动力继续写出更多好文章。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Elasticsearch 官方推出了6.3.0，今天，让我们看一下 Elasticsearch6.3.0给我们带来的新特性吧，如果想看官网的同学可以参考&lt;a href=&quot;https://www.elastic.co/guide/en/elasticsearch/refer
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>how to use sqoop in hadoop</title>
    <link href="https://t1ger.github.io/2018/06/11/how-to-use-sqoop-in-hadoop/"/>
    <id>https://t1ger.github.io/2018/06/11/how-to-use-sqoop-in-hadoop/</id>
    <published>2018-06-11T03:27:36.000Z</published>
    <updated>2018-06-12T10:12:39.676Z</updated>
    
    <content type="html"><![CDATA[<h5 id="install"><a href="#install" class="headerlink" title="install"></a><b>install</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">#download Sqoop 1.4.7 version</span><br><span class="line">[root@localhost ~]# wget https://mirrors.tuna.tsinghua.edu.cn/apache/sqoop/1.4.7/sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz</span><br><span class="line">[root@localhost ~]# tar zxf sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz</span><br><span class="line">[root@localhost ~]# wget https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-8.0.11.tar.gz</span><br><span class="line">#after untar file, move jar package to Sqoop lib directory </span><br><span class="line">[root@localhost ~]# cd /usr/local/sqoop/conf &amp;&amp; cp sqoop-env-template.sh sqoop-env.sh</span><br><span class="line">[root@localhost ~]# cat sqoop-env.sh</span><br><span class="line">#Set path to where bin/hadoop is available</span><br><span class="line">export HADOOP_COMMON_HOME=/usr/local/hadoop</span><br><span class="line">#Set path to where hadoop-*-core.jar is available</span><br><span class="line">export HADOOP_MAPRED_HOME=/usr/local/hadoop/share/hadoop/mapreduce</span><br><span class="line">#set the path to where bin/hbase is available</span><br><span class="line">#export HBASE_HOME=</span><br><span class="line">#Set the path to where bin/hive is available</span><br><span class="line">#export HIVE_HOME=</span><br><span class="line">#Set the path for where zookeper config dir is</span><br><span class="line">#export ZOOCFGDIR=</span><br><span class="line"></span><br><span class="line">#test </span><br><span class="line"> /usr/local/sqoop/bin/sqoop list-databases \</span><br><span class="line">  --connect jdbc:mysql://&lt;dburi&gt;/&lt;dbname&gt;  \</span><br><span class="line">  --username &lt;username&gt; --password &lt;password&gt; </span><br><span class="line"> /usr/local/sqoop/bin/sqoop list-tables  </span><br><span class="line"> --connect jdbc:mysql://&lt;dburi&gt;/&lt;dbname&gt; \</span><br><span class="line"> --username &lt;username&gt; --password &lt;password&gt;</span><br></pre></td></tr></table></figure>
<h5 id="application-scenarios"><a href="#application-scenarios" class="headerlink" title="application scenarios"></a><b>application scenarios</b></h5><p>tips: before you use command,make sure to su hadoop</p>
<ol>
<li>mysql -&gt; hdfs</li>
<li>hdfs  -&gt; mysql</li>
<li>mysql -&gt; hive</li>
<li>hive  -&gt; mysql</li>
<li>use sql as import condition</li>
</ol>
<ul>
<li><p>from mysql to hdfs<br>–check-column (col): Specifies the column to be examined when determining which rows to import. (the column should not be of type CHAR/NCHAR/VARCHAR/VARNCHAR/ LONGVARCHAR/LONGNVARCHAR)<br>–incremental (mode): Specifies how Sqoop determines which rows are new. Legal values for mode include append and lastmodified.<br>–last-value (value): Specifies the maximum value of the check column from the previous import</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line">sqoop import --connect jdbc:mysql://&lt;dburi&gt;/&lt;dbname&gt; \</span><br><span class="line">--username &lt;username&gt; --password &lt;password&gt; \</span><br><span class="line">--table &lt;tablename&gt; --check-column &lt;col&gt; --incremental &lt;mode&gt; --last-value &lt;value&gt; --target-dir &lt;hdfs-dir&gt;</span><br><span class="line"></span><br><span class="line">/usr/local/sqoop/bin/sqoop import \</span><br><span class="line">--connect jdbc:mysql://192.168.100.112/datbase?zeroDateTimeBehavior=CONVERT_TO_NULL \</span><br><span class="line">--username username --password password \</span><br><span class="line">--table pv_daxue \</span><br><span class="line">--target-dir /usr/sqoop/daxue</span><br><span class="line"></span><br><span class="line">#save as parquet(textfile,orcfile,parquet)</span><br><span class="line">/usr/local/sqoop/bin/sqoop import \</span><br><span class="line">--connect jdbc:mysql://192.168.100.112/datbase?zeroDateTimeBehavior=CONVERT_TO_NULL \</span><br><span class="line">--username username --password password \</span><br><span class="line">--table pv_daxue  --target-dir /usr/sqoop/daxue \</span><br><span class="line">--as-parquetfile</span><br><span class="line"></span><br><span class="line">#save columns id,account </span><br><span class="line">/usr/local/sqoop/bin/sqoop import \</span><br><span class="line">--connect jdbc:mysql://192.168.100.112/datbase?zeroDateTimeBehavior=CONVERT_TO_NULL \</span><br><span class="line">--username username --password password \</span><br><span class="line">--table pv_daxue  --target-dir /usr/sqoop/daxue \</span><br><span class="line">--columns id,account \</span><br><span class="line">--as-textfile</span><br><span class="line"></span><br><span class="line">tips: Parameters --as-sequencefile --as-avrodatafile and --as-parquetfile are not supported with --direct params in MySQL case. </span><br><span class="line"># after insert one record ,append import again</span><br><span class="line"># Append mode</span><br><span class="line">/usr/local/sqoop/bin/sqoop import \</span><br><span class="line">--connect jdbc:mysql://192.168.100.112/datbase?zeroDateTimeBehavior=CONVERT_TO_NULL \</span><br><span class="line">--username username --password password \</span><br><span class="line">--table pv_daxue --check-column id \</span><br><span class="line">--incremental append --target-dir /usr/sqoop/daxue \</span><br><span class="line">-last-value 5</span><br><span class="line"></span><br><span class="line"># Lastmodified mode</span><br><span class="line">#first import</span><br><span class="line">/usr/local/sqoop/bin/sqoop import \</span><br><span class="line">--connect jdbc:mysql://192.168.100.112/datbase?zeroDateTimeBehavior=CONVERT_TO_NULL \</span><br><span class="line">--username username --password password \</span><br><span class="line">--table pv_daxue --target-dir /usr/sqoop/daxue -m1</span><br><span class="line"></span><br><span class="line">bash-4.1$ hadoop fs -cat /usr/sqoop/daxue/part-m-00000</span><br><span class="line">1,hello,2018-06-12 23:48:32.0</span><br><span class="line">2,word,2018-06-12 23:48:32.0</span><br><span class="line">3,marry,2018-06-12 23:48:32.0</span><br><span class="line">4,tony,2018-06-12 23:48:32.0</span><br><span class="line">5,jack,2018-06-12 23:48:33.0</span><br><span class="line">6,james,2018-06-12 23:52:03.0</span><br><span class="line">#after insert one record , Lastmodified  import again</span><br><span class="line">/usr/local/sqoop/bin/sqoop import \</span><br><span class="line">--connect jdbc:mysql://192.168.100.112/datbase?zeroDateTimeBehavior=CONVERT_TO_NULL \</span><br><span class="line">--username username --password password \</span><br><span class="line">--table pv_daxue --check-column last_mod  --incremental lastmodified --last-value &quot;2018-06-12 10:52:03&quot; \</span><br><span class="line">--target-dir /usr/sqoop/daxue -m 1 --append </span><br><span class="line"></span><br><span class="line">18/06/12 10:59:48 INFO mapreduce.ImportJobBase: Transferred 60 bytes in 4.2309 seconds (14.1813 bytes/sec)</span><br><span class="line">18/06/12 10:59:48 INFO mapreduce.ImportJobBase: Retrieved 2 records</span><br><span class="line">bash-4.1$ hadoop fs -cat /usr/sqoop/daxue/part-m-00001</span><br><span class="line">6,james,2018-06-12 23:52:03.0</span><br><span class="line">7,hello,2018-06-12 23:58:08.0</span><br><span class="line"></span><br><span class="line">#merage by mode, after execute sql &quot;update customertest set name = &apos;Hello&apos; where id = 1;&quot;</span><br><span class="line">/usr/local/sqoop/bin/sqoop import \</span><br><span class="line">--connect jdbc:mysql://192.168.100.112/datbase?zeroDateTimeBehavior=CONVERT_TO_NULL \</span><br><span class="line">--username username --password password \</span><br><span class="line">--table pv_daxue  --check-column last_mod  --incremental lastmodified --last-value &quot;2018-06-12 23:52:03&quot; \</span><br><span class="line">--target-dir /usr/sqoop/daxue -m 1 --merge-key id </span><br><span class="line"></span><br><span class="line">bash-4.1$ hadoop fs -cat /usr/sqoop/daxue/part-r-00000</span><br><span class="line">1,Hello,2018-06-13 00:07:41.0</span><br><span class="line">2,word,2018-06-12 23:48:32.0</span><br><span class="line">3,marry,2018-06-12 23:48:32.0</span><br><span class="line">4,tony,2018-06-12 23:48:32.0</span><br><span class="line">5,jack,2018-06-12 23:48:33.0</span><br><span class="line">6,james,2018-06-12 23:52:03.0</span><br><span class="line">7,me,2018-06-12 23:58:08.0</span><br></pre></td></tr></table></figure>
</li>
<li><p>from hdfs to mysql<br>According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn’t set</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sqoop export --connect jdbc:mysql://&lt;dburi&gt;/&lt;dbname&gt; \</span><br><span class="line">--username &lt;username&gt; --password &lt;password&gt; \</span><br><span class="line">--table &lt;tablename&gt; --export-dir &lt;hdfs-dir&gt;</span><br><span class="line"></span><br><span class="line">/usr/local/sqoop/bin/sqoop export  \</span><br><span class="line">--connect &quot;jdbc:mysql://192.168.100.112/datbase?useSSL=false&amp;useUnicode=true&amp;characterEncoding=utf-8&quot; \</span><br><span class="line">--username username --password password \</span><br><span class="line">--table pv_daxue  --export-dir /usr/sqoop/daxue</span><br></pre></td></tr></table></figure>
</li>
<li><p>from mysql to hive</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">sqoop import --connect jdbc:mysql://&lt;dburi&gt;/&lt;dbname&gt; \</span><br><span class="line">--username &lt;username&gt; --password &lt;password&gt; \</span><br><span class="line">--table &lt;tablename&gt; --check-column &lt;col&gt; --incremental &lt;mode&gt; --last-value &lt;value&gt; \</span><br><span class="line">--fields-terminated-by &quot;\t&quot; --lines-terminated-by &quot;\n&quot; \</span><br><span class="line">--hive-import --target-dir &lt;hdfs-dir&gt; --hive-table &lt;hive-tablename&gt;</span><br><span class="line"></span><br><span class="line">/usr/local/sqoop/bin/sqoop import \</span><br><span class="line">--connect &quot;jdbc:mysql://192.168.100.112/datbase?useSSL=false&amp;useUnicode=true&amp;characterEncoding=utf-8&quot; \</span><br><span class="line">--username username --password password \</span><br><span class="line">–-table hive_table  -–hive-import  --hive-database database –-hive-table hive_test or -–create-hive-table hive_test \  --delete-target-dir  --split-by id</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># --map-column-hive </span><br><span class="line">MySQL(bigint) --&gt; Hive(bigint) </span><br><span class="line">MySQL(tinyint) --&gt; Hive(tinyint) </span><br><span class="line">MySQL(int) --&gt; Hive(int) </span><br><span class="line">MySQL(double) --&gt; Hive(double) </span><br><span class="line">MySQL(bit) --&gt; Hive(boolean) </span><br><span class="line">MySQL(varchar) --&gt; Hive(string) </span><br><span class="line">MySQL(decimal) --&gt; Hive(double) </span><br><span class="line">MySQL(date/timestamp) --&gt; Hive(string)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">/usr/local/sqoop/bin/sqoop import \</span><br><span class="line">--connect &quot;jdbc:mysql://192.168.100.112/datbase?useSSL=false&amp;useUnicode=true&amp;characterEncoding=utf-8&quot; \</span><br><span class="line">--username username --password password \</span><br><span class="line">–-table hive_table  -–hive-import  \</span><br><span class="line">--map-column-hive cost=&quot;DECIMAL&quot;,date=&quot;DATE&quot; \ </span><br><span class="line">--hive-database database –-hive-table hive_test or -–create-hive-table hive_test \  </span><br><span class="line">--delete-target-dir  --split-by id</span><br></pre></td></tr></table></figure>
</li>
<li><p>from hive to mysql</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#Refer above from hdfs to mysql,only need specify the HDFS path corresponding to the Hive table</span><br><span class="line">/usr/local/sqoop/bin/sqoop export  \</span><br><span class="line">--connect &quot;jdbc:mysql://192.168.100.112/datbase?useSSL=false&amp;useUnicode=true&amp;characterEncoding=utf-8&quot; \</span><br><span class="line">--username username --password password</span><br><span class="line">--table customer --export-dir /user/hive/warehouse/user.db/customer --fields-terminated-by &apos;\001&apos;</span><br></pre></td></tr></table></figure>
</li>
<li><p>use sql as import condition</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sqoop import --connect jdbc:mysql://&lt;dburi&gt;/&lt;dbname&gt; --username &lt;username&gt; --password &lt;password&gt; --query &lt;query-sql&gt; --split-by &lt;sp-column&gt; --hive-import --hive-table &lt;hive-tablename&gt; --target-dir &lt;hdfs-dir&gt;</span><br><span class="line"></span><br><span class="line">/usr/local/sqoop/bin/sqoop import \</span><br><span class="line">--connect jdbc:mysql://192.168.100.112/datbase?zeroDateTimeBehavior=CONVERT_TO_NULL \</span><br><span class="line">--username username --password password --table pv_daxue  --target-dir /usr/sqoop/daxue --delete-target-dir \</span><br><span class="line">--query &apos;select id,account from version where account=&quot;ddd&quot; and $CONDITIONS &apos; \</span><br><span class="line">--as-parquetfile</span><br></pre></td></tr></table></figure>
</li>
<li><p>from mysql to hbase</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">/usr/local/sqoop/bin/sqoop import \</span><br><span class="line">--connect jdbc:mysql://192.168.100.112/datbase?zeroDateTimeBehavior=CONVERT_TO_NULL \</span><br><span class="line">--username username --password password </span><br><span class="line">--query &apos;select id,account from version where account=&quot;ddd&quot; and $CONDITIONS &apos; \</span><br><span class="line">--hbase-table pv_daxue  --hbase-create-table \ </span><br><span class="line">--hbase-row-key id --split-by date -m 7 \ </span><br><span class="line">--column-family tiger</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h5 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a><b>遇到的问题</b></h5><ul>
<li><p>ERROR tool.ImportTool: Import failed: java.io.FileNotFoundException</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">18/06/11 15:42:43 ERROR tool.ImportTool: Import failed: java.io.FileNotFoundException: File does not exist: hdfs://172.16.56.143:8020/usr/local/sqoop-1.4.7.bin__hadoop-2.6.0/lib/parquet-jackson-1.6.0.jar</span><br><span class="line"></span><br><span class="line">[hadoop@node1 conf]$ /usr/local/hadoop/bin/hadoop fs  -mkdir -p /usr/local/sqoop-1.4.7.bin__hadoop-2.6.0/lib</span><br><span class="line">[hadoop@node1 conf]$ /usr/local/hadoop/bin/hadoop fs  -put  /usr/local/sqoop-1.4.7.bin__hadoop-2.6.0/lib/* hdfs://172.16.56.143:8020/usr/local/sqoop-1.4.7.bin__hadoop-2.6.0/lib</span><br></pre></td></tr></table></figure>
</li>
<li><p>Caused by: com.mysql.cj.exceptions.CJException: The connection property ‘zeroDateTimeBehavior’ acceptable values are: ‘CONVERT_TO_NULL’, ‘EXCEPTION’ or ‘ROUND’. The value ‘convertToNull’ is not acceptable.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#using the following code below:</span><br><span class="line">jdbc:mysql://localhost:3306/database?zeroDateTimeBehavior=CONVERT_TO_NULL</span><br></pre></td></tr></table></figure>
<p>  if config lzo ,you  perhaps see ,use command “hadoop checknative” check</p>
</li>
<li><p>ERROR sqoop.Sqoop: Got exception running Sqoop: java.lang.IllegalArgumentException: Compression codec com.hadoop.compression.lzo.LzoCodec not found.<br>java.lang.IllegalArgumentException: Compression codec com.hadoop.compression.lzo.LzoCodec not found.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># install lzo support</span><br></pre></td></tr></table></figure>
</li>
<li><p>No primary key could be found for tablescore</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">specify one with --split-by or perform a sequential import with&apos;-m 1&apos;</span><br></pre></td></tr></table></figure>
</li>
<li><p>ERROR hive.HiveConfig: Could not load org.apache.hadoop.hive.conf.HiveConf. Make sure HIVE_CONF_DIR is set correctly.<br>ERROR tool.ImportTool: Import failed: java.io.IOException: java.lang.ClassNotFoundException: org.apache.hadoop.hive.conf.HiveConf</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#add this one in .bash_profile:</span><br><span class="line">export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:/usr/lib/hive/lib/*</span><br><span class="line">source ~/.bash_profile</span><br></pre></td></tr></table></figure>
</li>
<li><p>FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException(message:For direct MetaStore DB connections, we don’t support retries at the client level.)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; alter database hive character set latin1;</span><br></pre></td></tr></table></figure>
</li>
<li><p>java.lang.RuntimeException: Can’t parse input data: ‘1Hello2018-06-13 00:07:41.0’</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">--fields-terminated-by &apos;\001&apos;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>ref<br><a href="http://sqoop.apache.org/docs/1.4.7/SqoopUserGuide.html#_incremental_imports" target="_blank" rel="noopener">Incremental Imports</a><br><a href="https://www.cnblogs.com/ljy2013/p/4872126.html" target="_blank" rel="noopener">sqoop的增量导入（increment import）</a><br><a href="https://github.com/kevinweil/hadoop-lzo" target="_blank" rel="noopener">hadoop-lzo</a><br><a href="http://www.oberhumer.com/opensource/lzo/#download" target="_blank" rel="noopener">lzo</a><br><a href="https://www.zybuluo.com/aitanjupt/note/209968#%E4%BD%BF%E7%94%A8sqoop%E4%BB%8Emysql%E5%AF%BC%E5%85%A5%E6%95%B0%E6%8D%AE%E5%88%B0hive" target="_blank" rel="noopener">Sqoop从MySQL导入数据</a></p>
<hr>
<p>您的鼓励是我写作最大的动力</p>
<p>俗话说，投资效率是最好的投资。 如果您感觉我的文章质量不错，读后收获很大，预计能为您提高 10% 的工作效率，不妨小额捐助我一下，让我有动力继续写出更多好文章。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;install&quot;&gt;&lt;a href=&quot;#install&quot; class=&quot;headerlink&quot; title=&quot;install&quot;&gt;&lt;/a&gt;&lt;b&gt;install&lt;/b&gt;&lt;/h5&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Rsyslog 连接 Kafka 指北</title>
    <link href="https://t1ger.github.io/2018/05/22/Rsyslog-%E8%BF%9E%E6%8E%A5-Kafka-%E6%8C%87%E5%8C%97/"/>
    <id>https://t1ger.github.io/2018/05/22/Rsyslog-连接-Kafka-指北/</id>
    <published>2018-05-22T07:20:48.000Z</published>
    <updated>2018-05-22T06:51:56.401Z</updated>
    
    <content type="html"><![CDATA[<h5 id="运行环境"><a href="#运行环境" class="headerlink" title="运行环境"></a><b>运行环境</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@bogon ~]# cat /etc/redhat-release </span><br><span class="line">CentOS Linux release 7.4.1708 (Core) </span><br><span class="line"></span><br><span class="line">[root@bogon ~]# rpm -qa|grep rsyslog</span><br><span class="line">rsyslog-kafka-8.28.0-1.el7.x86_64</span><br><span class="line">rsyslog-8.28.0-1.el7.x86_64</span><br></pre></td></tr></table></figure>
<h5 id="安装"><a href="#安装" class="headerlink" title="安装"></a><b>安装</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">wget -O /etc/yum.repos.d/rsyslog.repo http://rpms.adiscon.com/v8-stable/rsyslog.repo</span><br><span class="line">yum install rsyslog rsyslog-kafka.x86_64</span><br></pre></td></tr></table></figure>
<p>国内的同学可能无法安装，同学们也可以通过<a href="http://rpms.adiscon.com/v8-stable/epel-7/x86_64/RPMS/" target="_blank" rel="noopener">这里</a>下载安装</p>
<h5 id="配置"><a href="#配置" class="headerlink" title="配置"></a><b>配置</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@bogon ~]# cat /etc/rsyslog.d/kafka.conf</span><br><span class="line">module(load=&quot;omkafka&quot;)</span><br><span class="line">action (</span><br><span class="line">        type=&quot;omkafka&quot;</span><br><span class="line">        topic=&quot;topicA&quot;</span><br><span class="line">        broker=&quot;cdh1:9092,cdh2:9092,cdh3:9092&quot;</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#如果保存到本地</span><br><span class="line">[root@bogon ~]# cat /etc/rsyslog.d/router.conf</span><br><span class="line">template (name=&quot;DynFile&quot; type=&quot;string&quot; string=&quot;/data/%fromhost-ip%.log&quot;)</span><br><span class="line">if $fromhost-ip startswith &apos;192.168.100.2&apos; and $programname != &apos;Type=SESSION;&apos; and $programname != &apos;Type=Login;&apos; and $programname != &apos;Type=AuthLog;&apos; and $programname != &apos;Type=Ftp&apos; then &#123;</span><br><span class="line">    action(type=&quot;omfile&quot; dynaFile=&quot;DynFile&quot;)</span><br><span class="line">    stop</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>低版本的rsyslog保存到本地配置如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]# rpm -qa|grep rsyslog</span><br><span class="line">rsyslog-5.8.10-6.el6.x86_64</span><br><span class="line"></span><br><span class="line">添加到 /etc/rsyslog.conf </span><br><span class="line">#### GLOBAL DIRECTIVES ####</span><br><span class="line">$template RemoteLogs,&quot;/data/var/log/%HOSTNAME%/%fromhost-ip%_%$YEAR%-%$MONTH%-%$DAY%.log&quot; *</span><br><span class="line">*.* ?RemoteLogs</span><br><span class="line">&amp;~</span><br></pre></td></tr></table></figure></p>
<p>通过kafka查看消息<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/kafka&amp;&amp; bin/kafka-console-consumer.sh  --bootstrap-server cdh1:9092,cdh2:9092,cdh3:9092   --topic topicA</span><br></pre></td></tr></table></figure></p>
<p>ref<br><a href="https://doc.yonyoucloud.com/doc/logstash-best-practice-cn/ecosystem/rsyslog.html" target="_blank" rel="noopener">Rsyslog</a><br><a href="http://wdxtub.com/2016/08/17/rsyslog-kafka-guide/" target="_blank" rel="noopener">Rsyslog 连接 Kafka 指南</a><br><a href="https://serverfault.com/questions/807108/how-to-call-template-so-rsyslog-8-creates-one-log-file-per-client" target="_blank" rel="noopener">How to call template so rsyslog 8 creates one log file per client</a><br><a href="http://wiki.rsyslog.com/index.php/DailyLogRotation" target="_blank" rel="noopener">DailyLogRotation</a><br><a href="http://blog.kompaz.win/2018/01/11/20180111%20CentOS7%20rsyslog%20+loganalyzer%E9%85%8D%E7%BD%AE/" target="_blank" rel="noopener">CentOS7 rsyslog +loganalyzer配置</a></p>
<hr>
<p>您的鼓励是我写作最大的动力</p>
<p>俗话说，投资效率是最好的投资。 如果您感觉我的文章质量不错，读后收获很大，预计能为您提高 10% 的工作效率，不妨小额捐助我一下，让我有动力继续写出更多好文章。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;运行环境&quot;&gt;&lt;a href=&quot;#运行环境&quot; class=&quot;headerlink&quot; title=&quot;运行环境&quot;&gt;&lt;/a&gt;&lt;b&gt;运行环境&lt;/b&gt;&lt;/h5&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutt
    
    </summary>
    
    
  </entry>
  
</feed>
