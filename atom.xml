<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>t1ger的茶馆</title>
  <subtitle>头顶有光终是幻，足下生云未是仙</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://t1ger.github.io/"/>
  <updated>2018-11-15T06:11:12.188Z</updated>
  <id>https://t1ger.github.io/</id>
  
  <author>
    <name>t1ger</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>How to install Cabot on CentOS</title>
    <link href="https://t1ger.github.io/2018/11/14/How-to-install-Cabot-on-CentOS/"/>
    <id>https://t1ger.github.io/2018/11/14/How-to-install-Cabot-on-CentOS/</id>
    <published>2018-11-14T09:56:36.000Z</published>
    <updated>2018-11-15T06:11:12.188Z</updated>
    
    <content type="html"><![CDATA[<p>Cabot环境搭建</p>
<p>初始化mysql数据库<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ mysql -u root -p -e &quot;CREATE USER cabot@localhost IDENTIFIED BY &apos;cabot&apos;&quot;;</span><br><span class="line">$ mysql -u root -p -e &quot;CREATE DATABASE cabot&quot;;</span><br><span class="line">$ mysql -u root -p -e &quot;GRANT ALL PRIVILEGES ON \`cabot\`.* TO \`cabot\`@localhost&quot;;</span><br></pre></td></tr></table></figure></p>
<p>下载cabot</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/arachnys/cabot.git</span><br></pre></td></tr></table></figure>
<p>安装依赖软件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ sudo yum install ruby</span><br><span class="line">$ sudo gem install foreman</span><br><span class="line">$ sudo pip install fabric</span><br><span class="line">$ sudo yum install python-devel</span><br><span class="line">$ sudo yum install openldap-devel</span><br><span class="line">$ sudo pip install -r requirements.txt </span><br><span class="line">$ sudo pip install -r requirements-plugins.txt </span><br><span class="line">$ sudo pip install -r requirements-dev.txt</span><br><span class="line">$ sudo pip install MySQL-python</span><br><span class="line">$ sudo yum install nodejs</span><br><span class="line">$ sudo npm install -g less</span><br><span class="line">$ sudo npm install -g coffee-script</span><br></pre></td></tr></table></figure></p>
<p>修改配置文件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">$ cp production.env.example production.env</span><br><span class="line">$ vi production.env</span><br><span class="line">DEBUG=t</span><br><span class="line">DATABASE_URL=mysql://cabot:cabot@localhost:3306/cabot</span><br><span class="line">DJANGO_SETTINGS_MODULE=cabot.settings</span><br><span class="line">LOG_FILE=log</span><br><span class="line">PORT=5008</span><br><span class="line"></span><br><span class="line"># Local time zone for this installation. Choices can be found here:</span><br><span class="line"># http://en.wikipedia.org/wiki/List_of_tz_zones_by_name</span><br><span class="line">TIME_ZONE=Asia/Shanghai</span><br><span class="line"></span><br><span class="line"># Django settings</span><br><span class="line">CELERY_BROKER_URL=redis://localhost:6379/1</span><br><span class="line">DJANGO_SECRET_KEY=2FL6ORhHwr5eX34pP9mMugnIOd3jzVuT45f7w430Mt5PnEwbcJgma0q8zUXNZ68A</span><br><span class="line"></span><br><span class="line"># Hostname of your Graphite server instance</span><br><span class="line">GRAPHITE_API=http://*.*.*.*:12346/</span><br><span class="line">GRAPHITE_USER=username</span><br><span class="line">GRAPHITE_PASS=password</span><br></pre></td></tr></table></figure></p>
<p>修改启动文件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">$ vi .foreman</span><br><span class="line"></span><br><span class="line"># vi: set ft=yaml :</span><br><span class="line">procfile: Procfile</span><br><span class="line">env: conf/production.env</span><br><span class="line"></span><br><span class="line">$ vi gunicorn.conf</span><br><span class="line"></span><br><span class="line"># -*- mode: python -*-</span><br><span class="line"># vi: set ft=python :</span><br><span class="line">import os</span><br><span class="line">bind = &apos;0.0.0.0:%s&apos; % os.environ[&apos;PORT&apos;]</span><br><span class="line">workers = 3</span><br><span class="line"></span><br><span class="line">$ vi Procfile</span><br><span class="line">web:       gunicorn cabot.wsgi:application --config gunicorn.conf</span><br><span class="line">celery:    celery worker -B -A cabot --loglevel=INFO --concurrency=16 -Ofair</span><br><span class="line">beat:      celery beat -A cabot --loglevel=INFO</span><br></pre></td></tr></table></figure></p>
<p>初始化数据库<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sh setup_dev.sh</span><br></pre></td></tr></table></figure></p>
<p>启动web程序<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ foreman start web</span><br></pre></td></tr></table></figure></p>
<p>启动celery<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">foreman start celery</span><br></pre></td></tr></table></figure></p>
<p>登录管理页面(第一次登录需要设置管理员账号)</p>
<p>使用supervisor管理Cabot,Celery进程<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">$ sudo vi /etc/supervisord.d/cabot.conf</span><br><span class="line"></span><br><span class="line">[program:web]</span><br><span class="line">command=foreman start web</span><br><span class="line">autorstart=true</span><br><span class="line">autorestart=true</span><br><span class="line">redirect_stderr=true</span><br><span class="line">stopsignal=TERM</span><br><span class="line">stdout_logfile=/var/log/web.log</span><br><span class="line">directory=/opt/cabot</span><br><span class="line"></span><br><span class="line">[program:celery]</span><br><span class="line">command=foreman start celery</span><br><span class="line">autorstart=true</span><br><span class="line">autorestart=true</span><br><span class="line">redirect_stderr=true</span><br><span class="line">stopsignal=TERM</span><br><span class="line">stdout_logfile=/var/log/celery.log</span><br><span class="line">directory=/opt/cabot</span><br></pre></td></tr></table></figure></p>
<p>使用supervisor管理程序<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo supervisorctl reload</span><br></pre></td></tr></table></figure></p>
<p>遇到的错误<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">1. &quot;Missing staticfiles manifest entry for &apos;%s&apos;&quot; % clean_name</span><br><span class="line">#解决方法</span><br><span class="line">python manage.py collectstatic</span><br><span class="line"></span><br><span class="line">2. CommandError: An error occurred during rendering /usr/local/whistle/webapps/cabot/cabot/templates/cabotapp/statuscheck_report.html:</span><br><span class="line"> /bin/sh: lessc: command not found</span><br><span class="line">#解决方法</span><br><span class="line">pip install nodeenv</span><br><span class="line">nodeenv nodeenv</span><br><span class="line">source nodeenv/bin/activate</span><br><span class="line">npm install -g less</span><br></pre></td></tr></table></figure></p>
<p>ref<br><a href="https://my.oschina.net/cdsc/blog/1480219" target="_blank" rel="noopener">Cabot Alert Mysql环境搭建</a><br><a href="https://github.com/ialbert/biostar-central/issues/193" target="_blank" rel="noopener">Cannot run Biostars out of the box: “lessc: command not found”</a><br><a href="https://stackoverflow.com/questions/44160666/valueerror-missing-staticfiles-manifest-entry-for-favicon-ico" target="_blank" rel="noopener">ValueError: Missing staticfiles manifest entry for ‘favicon.ico’</a></p>
<hr>
<p>您的鼓励是我写作最大的动力</p>
<p>俗话说，投资效率是最好的投资。 如果您感觉我的文章质量不错，读后收获很大，预计能为您提高 10% 的工作效率，不妨小额捐助我一下，让我有动力继续写出更多好文章。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Cabot环境搭建&lt;/p&gt;
&lt;p&gt;初始化mysql数据库&lt;br&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span c
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>mysql audit plugin</title>
    <link href="https://t1ger.github.io/2018/11/05/mysql-audit-plugin/"/>
    <id>https://t1ger.github.io/2018/11/05/mysql-audit-plugin/</id>
    <published>2018-11-05T09:40:06.000Z</published>
    <updated>2018-11-05T09:59:45.781Z</updated>
    
    <content type="html"><![CDATA[<p>此文转载于<a href="https://blog.csdn.net/heizistudio/article/details/50954294" target="_blank" rel="noopener">ora600</a>,略作调整.</p>
<p>下载地址如下<br><a href="http://pan.baidu.com/s/1dFGFCrv" target="_blank" rel="noopener">http://pan.baidu.com/s/1dFGFCrv</a></p>
<p>mysql5.6.X.tar.gz到mysql-5.7.8-rc.tar.gz是一个版本—-audit5_6_21.so<br>mysql5.7.1.tar.gz—mysql5.7.9.tar.gz是一个版本—-audit5_7_9.so<br>mysql5.7.10–mysql5.7.22是一个版本</p>
<p>一、查找插件所在位置<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; show variables like &apos;%plugin_dir%&apos;;</span><br><span class="line">+---------------+------------------------------+</span><br><span class="line">| Variable_name | Value                        |</span><br><span class="line">+---------------+------------------------------+</span><br><span class="line">| plugin_dir    | /usr/local/mysql/lib/plugin/ |</span><br><span class="line">+---------------+------------------------------+</span><br><span class="line">1 row in set (0.00 sec)</span><br><span class="line">---------------------</span><br></pre></td></tr></table></figure></p>
<p>二、将audit_版本号.so插件下载后放到plugin_dir位置<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mv audit_版本号.so  audit.so</span><br></pre></td></tr></table></figure></p>
<p>三、加载插件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; install plugin audit  SONAME &apos;audit.so&apos;;</span><br></pre></td></tr></table></figure></p>
<p>四、卸载插件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; uninstall plugin audit;</span><br></pre></td></tr></table></figure></p>
<p>使用插件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; show variables like &apos;%audit%&apos;;</span><br><span class="line">+----------------+----------------------+</span><br><span class="line">| Variable_name  | Value                |</span><br><span class="line">+----------------+----------------------+</span><br><span class="line">| audit_logfile  | /tmp/mysql_audit.log |</span><br><span class="line">| audit_myswitch | OFF                  |</span><br><span class="line">| audit_num      | 0                    |</span><br><span class="line">| audit_sql      | all_sql              |</span><br><span class="line">| audit_user     | all_user             |</span><br><span class="line">+----------------+----------------------+</span><br><span class="line">5 rows in set (0.01 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; set global audit_logfile=&apos;/tmp/mysql_audit_1.log&apos;;----只读变量，审计仅指定在/tmp/mysql_audit.log文件，保障权限可以写</span><br><span class="line">ERROR 1238 (HY000): Variable &apos;audit_logfile&apos; is a read only variable</span><br><span class="line"></span><br><span class="line">set global audit_sql=&apos;delete;select;drop&apos;;   -----这些审计关键字用;分开</span><br><span class="line">set global audit_user=&apos;user2;user3&apos;;         ----审计用户用;隔开</span><br><span class="line">set global audit_num =0;                          ----审计sql影响的最少行数，默认为0</span><br><span class="line">set global audit_myswitch=on|off|ON|OFF|1|0;       -----开启关闭审计</span><br></pre></td></tr></table></figure></p>
<p>查看日志linux下tailf /tmp/mysql_audit.log</p>
<p>ref<br><a href="https://blog.csdn.net/heizistudio/article/details/50954294" target="_blank" rel="noopener">mysql审计插件(运维不在背锅)</a><br><a href="https://dev.mysql.com/doc/refman/5.5/en/writing-audit-plugins.html" target="_blank" rel="noopener">Writing Audit Plugins</a><br><a href="https://github.com/mcafee/mysql-audit/wiki/Configuration" target="_blank" rel="noopener">mcafee/mysql-audit</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;此文转载于&lt;a href=&quot;https://blog.csdn.net/heizistudio/article/details/50954294&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;ora600&lt;/a&gt;,略作调整.&lt;/p&gt;
&lt;p&gt;下载地址如下&lt;b
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Spark On YARN</title>
    <link href="https://t1ger.github.io/2018/10/30/Spark-On-YARN/"/>
    <id>https://t1ger.github.io/2018/10/30/Spark-On-YARN/</id>
    <published>2018-10-30T07:27:47.000Z</published>
    <updated>2018-10-30T10:02:56.495Z</updated>
    
    <content type="html"><![CDATA[<h5 id="Deployment-Modes"><a href="#Deployment-Modes" class="headerlink" title="Deployment Modes"></a><b>Deployment Modes</b></h5><p>在YARN中，每个应用实例有一个ApplicationMaster进程，这是应用实例开启的第一个容器。 ResourceManager向ApplicationMaster申请资源，<br>在资源分配后，应用实例会通知NodeManagers去启动容器。ApplicationMasters 评估每个客户端的需要:进程启动的应用可以被中断，持续协调管理进程。</p>
<h5 id="Cluster-Deployment-Mode"><a href="#Cluster-Deployment-Mode" class="headerlink" title="Cluster Deployment Mode"></a><b>Cluster Deployment Mode</b></h5><p>在集群模式，Spark driver 运行在ApplicationMaster集群主机里边，在Yarn容器里的进程负责驱动应用和向YARN请求资源。<br>集群模式不适合交互<br><img src="https://www.cloudera.com/documentation/enterprise/latest/images/xspark-yarn-cluster.png.pagespeed.ic.f4CfMwda2i.webp" alt="cluster mode"></p>
<h5 id="Cluster-Deployment-Mode-1"><a href="#Cluster-Deployment-Mode-1" class="headerlink" title="Cluster Deployment Mode"></a><b>Cluster Deployment Mode</b></h5><p>在客户端模式，Spark driver运行在提交job的主机上,ApplicationMaster响应来自于Yarn容器的请求，在容器启动后，客户端和容器协调完成任务调度<br><img src="https://www.cloudera.com/documentation/enterprise/latest/images/xspark-yarn-client.png.pagespeed.ic.Nm0CUtnR01.webp" alt="client mode"></p>
<h5 id="Configuring-the-Environment"><a href="#Configuring-the-Environment" class="headerlink" title="Configuring the Environment"></a><b>Configuring the Environment</b></h5><p>Spark 需要配置 HADOOP_CONF_DIR or YARN_CONF_DIR 环境变量指向包含客户端目录的配置文件，这些配置文件用于写入HDFS和连接YARN ResourceManager.如果使用Cloudera Manager的部署客户端配置，这些变量会自动配置好.<br>否则在提交job会出现如下错误<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Exception in thread &quot;main&quot; java.lang.Exception: When running with master &apos;yarn&apos; either HADOOP_CONF_DIR or YARN_CONF_DIR must be set in the environment.</span><br><span class="line">    at org.apache.spark.deploy.SparkSubmitArguments.validateSubmitArguments(SparkSubmitArguments.scala:251)</span><br><span class="line">    at org.apache.spark.deploy.SparkSubmitArguments.validateArguments(SparkSubmitArguments.scala:228)</span><br><span class="line">    at org.apache.spark.deploy.SparkSubmitArguments.&lt;init&gt;(SparkSubmitArguments.scala:109)</span><br><span class="line">    at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:114)</span><br></pre></td></tr></table></figure></p>
<h5 id="Running-a-Spark-Shell-Application-on-YARN"><a href="#Running-a-Spark-Shell-Application-on-YARN" class="headerlink" title="Running a Spark Shell Application on YARN"></a><b>Running a Spark Shell Application on YARN</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#launch a Spark application in cluster mode</span><br><span class="line">[root@cdh2 admin]# cat job.sh </span><br><span class="line">sudo -uhdfs  spark-submit --class org.apache.spark.examples.SparkPi \</span><br><span class="line">    --verbose \</span><br><span class="line">    --master yarn \</span><br><span class="line">    --deploy-mode cluster\</span><br><span class="line">    --num-executors 3 \</span><br><span class="line">    --driver-memory 2g \</span><br><span class="line">    --executor-memory 512m \</span><br><span class="line">    --executor-cores 1 \</span><br><span class="line">    --queue thequeue \</span><br><span class="line">    /opt/cloudera/parcels/CDH-6.0.1-1.cdh6.0.1.p0.590678/lib/spark/examples/jars/spark-examples_2.11-2.2.0-cdh6.0.1.jar \</span><br><span class="line">    10 </span><br><span class="line"></span><br><span class="line">#run spark-shell in client mode:	</span><br><span class="line"> ./bin/spark-shell --master yarn --deploy-mode client</span><br></pre></td></tr></table></figure>
<h5 id="Spark-On-YARN相关的配置参数"><a href="#Spark-On-YARN相关的配置参数" class="headerlink" title="Spark On YARN相关的配置参数"></a><b>Spark On YARN相关的配置参数</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">spark.driver.memory ：默认值512m</span><br><span class="line">spark.executor.memory ：默认值512m</span><br><span class="line">spark.yarn.am.memory ：默认值512m</span><br><span class="line">spark.yarn.executor.memoryOverhead ：值为 executorMemory * 0.07, with minimum of 384</span><br><span class="line">spark.yarn.driver.memoryOverhead ：值为 driverMemory * 0.07, with minimum of 384</span><br><span class="line">spark.yarn.am.memoryOverhead ：值为 AM memory * 0.07, with minimum of 384</span><br><span class="line"></span><br><span class="line">#--executor-memory/spark.executor.memory 控制 executor 的堆的大小，但是 JVM 本身也会占用一定的堆空间，比如内部的 String 或者直接 byte buffer， spark.yarn.XXX.memoryOverhead 属性决定向 YARN 请求的每个 executor 或dirver或am 的额外堆内存大小，默认值为 max(384, 0.07 * spark.executor.memory )</span><br><span class="line">#在 executor 执行的时候配置过大的 memory 经常会导致过长的GC延时，64G是推荐的一个 executor 内存大小的上限。</span><br><span class="line">#HDFS client 在大量并发线程时存在性能问题。大概的估计是每个 executor 中最多5个并行的 task 就可以占满写入带宽</span><br></pre></td></tr></table></figure>
<p>YARN中有几个关键参数，参考YARN的内存和CPU配置<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">yarn.app.mapreduce.am.resource.mb ：AM能够申请的最大内存，默认值为1536MB</span><br><span class="line">yarn.nodemanager.resource.memory-mb ：nodemanager能够申请的最大内存，默认值为8192MB</span><br><span class="line">yarn.scheduler.minimum-allocation-mb ：调度时一个container能够申请的最小资源，默认值为1024MB</span><br><span class="line">yarn.scheduler.maximum-allocation-mb ：调度时一个container能够申请的最大资源，默认值为8192MB</span><br></pre></td></tr></table></figure></p>
<p>设置AM申请的内存值，要么使用cluster模式，要么在client模式中，是有 –conf 手动设置 spark.yarn.am.memory 属性，例如：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">sudo -uhdfs  spark-submit --class org.apache.spark.examples.SparkPi \</span><br><span class="line">    --verbose \</span><br><span class="line">    --master yarn \</span><br><span class="line">    --deploy-mode cluster\</span><br><span class="line">    --num-executors 3 \</span><br><span class="line">    --driver-memory 2g \</span><br><span class="line">    --executor-memory 512m \</span><br><span class="line">    --executor-cores 1 \</span><br><span class="line">	--conf spark.yarn.am.memory=1024m \rr</span><br><span class="line">    --queue thequeue \</span><br><span class="line">    /opt/cloudera/parcels/CDH-6.0.1-1.cdh6.0.1.p0.590678/lib/spark/examples/jars/spark-examples_2.11-2.2.0-cdh6.0.1.jar \</span><br><span class="line">    10</span><br></pre></td></tr></table></figure></p>
<p>ref<br><a href="https://www.cloudera.com/documentation/enterprise/latest/topics/cdh_ig_running_spark_on_yarn.html" target="_blank" rel="noopener">Running Spark Applications on YARN</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;Deployment-Modes&quot;&gt;&lt;a href=&quot;#Deployment-Modes&quot; class=&quot;headerlink&quot; title=&quot;Deployment Modes&quot;&gt;&lt;/a&gt;&lt;b&gt;Deployment Modes&lt;/b&gt;&lt;/h5&gt;&lt;p&gt;在YARN中，
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>how to cleanup keys without exipiration in Redis</title>
    <link href="https://t1ger.github.io/2018/10/24/how-to-cleanup-keys-without-exipiration-in-Redis/"/>
    <id>https://t1ger.github.io/2018/10/24/how-to-cleanup-keys-without-exipiration-in-Redis/</id>
    <published>2018-10-24T04:02:18.000Z</published>
    <updated>2018-10-24T03:29:42.634Z</updated>
    
    <content type="html"><![CDATA[<p>在排查redis内存一直增长问题的时候，我们首先想到的是如何找出有多少key没有设置失效时间</p>
<p>我们可以通过 info keyspace 命令查看：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">127.0.0.1:6379&gt; info keyspace</span><br><span class="line"># Keyspace</span><br><span class="line">db0:keys=473688,expires=1645,avg_ttl=1625894</span><br></pre></td></tr></table></figure></p>
<p>keys是总共的key, expires是要失效的key,ave_ttl是这些key的平均失效时间，单位是ms</p>
<p>注意，我们不能再生产环境使用keys命令，如果没有开启RDB,需要通过执行bgsave 来获得rdb文件，之后拷贝到测试环境</p>
<p>我们可以使用docker来加载<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker run --name redis_dump -d -v `pwd`:/data -p 6379 redis:3.2</span><br><span class="line"></span><br><span class="line">docker exec -ti redis_dump bash</span><br></pre></td></tr></table></figure></p>
<p>现在我们来统计未设置失效时间的key吧<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">redis-cli keys &quot;*&quot; &gt; keys</span><br><span class="line">cat keys | xargs -n 1 -L 1 redis-cli ttl &gt; ttl</span><br><span class="line">paste -d &quot; &quot; keys ttl | grep .*-1$ | cut -d &quot; &quot; -f 1 &gt; without_ttl</span><br><span class="line"></span><br><span class="line"># We can create a script for deleting the keys </span><br><span class="line">cat without_ttl | awk &apos;&#123;print &quot;redis-cli del &quot;$1&#125;&apos; &gt; redis.sh</span><br></pre></td></tr></table></figure></p>
<p>再来一个大红包吧，怎么找出最大的key呢<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]# redis-cli --bigkeys</span><br><span class="line"></span><br><span class="line"># Scanning the entire keyspace to find biggest keys as well as</span><br><span class="line"># average sizes per key type.  You can use -i 0.1 to sleep 0.1 sec</span><br><span class="line"># per 100 SCAN commands (not usually needed).</span><br><span class="line"></span><br><span class="line">[00.00%] Biggest string found so far &apos;courseinfo_201703827_2018-10-29&apos; with 14189 bytes</span><br><span class="line">[00.00%] Biggest string found so far &apos;courseinfo_3160205116_2017-09-04&apos; with 16717 bytes</span><br><span class="line">[00.01%] Biggest string found so far &apos;courseinfo_201707708_2018-12-03&apos; with 20835 bytes</span><br><span class="line">[00.02%] Biggest string found so far &apos;courseinfo_3170204119_2017-10-02&apos; with 23037 bytes</span><br><span class="line">[00.07%] Biggest string found so far &apos;courseinfo_3160204209_2017-09-18&apos; with 29201 bytes</span><br><span class="line">[00.38%] Biggest string found so far &apos;courseinfo_201504330342_2018-05-21&apos; with 138565 bytes</span><br><span class="line">[00.77%] Biggest string found so far &apos;courseinfo_201606060919_2018-03-26&apos; with 201381 bytes</span><br><span class="line">[00.98%] Biggest string found so far &apos;courseinfo_201501310518_2018-05-14&apos; with 215428 bytes</span><br><span class="line">[01.07%] Biggest string found so far &apos;courseinfo_201706061709_2018-06-11&apos; with 223421 bytes</span><br><span class="line">[04.43%] Biggest string found so far &apos;courseinfo_201706061633_2018-03-05&apos; with 249319 bytes</span><br><span class="line">[05.33%] Biggest string found so far &apos;courseinfo_201706062917_2018-03-05&apos; with 278392 bytes</span><br><span class="line">[06.03%] Biggest string found so far &apos;courseinfo_201707030224_2018-03-19&apos; with 281098 bytes</span><br><span class="line">[19.89%] Biggest string found so far &apos;courseinfo_201706062917_2018-03-19&apos; with 296014 bytes</span><br><span class="line"></span><br><span class="line">-------- summary -------</span><br><span class="line"></span><br><span class="line">Sampled 472908 keys in the keyspace!</span><br><span class="line">Total key length in bytes is 17874174 (avg len 37.80)</span><br><span class="line"></span><br><span class="line">Biggest string found &apos;courseinfo_201706062917_2018-03-19&apos; has 296014 bytes</span><br><span class="line"></span><br><span class="line">472908 strings with 2259279653 bytes (100.00% of keys, avg size 4777.42)</span><br><span class="line">0 lists with 0 items (00.00% of keys, avg size 0.00)</span><br><span class="line">0 sets with 0 members (00.00% of keys, avg size 0.00)</span><br><span class="line">0 hashs with 0 fields (00.00% of keys, avg size 0.00)</span><br><span class="line">0 zsets with 0 members (00.00% of keys, avg size 0.00)</span><br></pre></td></tr></table></figure></p>
<p>很快我们就会发现我们的问题了，courseinfo_201706062917_2018-03-19 就是我们要找的</p>
<p>ref<br><a href="https://jmaitrehenry.ca/2017/11/22/found-keys-without-expiration-in-redis/" target="_blank" rel="noopener">Found and cleanup keys without expiration in Redis</a></p>
<hr>
<p>您的鼓励是我写作最大的动力</p>
<p>俗话说，投资效率是最好的投资。 如果您感觉我的文章质量不错，读后收获很大，预计能为您提高 10% 的工作效率，不妨小额捐助我一下，让我有动力继续写出更多好文章。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在排查redis内存一直增长问题的时候，我们首先想到的是如何找出有多少key没有设置失效时间&lt;/p&gt;
&lt;p&gt;我们可以通过 info keyspace 命令查看：&lt;br&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>How to install CDH6.0 Cluster on Centos7.5</title>
    <link href="https://t1ger.github.io/2018/09/03/How-to-install-CDH6-0-Cluster-on-Centos7-5/"/>
    <id>https://t1ger.github.io/2018/09/03/How-to-install-CDH6-0-Cluster-on-Centos7-5/</id>
    <published>2018-09-03T02:21:26.000Z</published>
    <updated>2018-09-03T10:22:33.977Z</updated>
    
    <content type="html"><![CDATA[<h5 id="Cloudera-简介"><a href="#Cloudera-简介" class="headerlink" title="Cloudera 简介"></a><b>Cloudera 简介</b></h5><ul>
<li>Cloudera 官网：<a href="https://www.cloudera.com" target="_blank" rel="noopener">https://www.cloudera.com</a></li>
<li>Cloudera 官方文档： <a href="https://www.cloudera.com/documentation/enterprise/latest.html" target="_blank" rel="noopener">https://www.cloudera.com/documentation/enterprise/latest.html</a></li>
</ul>
<h5 id="安装Cloudera-Manager和CDH"><a href="#安装Cloudera-Manager和CDH" class="headerlink" title="安装Cloudera Manager和CDH"></a><b>安装Cloudera Manager和CDH</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">系统环境：CentOS7.5 </span><br><span class="line">软件环境：Oracle JDK、Cloudera Manager Server 和 Agent 、数据库、CDH各组件</span><br></pre></td></tr></table></figure>
<ul>
<li><p>系统初始化<br>关闭防火墙 禁用selinux,服务器之间免密，时间保持同步</p>
</li>
<li><p>Cloudera安装,官方文档参考<a href="https://www.cloudera.com/documentation/enterprise/6/6.0/topics/install_cm_cdh.html" target="_blank" rel="noopener">这里</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">#Configure a Repository</span><br><span class="line">wget https://archive.cloudera.com/cm6/6.0.0/redhat7/yum/cloudera-manager.repo -P /etc/yum.repos.d/</span><br><span class="line">sudo rpm --import https://archive.cloudera.com/cm6/6.0.0/redhat7/yum/RPM-GPG-KEY-cloudera</span><br><span class="line"></span><br><span class="line">#Installing the JDK</span><br><span class="line">sudo yum install oracle-j2sdk1.8</span><br><span class="line"></span><br><span class="line">#Install Cloudera Manager Packages</span><br><span class="line">sudo yum install cloudera-manager-daemons cloudera-manager-agent cloudera-manager-server</span><br><span class="line"></span><br><span class="line">#Install Databases</span><br><span class="line">wget http://repo.mysql.com/mysql-community-release-el7-5.noarch.rpm</span><br><span class="line">sudo rpm -ivh mysql-community-release-el7-5.noarch.rpm</span><br><span class="line">sudo yum update</span><br><span class="line">sudo yum install mysql-server</span><br><span class="line">sudo systemctl start mysqld</span><br><span class="line"></span><br><span class="line">#Set up the Cloudera Manager Database</span><br><span class="line">1. /opt/cloudera/cm/schema/scm_prepare_database.sh \</span><br><span class="line">[options] &lt;databaseType&gt; &lt;databaseName&gt; &lt;databaseUser&gt; &lt;password&gt;</span><br><span class="line">2. If it exists, remove the embedded PostgreSQL properties file:</span><br><span class="line">sudo rm /etc/cloudera-scm-server/db.mgmt.properties</span><br><span class="line"></span><br><span class="line">Example：</span><br><span class="line">sudo /opt/cloudera/cm/schema/scm_prepare_database.sh mysql scm scm</span><br><span class="line"></span><br><span class="line">#Install CDH and Other Software</span><br><span class="line">sudo systemctl start cloudera-scm-server</span><br><span class="line"></span><br><span class="line">we go to browser http://&lt;server_host&gt;:7180,login admin/admin</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建必需的数据库，可以参考<a href="https://www.cloudera.com/documentation/enterprise/latest/topics/install_cm_mariadb.html" target="_blank" rel="noopener">这里</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">create database metastore DEFAULT CHARACTER SET utf8;</span><br><span class="line">grant all on metastore.* TO &apos;hive&apos;@&apos;%&apos; IDENTIFIED BY &apos;hive&apos;;</span><br><span class="line"></span><br><span class="line">create database amon DEFAULT CHARACTER SET utf8;</span><br><span class="line">grant all on amon.* TO &apos;amon&apos;@&apos;%&apos; IDENTIFIED BY &apos;amon&apos;;</span><br><span class="line"></span><br><span class="line">create database hue DEFAULT CHARACTER SET utf8;</span><br><span class="line">grant all on hue.* TO &apos;hue&apos;@&apos;%&apos; IDENTIFIED BY &apos;hue&apos;;</span><br><span class="line"></span><br><span class="line">create database rman DEFAULT CHARACTER SET utf8;</span><br><span class="line">grant all on rman.* TO &apos;rman&apos;@&apos;%&apos; IDENTIFIED BY &apos;rman&apos;;</span><br><span class="line"></span><br><span class="line">create database navms DEFAULT CHARACTER SET utf8;</span><br><span class="line">grant all on navms.* TO &apos;navms&apos;@&apos;%&apos; IDENTIFIED BY &apos;navms&apos;;</span><br><span class="line"></span><br><span class="line">create database nas DEFAULT CHARACTER SET utf8;</span><br><span class="line">grant all on nas.* TO &apos;nas&apos;@&apos;%&apos; IDENTIFIED BY &apos;nas&apos;;</span><br><span class="line"></span><br><span class="line">create database oos DEFAULT CHARACTER SET utf8;</span><br><span class="line">grant all on oos.* TO &apos;oos&apos;@&apos;%&apos; IDENTIFIED BY &apos;oos&apos;;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h5 id="卸载重装CM服务"><a href="#卸载重装CM服务" class="headerlink" title="卸载重装CM服务"></a><b>卸载重装CM服务</b></h5><p>如果，第一次没有安装成功，那这部分就对你有帮助了<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># 安装CDH manager的服务器上面执行</span><br><span class="line">yum remove cloudera-manager-server -y</span><br><span class="line"></span><br><span class="line"># 在所有的服务器执行下面操作</span><br><span class="line">systemctl stop  cloudera-scm-agent</span><br><span class="line">yum remove cloudera-manager-agennt-y</span><br><span class="line">ps -ef | grep cmf | grep -v grep | awk &apos;&#123;print $2&#125;&apos; | xargs kill -9</span><br><span class="line">find / -name clouder* | xargs rm -rf </span><br><span class="line">find / -name cmf* | xargs rm -rf</span><br></pre></td></tr></table></figure></p>
<h5 id="Custom-Installation-Solutions"><a href="#Custom-Installation-Solutions" class="headerlink" title="Custom Installation Solutions"></a><b>Custom Installation Solutions</b></h5><p>如果在线安装很慢，我们可以通过以下方式来加速安装</p>
<ul>
<li><p>Creating a Permanent Internal Repository(Option)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">#Setting Up a Web Server</span><br><span class="line">sudo yum install httpd</span><br><span class="line">sudo systemctl start httpd</span><br><span class="line"></span><br><span class="line">#Edit the Apache HTTP Server configuration file (/etc/httpd/conf/httpd.conf by default) to add or edit the following line in the &lt;IfModule mime_module&gt; section:</span><br><span class="line">AddType application/x-gzip .gz .tgz .parcel</span><br><span class="line"></span><br><span class="line">#Downloading and Publishing the Package Repository</span><br><span class="line"></span><br><span class="line">#Cloudera Manager 6</span><br><span class="line">sudo mkdir -p /var/www/html/cloudera-repos</span><br><span class="line">sudo wget --recursive --no-parent --no-host-directories https://archive.cloudera.com/cm6/6.0.0/redhat7/ -P /var/www/html/cloudera-repos</span><br><span class="line">sudo chmod -R ugo+rX /var/www/html/cloudera-repos/cm6</span><br><span class="line"></span><br><span class="line">#CDH 6</span><br><span class="line">sudo mkdir -p /var/www/html/cloudera-repos</span><br><span class="line">sudo wget --recursive --no-parent --no-host-directories https://archive.cloudera.com/cdh6/6.0.0/redhat7/ -P /var/www/html/cloudera-repos</span><br><span class="line"></span><br><span class="line">sudo wget --recursive --no-parent --no-host-directories https://archive.cloudera.com/gplextras6/6.0.0/redhat7/ -P /var/www/html/cloudera-repos</span><br><span class="line"></span><br><span class="line">sudo chmod -R ugo+rX /var/www/html/cloudera-repos/cdh6</span><br><span class="line">sudo chmod -R ugo+rX /var/www/html/cloudera-repos/gplextras6</span><br></pre></td></tr></table></figure>
</li>
<li><p>Creating a Temporary Internal Repository(Option)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># Download the repository you need following the instructions in Downloading and Publishing the Package Repository.</span><br><span class="line">cd /var/www/html</span><br><span class="line">python -m SimpleHTTPServer 8900</span><br><span class="line"></span><br><span class="line">#Visit the Repository URL http://&lt;web_server&gt;:8900/cloudera-repos/</span><br></pre></td></tr></table></figure>
</li>
<li><p>Configuring Hosts to Use the Internal Repository</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># Create /etc/yum.repos.d/cloudera-repo.repo files on cluster hosts with the following content, </span><br><span class="line">where &lt;web_server&gt; is the hostname of the web server:</span><br><span class="line"></span><br><span class="line">[cloudera-repo]</span><br><span class="line">name=cloudera-repo</span><br><span class="line">baseurl=http://&lt;web_server&gt;/cm/5</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=0</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h5 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a><b>遇到的问题</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># jdbc驱动未找到</span><br><span class="line">mkdir -p /usr/share/java</span><br><span class="line">rz mysql-connector-java-5.1.45-bin.jar</span><br><span class="line">ln -s mysql-connector-java-5.1.45-bin.jar mysql-connector-java.jar</span><br><span class="line">systemctl restart cloudera-scm-server.service</span><br><span class="line"></span><br><span class="line"># host命令为找到</span><br><span class="line">yum install bind-utils -y</span><br></pre></td></tr></table></figure>
<p>ref<br><a href="https://gist.github.com/lilongen/b179b3868d2c2839ca7303b7605ce16b" target="_blank" rel="noopener">deploy-cm-cdh-on-centos7</a><br><a href="https://www.cloudera.com/documentation/enterprise/6/6.0/topics/cm_ig_create_local_package_repo.html#download_publish_package_repo" target="_blank" rel="noopener">Custom Installation Solutions</a><br><a href="https://www.cloudera.com/documentation/enterprise/6/latest/topics/cm_ig_create_local_parcel_repo.html#" target="_blank" rel="noopener">Using an Internally Hosted Remote Parcel Repository</a></p>
<hr>
<p>您的鼓励是我写作最大的动力</p>
<p>俗话说，投资效率是最好的投资。 如果您感觉我的文章质量不错，读后收获很大，预计能为您提高 10% 的工作效率，不妨小额捐助我一下，让我有动力继续写出更多好文章。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;Cloudera-简介&quot;&gt;&lt;a href=&quot;#Cloudera-简介&quot; class=&quot;headerlink&quot; title=&quot;Cloudera 简介&quot;&gt;&lt;/a&gt;&lt;b&gt;Cloudera 简介&lt;/b&gt;&lt;/h5&gt;&lt;ul&gt;
&lt;li&gt;Cloudera 官网：&lt;a href=&quot;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Elasticsearch6.3的新特性</title>
    <link href="https://t1ger.github.io/2018/06/15/Elasticsearch6-3%E7%9A%84%E6%96%B0%E7%89%B9%E6%80%A7/"/>
    <id>https://t1ger.github.io/2018/06/15/Elasticsearch6-3的新特性/</id>
    <published>2018-06-15T03:02:17.000Z</published>
    <updated>2018-06-20T06:57:05.131Z</updated>
    
    <content type="html"><![CDATA[<p>Elasticsearch 官方推出了6.3.0，今天，让我们看一下 Elasticsearch6.3.0给我们带来的新特性吧，如果想看官网的同学可以参考<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/release-notes-6.3.0.html" target="_blank" rel="noopener">这里</a></p>
<ul>
<li>Elasticsearch6.3默认包含了X-Pack,X-Pack包括APM,Canvas</li>
<li>支持SQL </li>
<li>支持Java 10</li>
<li>汇总统计</li>
<li>安全更新</li>
</ul>
<h5 id="支持SQL"><a href="#支持SQL" class="headerlink" title="支持SQL"></a><b>支持SQL</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line">#创建index并添加数据</span><br><span class="line">[root@localhost ~]# curl -X PUT &quot;localhost:9200/library/book/_bulk?refresh&quot; -H &apos;Content-Type: application/json&apos; -d&apos;</span><br><span class="line">&gt; &#123;&quot;index&quot;:&#123;&quot;_id&quot;: &quot;Leviathan Wakes&quot;&#125;&#125;</span><br><span class="line">&gt; &#123;&quot;name&quot;: &quot;Leviathan Wakes&quot;, &quot;author&quot;: &quot;James S.A. Corey&quot;, &quot;release_date&quot;: &quot;2011-06-02&quot;, &quot;page_count&quot;: 561&#125;</span><br><span class="line">&gt; &#123;&quot;index&quot;:&#123;&quot;_id&quot;: &quot;Hyperion&quot;&#125;&#125;</span><br><span class="line">&gt; &#123;&quot;name&quot;: &quot;Hyperion&quot;, &quot;author&quot;: &quot;Dan Simmons&quot;, &quot;release_date&quot;: &quot;1989-05-26&quot;, &quot;page_count&quot;: 482&#125;</span><br><span class="line">&gt; &#123;&quot;index&quot;:&#123;&quot;_id&quot;: &quot;Dune&quot;&#125;&#125;</span><br><span class="line">&gt; &#123;&quot;name&quot;: &quot;Dune&quot;, &quot;author&quot;: &quot;Frank Herbert&quot;, &quot;release_date&quot;: &quot;1965-06-01&quot;, &quot;page_count&quot;: 604&#125;</span><br><span class="line">&gt; &apos;</span><br><span class="line">&#123;&quot;took&quot;:426,&quot;errors&quot;:false,&quot;items&quot;:[&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;library&quot;,&quot;_type&quot;:&quot;book&quot;,&quot;_id&quot;:&quot;Leviathan Wakes&quot;,&quot;_version&quot;:1,&quot;result&quot;:&quot;created&quot;,&quot;forced_refresh&quot;:true,&quot;_shards&quot;:&#123;&quot;total&quot;:2,&quot;successful&quot;:1,&quot;failed&quot;:0&#125;,&quot;_seq_no&quot;:0,&quot;_primary_term&quot;:1,&quot;status&quot;:201&#125;&#125;,&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;library&quot;,&quot;_type&quot;:&quot;book&quot;,&quot;_id&quot;:&quot;Hyperion&quot;,&quot;_version&quot;:1,&quot;result&quot;:&quot;created&quot;,&quot;forced_refresh&quot;:true,&quot;_shards&quot;:&#123;&quot;total&quot;:2,&quot;successful&quot;:1,&quot;failed&quot;:0&#125;,&quot;_seq_no&quot;:0,&quot;_primary_term&quot;:1,&quot;status&quot;:201&#125;&#125;,&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;library&quot;,&quot;_type&quot;:&quot;book&quot;,&quot;_id&quot;:&quot;Dune&quot;,&quot;_version&quot;:1,&quot;result&quot;:&quot;created&quot;,&quot;forced_refresh&quot;:true,&quot;_shards&quot;:&#123;&quot;total&quot;:2,&quot;successful&quot;:1,&quot;failed&quot;:0&#125;,&quot;_seq_no&quot;:1,&quot;_primary_term&quot;:1,&quot;status&quot;:201&#125;&#125;]&#125;</span><br><span class="line"></span><br><span class="line">#通过SQL REST API执行SQL</span><br><span class="line">[root@localhost ~]# curl -X POST &quot;localhost:9200/_xpack/sql?format=txt&quot; -H &apos;Content-Type: application/json&apos; -d&apos;</span><br><span class="line">&gt; &#123;</span><br><span class="line">&gt;     &quot;query&quot;: &quot;SELECT * FROM library WHERE release_date &lt; \u00272000-01-01\u0027&quot;</span><br><span class="line">&gt; &#125;</span><br><span class="line">&gt; &apos;</span><br><span class="line">    author     |     name      |  page_count   |      release_date      </span><br><span class="line">---------------+---------------+---------------+------------------------</span><br><span class="line">Dan Simmons    |Hyperion       |482            |1989-05-26T00:00:00.000Z</span><br><span class="line">Frank Herbert  |Dune           |604            |1965-06-01T00:00:00.000Z</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#支持SQL REST API</span><br><span class="line">[root@localhost ~]# curl -X POST &quot;localhost:9200/_xpack/sql/translate&quot; -H &apos;Content-Type: application/json&apos; -d&apos;</span><br><span class="line">&gt; &#123;</span><br><span class="line">&gt;     &quot;query&quot;: &quot;SELECT * FROM library ORDER BY page_count DESC&quot;,</span><br><span class="line">&gt;     &quot;fetch_size&quot;: 10</span><br><span class="line">&gt; &#125;</span><br><span class="line">&gt; &apos;</span><br><span class="line">&#123;&quot;size&quot;:10,&quot;_source&quot;:&#123;&quot;includes&quot;:[&quot;author&quot;,&quot;name&quot;],&quot;excludes&quot;:[]&#125;,&quot;docvalue_fields&quot;:[&quot;page_count&quot;,&quot;release_date&quot;],&quot;sort&quot;:[&#123;&quot;page_count&quot;:&#123;&quot;order&quot;:&quot;desc&quot;&#125;&#125;]&#125;</span><br><span class="line"></span><br><span class="line">#支持SQL CLI</span><br><span class="line">[root@localhost ~]# /usr/share/elasticsearch/bin/elasticsearch-sql-cli</span><br><span class="line">     .sssssss.`                     .sssssss.</span><br><span class="line">  .:sXXXXXXXXXXo`                `ohXXXXXXXXXho.</span><br><span class="line"> .yXXXXXXXXXXXXXXo`            `oXXXXXXXXXXXXXXX-</span><br><span class="line">.XXXXXXXXXXXXXXXXXXo`        `oXXXXXXXXXXXXXXXXXX.</span><br><span class="line">.XXXXXXXXXXXXXXXXXXXXo.    .oXXXXXXXXXXXXXXXXXXXXh</span><br><span class="line">.XXXXXXXXXXXXXXXXXXXXXXo``oXXXXXXXXXXXXXXXXXXXXXXy</span><br><span class="line">`yXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX.</span><br><span class="line"> `oXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXo`</span><br><span class="line">   `oXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXo`</span><br><span class="line">     `oXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXo`</span><br><span class="line">       `oXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXo`</span><br><span class="line">         `oXXXXXXXXXXXXXXXXXXXXXXXXXXXXo`</span><br><span class="line">           .XXXXXXXXXXXXXXXXXXXXXXXXXo`</span><br><span class="line">         .oXXXXXXXXXXXXXXXXXXXXXXXXo`</span><br><span class="line">       `oXXXXXXXXXXXXXXXXXXXXXXXXo`   `odo`</span><br><span class="line">     `oXXXXXXXXXXXXXXXXXXXXXXXXo`   `oXXXXXo`</span><br><span class="line">   `oXXXXXXXXXXXXXXXXXXXXXXXXo`   `oXXXXXXXXXo`</span><br><span class="line"> `oXXXXXXXXXXXXXXXXXXXXXXXXo`   `oXXXXXXXXXXXXXo`</span><br><span class="line">`yXXXXXXXXXXXXXXXXXXXXXXXo`    oXXXXXXXXXXXXXXXXX.</span><br><span class="line">.XXXXXXXXXXXXXXXXXXXXXXo`   `oXXXXXXXXXXXXXXXXXXXy</span><br><span class="line">.XXXXXXXXXXXXXXXXXXXXo`     /XXXXXXXXXXXXXXXXXXXXX</span><br><span class="line">.XXXXXXXXXXXXXXXXXXo`        `oXXXXXXXXXXXXXXXXXX-</span><br><span class="line"> -XXXXXXXXXXXXXXXo`            `oXXXXXXXXXXXXXXXo`</span><br><span class="line">  .oXXXXXXXXXXXo`                `oXXXXXXXXXXXo.</span><br><span class="line">    `.sshXXyso`        SQL         `.sshXhss.`</span><br><span class="line"></span><br><span class="line">sql&gt; SELECT * FROM library WHERE release_date &lt; &apos;2000-01-01&apos;;</span><br><span class="line">    author     |     name      |  page_count   |      release_date      </span><br><span class="line">---------------+---------------+---------------+------------------------</span><br><span class="line">Dan Simmons    |Hyperion       |482            |1989-05-26T00:00:00.000Z</span><br><span class="line">Frank Herbert  |Dune           |604            |1965-06-01T00:00:00.000Z</span><br><span class="line"></span><br><span class="line">#支持SQL JDBC</span><br><span class="line"></span><br><span class="line">String address = &quot;jdbc:es://&quot; + elasticsearchAddress;     </span><br><span class="line">Properties connectionProperties = connectionProperties(); </span><br><span class="line">Connection connection = DriverManager.getConnection(address, connectionProperties);</span><br><span class="line">try (Statement statement = connection.createStatement();</span><br><span class="line">        ResultSet results = statement.executeQuery(</span><br><span class="line">            &quot;SELECT name, page_count FROM library ORDER BY page_count DESC LIMIT 1&quot;)) &#123;</span><br><span class="line">    assertTrue(results.next());</span><br><span class="line">    assertEquals(&quot;Don Quixote&quot;, results.getString(1));</span><br><span class="line">    assertEquals(1072, results.getInt(2));</span><br><span class="line">    SQLException e = expectThrows(SQLException.class, () -&gt; results.getInt(1));</span><br><span class="line">    assertTrue(e.getMessage(), e.getMessage().contains(&quot;unable to convert column 1 to an int&quot;));</span><br><span class="line">    assertFalse(results.next());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="支持Java-10"><a href="#支持Java-10" class="headerlink" title="支持Java 10"></a><b>支持Java 10</b></h5><p>Elasticsearch 支持Java9 和Java10，保持和Java快速发布周期一致。但是官方建议大多数用户使用java8，有兴趣的同学可以看<a href="https://jaxenter.com/no-more-public-updates-java-8-143703.html" target="_blank" rel="noopener">这里</a></p>
<h5 id="汇总统计"><a href="#汇总统计" class="headerlink" title="汇总统计"></a><b>汇总统计</b></h5><p>用户可以建立汇总统计job，job会汇聚统计最近搜索的更新数据。这个功能和SQl功能一样，都是实验性质的功能。</p>
<h5 id="安全更新"><a href="#安全更新" class="headerlink" title="安全更新"></a><b>安全更新</b></h5><p>XPackExtension  扩展机制被移除了，引入SPI扩展机制。</p>
<h5 id="Bug-fixes"><a href="#Bug-fixes" class="headerlink" title="Bug fixes"></a><b>Bug fixes</b></h5><p>具体可以参考<a href="https://www.elastic.co/guide/en/logstash/6.3/logstash-6-3-0.html" target="_blank" rel="noopener">这里</a></p>
<p>说了这么多，也许有朋友问了，怎么才能升级到Elasticsearch6.3呢，官方建议是：、<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">1. 6.x到6.y - 可以通过一次升级一个节点来执行</span><br><span class="line">2. 5.x至6.x - 需要完全重启群集</span><br><span class="line">3. 2.x至6.x - 不支持</span><br><span class="line"></span><br><span class="line">[root@localhost ~]# curl -XGET &apos;http://localhost:9200/&apos;</span><br><span class="line">&#123;</span><br><span class="line">  &quot;name&quot; : &quot;99NPxaU&quot;,</span><br><span class="line">  &quot;cluster_name&quot; : &quot;elasticsearch&quot;,</span><br><span class="line">  &quot;cluster_uuid&quot; : &quot;gW7bp0I3RNKvZI50SAsjeg&quot;,</span><br><span class="line">  &quot;version&quot; : &#123;</span><br><span class="line">    &quot;number&quot; : &quot;6.3.0&quot;,</span><br><span class="line">    &quot;build_flavor&quot; : &quot;default&quot;,</span><br><span class="line">    &quot;build_type&quot; : &quot;rpm&quot;,</span><br><span class="line">    &quot;build_hash&quot; : &quot;424e937&quot;,</span><br><span class="line">    &quot;build_date&quot; : &quot;2018-06-11T23:38:03.357887Z&quot;,</span><br><span class="line">    &quot;build_snapshot&quot; : false,</span><br><span class="line">    &quot;lucene_version&quot; : &quot;7.3.1&quot;,</span><br><span class="line">    &quot;minimum_wire_compatibility_version&quot; : &quot;5.6.0&quot;,</span><br><span class="line">    &quot;minimum_index_compatibility_version&quot; : &quot;5.0.0&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;tagline&quot; : &quot;You Know, for Search&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<hr>
<p>您的鼓励是我写作最大的动力</p>
<p>俗话说，投资效率是最好的投资。 如果您感觉我的文章质量不错，读后收获很大，预计能为您提高 10% 的工作效率，不妨小额捐助我一下，让我有动力继续写出更多好文章。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Elasticsearch 官方推出了6.3.0，今天，让我们看一下 Elasticsearch6.3.0给我们带来的新特性吧，如果想看官网的同学可以参考&lt;a href=&quot;https://www.elastic.co/guide/en/elasticsearch/refer
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>how to use sqoop in hadoop</title>
    <link href="https://t1ger.github.io/2018/06/11/how-to-use-sqoop-in-hadoop/"/>
    <id>https://t1ger.github.io/2018/06/11/how-to-use-sqoop-in-hadoop/</id>
    <published>2018-06-11T03:27:36.000Z</published>
    <updated>2018-06-12T10:12:39.676Z</updated>
    
    <content type="html"><![CDATA[<h5 id="install"><a href="#install" class="headerlink" title="install"></a><b>install</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">#download Sqoop 1.4.7 version</span><br><span class="line">[root@localhost ~]# wget https://mirrors.tuna.tsinghua.edu.cn/apache/sqoop/1.4.7/sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz</span><br><span class="line">[root@localhost ~]# tar zxf sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz</span><br><span class="line">[root@localhost ~]# wget https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-8.0.11.tar.gz</span><br><span class="line">#after untar file, move jar package to Sqoop lib directory </span><br><span class="line">[root@localhost ~]# cd /usr/local/sqoop/conf &amp;&amp; cp sqoop-env-template.sh sqoop-env.sh</span><br><span class="line">[root@localhost ~]# cat sqoop-env.sh</span><br><span class="line">#Set path to where bin/hadoop is available</span><br><span class="line">export HADOOP_COMMON_HOME=/usr/local/hadoop</span><br><span class="line">#Set path to where hadoop-*-core.jar is available</span><br><span class="line">export HADOOP_MAPRED_HOME=/usr/local/hadoop/share/hadoop/mapreduce</span><br><span class="line">#set the path to where bin/hbase is available</span><br><span class="line">#export HBASE_HOME=</span><br><span class="line">#Set the path to where bin/hive is available</span><br><span class="line">#export HIVE_HOME=</span><br><span class="line">#Set the path for where zookeper config dir is</span><br><span class="line">#export ZOOCFGDIR=</span><br><span class="line"></span><br><span class="line">#test </span><br><span class="line"> /usr/local/sqoop/bin/sqoop list-databases \</span><br><span class="line">  --connect jdbc:mysql://&lt;dburi&gt;/&lt;dbname&gt;  \</span><br><span class="line">  --username &lt;username&gt; --password &lt;password&gt; </span><br><span class="line"> /usr/local/sqoop/bin/sqoop list-tables  </span><br><span class="line"> --connect jdbc:mysql://&lt;dburi&gt;/&lt;dbname&gt; \</span><br><span class="line"> --username &lt;username&gt; --password &lt;password&gt;</span><br></pre></td></tr></table></figure>
<h5 id="application-scenarios"><a href="#application-scenarios" class="headerlink" title="application scenarios"></a><b>application scenarios</b></h5><p>tips: before you use command,make sure to su hadoop</p>
<ol>
<li>mysql -&gt; hdfs</li>
<li>hdfs  -&gt; mysql</li>
<li>mysql -&gt; hive</li>
<li>hive  -&gt; mysql</li>
<li>use sql as import condition</li>
</ol>
<ul>
<li><p>from mysql to hdfs<br>–check-column (col): Specifies the column to be examined when determining which rows to import. (the column should not be of type CHAR/NCHAR/VARCHAR/VARNCHAR/ LONGVARCHAR/LONGNVARCHAR)<br>–incremental (mode): Specifies how Sqoop determines which rows are new. Legal values for mode include append and lastmodified.<br>–last-value (value): Specifies the maximum value of the check column from the previous import</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line">sqoop import --connect jdbc:mysql://&lt;dburi&gt;/&lt;dbname&gt; \</span><br><span class="line">--username &lt;username&gt; --password &lt;password&gt; \</span><br><span class="line">--table &lt;tablename&gt; --check-column &lt;col&gt; --incremental &lt;mode&gt; --last-value &lt;value&gt; --target-dir &lt;hdfs-dir&gt;</span><br><span class="line"></span><br><span class="line">/usr/local/sqoop/bin/sqoop import \</span><br><span class="line">--connect jdbc:mysql://192.168.100.112/datbase?zeroDateTimeBehavior=CONVERT_TO_NULL \</span><br><span class="line">--username username --password password \</span><br><span class="line">--table pv_daxue \</span><br><span class="line">--target-dir /usr/sqoop/daxue</span><br><span class="line"></span><br><span class="line">#save as parquet(textfile,orcfile,parquet)</span><br><span class="line">/usr/local/sqoop/bin/sqoop import \</span><br><span class="line">--connect jdbc:mysql://192.168.100.112/datbase?zeroDateTimeBehavior=CONVERT_TO_NULL \</span><br><span class="line">--username username --password password \</span><br><span class="line">--table pv_daxue  --target-dir /usr/sqoop/daxue \</span><br><span class="line">--as-parquetfile</span><br><span class="line"></span><br><span class="line">#save columns id,account </span><br><span class="line">/usr/local/sqoop/bin/sqoop import \</span><br><span class="line">--connect jdbc:mysql://192.168.100.112/datbase?zeroDateTimeBehavior=CONVERT_TO_NULL \</span><br><span class="line">--username username --password password \</span><br><span class="line">--table pv_daxue  --target-dir /usr/sqoop/daxue \</span><br><span class="line">--columns id,account \</span><br><span class="line">--as-textfile</span><br><span class="line"></span><br><span class="line">tips: Parameters --as-sequencefile --as-avrodatafile and --as-parquetfile are not supported with --direct params in MySQL case. </span><br><span class="line"># after insert one record ,append import again</span><br><span class="line"># Append mode</span><br><span class="line">/usr/local/sqoop/bin/sqoop import \</span><br><span class="line">--connect jdbc:mysql://192.168.100.112/datbase?zeroDateTimeBehavior=CONVERT_TO_NULL \</span><br><span class="line">--username username --password password \</span><br><span class="line">--table pv_daxue --check-column id \</span><br><span class="line">--incremental append --target-dir /usr/sqoop/daxue \</span><br><span class="line">-last-value 5</span><br><span class="line"></span><br><span class="line"># Lastmodified mode</span><br><span class="line">#first import</span><br><span class="line">/usr/local/sqoop/bin/sqoop import \</span><br><span class="line">--connect jdbc:mysql://192.168.100.112/datbase?zeroDateTimeBehavior=CONVERT_TO_NULL \</span><br><span class="line">--username username --password password \</span><br><span class="line">--table pv_daxue --target-dir /usr/sqoop/daxue -m1</span><br><span class="line"></span><br><span class="line">bash-4.1$ hadoop fs -cat /usr/sqoop/daxue/part-m-00000</span><br><span class="line">1,hello,2018-06-12 23:48:32.0</span><br><span class="line">2,word,2018-06-12 23:48:32.0</span><br><span class="line">3,marry,2018-06-12 23:48:32.0</span><br><span class="line">4,tony,2018-06-12 23:48:32.0</span><br><span class="line">5,jack,2018-06-12 23:48:33.0</span><br><span class="line">6,james,2018-06-12 23:52:03.0</span><br><span class="line">#after insert one record , Lastmodified  import again</span><br><span class="line">/usr/local/sqoop/bin/sqoop import \</span><br><span class="line">--connect jdbc:mysql://192.168.100.112/datbase?zeroDateTimeBehavior=CONVERT_TO_NULL \</span><br><span class="line">--username username --password password \</span><br><span class="line">--table pv_daxue --check-column last_mod  --incremental lastmodified --last-value &quot;2018-06-12 10:52:03&quot; \</span><br><span class="line">--target-dir /usr/sqoop/daxue -m 1 --append </span><br><span class="line"></span><br><span class="line">18/06/12 10:59:48 INFO mapreduce.ImportJobBase: Transferred 60 bytes in 4.2309 seconds (14.1813 bytes/sec)</span><br><span class="line">18/06/12 10:59:48 INFO mapreduce.ImportJobBase: Retrieved 2 records</span><br><span class="line">bash-4.1$ hadoop fs -cat /usr/sqoop/daxue/part-m-00001</span><br><span class="line">6,james,2018-06-12 23:52:03.0</span><br><span class="line">7,hello,2018-06-12 23:58:08.0</span><br><span class="line"></span><br><span class="line">#merage by mode, after execute sql &quot;update customertest set name = &apos;Hello&apos; where id = 1;&quot;</span><br><span class="line">/usr/local/sqoop/bin/sqoop import \</span><br><span class="line">--connect jdbc:mysql://192.168.100.112/datbase?zeroDateTimeBehavior=CONVERT_TO_NULL \</span><br><span class="line">--username username --password password \</span><br><span class="line">--table pv_daxue  --check-column last_mod  --incremental lastmodified --last-value &quot;2018-06-12 23:52:03&quot; \</span><br><span class="line">--target-dir /usr/sqoop/daxue -m 1 --merge-key id </span><br><span class="line"></span><br><span class="line">bash-4.1$ hadoop fs -cat /usr/sqoop/daxue/part-r-00000</span><br><span class="line">1,Hello,2018-06-13 00:07:41.0</span><br><span class="line">2,word,2018-06-12 23:48:32.0</span><br><span class="line">3,marry,2018-06-12 23:48:32.0</span><br><span class="line">4,tony,2018-06-12 23:48:32.0</span><br><span class="line">5,jack,2018-06-12 23:48:33.0</span><br><span class="line">6,james,2018-06-12 23:52:03.0</span><br><span class="line">7,me,2018-06-12 23:58:08.0</span><br></pre></td></tr></table></figure>
</li>
<li><p>from hdfs to mysql<br>According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn’t set</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sqoop export --connect jdbc:mysql://&lt;dburi&gt;/&lt;dbname&gt; \</span><br><span class="line">--username &lt;username&gt; --password &lt;password&gt; \</span><br><span class="line">--table &lt;tablename&gt; --export-dir &lt;hdfs-dir&gt;</span><br><span class="line"></span><br><span class="line">/usr/local/sqoop/bin/sqoop export  \</span><br><span class="line">--connect &quot;jdbc:mysql://192.168.100.112/datbase?useSSL=false&amp;useUnicode=true&amp;characterEncoding=utf-8&quot; \</span><br><span class="line">--username username --password password \</span><br><span class="line">--table pv_daxue  --export-dir /usr/sqoop/daxue</span><br></pre></td></tr></table></figure>
</li>
<li><p>from mysql to hive</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">sqoop import --connect jdbc:mysql://&lt;dburi&gt;/&lt;dbname&gt; \</span><br><span class="line">--username &lt;username&gt; --password &lt;password&gt; \</span><br><span class="line">--table &lt;tablename&gt; --check-column &lt;col&gt; --incremental &lt;mode&gt; --last-value &lt;value&gt; \</span><br><span class="line">--fields-terminated-by &quot;\t&quot; --lines-terminated-by &quot;\n&quot; \</span><br><span class="line">--hive-import --target-dir &lt;hdfs-dir&gt; --hive-table &lt;hive-tablename&gt;</span><br><span class="line"></span><br><span class="line">/usr/local/sqoop/bin/sqoop import \</span><br><span class="line">--connect &quot;jdbc:mysql://192.168.100.112/datbase?useSSL=false&amp;useUnicode=true&amp;characterEncoding=utf-8&quot; \</span><br><span class="line">--username username --password password \</span><br><span class="line">–-table hive_table  -–hive-import  --hive-database database –-hive-table hive_test or -–create-hive-table hive_test \  --delete-target-dir  --split-by id</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># --map-column-hive </span><br><span class="line">MySQL(bigint) --&gt; Hive(bigint) </span><br><span class="line">MySQL(tinyint) --&gt; Hive(tinyint) </span><br><span class="line">MySQL(int) --&gt; Hive(int) </span><br><span class="line">MySQL(double) --&gt; Hive(double) </span><br><span class="line">MySQL(bit) --&gt; Hive(boolean) </span><br><span class="line">MySQL(varchar) --&gt; Hive(string) </span><br><span class="line">MySQL(decimal) --&gt; Hive(double) </span><br><span class="line">MySQL(date/timestamp) --&gt; Hive(string)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">/usr/local/sqoop/bin/sqoop import \</span><br><span class="line">--connect &quot;jdbc:mysql://192.168.100.112/datbase?useSSL=false&amp;useUnicode=true&amp;characterEncoding=utf-8&quot; \</span><br><span class="line">--username username --password password \</span><br><span class="line">–-table hive_table  -–hive-import  \</span><br><span class="line">--map-column-hive cost=&quot;DECIMAL&quot;,date=&quot;DATE&quot; \ </span><br><span class="line">--hive-database database –-hive-table hive_test or -–create-hive-table hive_test \  </span><br><span class="line">--delete-target-dir  --split-by id</span><br></pre></td></tr></table></figure>
</li>
<li><p>from hive to mysql</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#Refer above from hdfs to mysql,only need specify the HDFS path corresponding to the Hive table</span><br><span class="line">/usr/local/sqoop/bin/sqoop export  \</span><br><span class="line">--connect &quot;jdbc:mysql://192.168.100.112/datbase?useSSL=false&amp;useUnicode=true&amp;characterEncoding=utf-8&quot; \</span><br><span class="line">--username username --password password</span><br><span class="line">--table customer --export-dir /user/hive/warehouse/user.db/customer --fields-terminated-by &apos;\001&apos;</span><br></pre></td></tr></table></figure>
</li>
<li><p>use sql as import condition</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sqoop import --connect jdbc:mysql://&lt;dburi&gt;/&lt;dbname&gt; --username &lt;username&gt; --password &lt;password&gt; --query &lt;query-sql&gt; --split-by &lt;sp-column&gt; --hive-import --hive-table &lt;hive-tablename&gt; --target-dir &lt;hdfs-dir&gt;</span><br><span class="line"></span><br><span class="line">/usr/local/sqoop/bin/sqoop import \</span><br><span class="line">--connect jdbc:mysql://192.168.100.112/datbase?zeroDateTimeBehavior=CONVERT_TO_NULL \</span><br><span class="line">--username username --password password --table pv_daxue  --target-dir /usr/sqoop/daxue --delete-target-dir \</span><br><span class="line">--query &apos;select id,account from version where account=&quot;ddd&quot; and $CONDITIONS &apos; \</span><br><span class="line">--as-parquetfile</span><br></pre></td></tr></table></figure>
</li>
<li><p>from mysql to hbase</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">/usr/local/sqoop/bin/sqoop import \</span><br><span class="line">--connect jdbc:mysql://192.168.100.112/datbase?zeroDateTimeBehavior=CONVERT_TO_NULL \</span><br><span class="line">--username username --password password </span><br><span class="line">--query &apos;select id,account from version where account=&quot;ddd&quot; and $CONDITIONS &apos; \</span><br><span class="line">--hbase-table pv_daxue  --hbase-create-table \ </span><br><span class="line">--hbase-row-key id --split-by date -m 7 \ </span><br><span class="line">--column-family tiger</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h5 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a><b>遇到的问题</b></h5><ul>
<li><p>ERROR tool.ImportTool: Import failed: java.io.FileNotFoundException</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">18/06/11 15:42:43 ERROR tool.ImportTool: Import failed: java.io.FileNotFoundException: File does not exist: hdfs://172.16.56.143:8020/usr/local/sqoop-1.4.7.bin__hadoop-2.6.0/lib/parquet-jackson-1.6.0.jar</span><br><span class="line"></span><br><span class="line">[hadoop@node1 conf]$ /usr/local/hadoop/bin/hadoop fs  -mkdir -p /usr/local/sqoop-1.4.7.bin__hadoop-2.6.0/lib</span><br><span class="line">[hadoop@node1 conf]$ /usr/local/hadoop/bin/hadoop fs  -put  /usr/local/sqoop-1.4.7.bin__hadoop-2.6.0/lib/* hdfs://172.16.56.143:8020/usr/local/sqoop-1.4.7.bin__hadoop-2.6.0/lib</span><br></pre></td></tr></table></figure>
</li>
<li><p>Caused by: com.mysql.cj.exceptions.CJException: The connection property ‘zeroDateTimeBehavior’ acceptable values are: ‘CONVERT_TO_NULL’, ‘EXCEPTION’ or ‘ROUND’. The value ‘convertToNull’ is not acceptable.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#using the following code below:</span><br><span class="line">jdbc:mysql://localhost:3306/database?zeroDateTimeBehavior=CONVERT_TO_NULL</span><br></pre></td></tr></table></figure>
<p>  if config lzo ,you  perhaps see ,use command “hadoop checknative” check</p>
</li>
<li><p>ERROR sqoop.Sqoop: Got exception running Sqoop: java.lang.IllegalArgumentException: Compression codec com.hadoop.compression.lzo.LzoCodec not found.<br>java.lang.IllegalArgumentException: Compression codec com.hadoop.compression.lzo.LzoCodec not found.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># install lzo support</span><br></pre></td></tr></table></figure>
</li>
<li><p>No primary key could be found for tablescore</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">specify one with --split-by or perform a sequential import with&apos;-m 1&apos;</span><br></pre></td></tr></table></figure>
</li>
<li><p>ERROR hive.HiveConfig: Could not load org.apache.hadoop.hive.conf.HiveConf. Make sure HIVE_CONF_DIR is set correctly.<br>ERROR tool.ImportTool: Import failed: java.io.IOException: java.lang.ClassNotFoundException: org.apache.hadoop.hive.conf.HiveConf</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#add this one in .bash_profile:</span><br><span class="line">export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:/usr/lib/hive/lib/*</span><br><span class="line">source ~/.bash_profile</span><br></pre></td></tr></table></figure>
</li>
<li><p>FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException(message:For direct MetaStore DB connections, we don’t support retries at the client level.)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; alter database hive character set latin1;</span><br></pre></td></tr></table></figure>
</li>
<li><p>java.lang.RuntimeException: Can’t parse input data: ‘1Hello2018-06-13 00:07:41.0’</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">--fields-terminated-by &apos;\001&apos;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>ref<br><a href="http://sqoop.apache.org/docs/1.4.7/SqoopUserGuide.html#_incremental_imports" target="_blank" rel="noopener">Incremental Imports</a><br><a href="https://www.cnblogs.com/ljy2013/p/4872126.html" target="_blank" rel="noopener">sqoop的增量导入（increment import）</a><br><a href="https://github.com/kevinweil/hadoop-lzo" target="_blank" rel="noopener">hadoop-lzo</a><br><a href="http://www.oberhumer.com/opensource/lzo/#download" target="_blank" rel="noopener">lzo</a><br><a href="https://www.zybuluo.com/aitanjupt/note/209968#%E4%BD%BF%E7%94%A8sqoop%E4%BB%8Emysql%E5%AF%BC%E5%85%A5%E6%95%B0%E6%8D%AE%E5%88%B0hive" target="_blank" rel="noopener">Sqoop从MySQL导入数据</a></p>
<hr>
<p>您的鼓励是我写作最大的动力</p>
<p>俗话说，投资效率是最好的投资。 如果您感觉我的文章质量不错，读后收获很大，预计能为您提高 10% 的工作效率，不妨小额捐助我一下，让我有动力继续写出更多好文章。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;install&quot;&gt;&lt;a href=&quot;#install&quot; class=&quot;headerlink&quot; title=&quot;install&quot;&gt;&lt;/a&gt;&lt;b&gt;install&lt;/b&gt;&lt;/h5&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Rsyslog 连接 Kafka 指北</title>
    <link href="https://t1ger.github.io/2018/05/22/Rsyslog-%E8%BF%9E%E6%8E%A5-Kafka-%E6%8C%87%E5%8C%97/"/>
    <id>https://t1ger.github.io/2018/05/22/Rsyslog-连接-Kafka-指北/</id>
    <published>2018-05-22T07:20:48.000Z</published>
    <updated>2018-05-22T06:51:56.401Z</updated>
    
    <content type="html"><![CDATA[<h5 id="运行环境"><a href="#运行环境" class="headerlink" title="运行环境"></a><b>运行环境</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@bogon ~]# cat /etc/redhat-release </span><br><span class="line">CentOS Linux release 7.4.1708 (Core) </span><br><span class="line"></span><br><span class="line">[root@bogon ~]# rpm -qa|grep rsyslog</span><br><span class="line">rsyslog-kafka-8.28.0-1.el7.x86_64</span><br><span class="line">rsyslog-8.28.0-1.el7.x86_64</span><br></pre></td></tr></table></figure>
<h5 id="安装"><a href="#安装" class="headerlink" title="安装"></a><b>安装</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">wget -O /etc/yum.repos.d/rsyslog.repo http://rpms.adiscon.com/v8-stable/rsyslog.repo</span><br><span class="line">yum install rsyslog rsyslog-kafka.x86_64</span><br></pre></td></tr></table></figure>
<p>国内的同学可能无法安装，同学们也可以通过<a href="http://rpms.adiscon.com/v8-stable/epel-7/x86_64/RPMS/" target="_blank" rel="noopener">这里</a>下载安装</p>
<h5 id="配置"><a href="#配置" class="headerlink" title="配置"></a><b>配置</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@bogon ~]# cat /etc/rsyslog.d/kafka.conf</span><br><span class="line">module(load=&quot;omkafka&quot;)</span><br><span class="line">action (</span><br><span class="line">        type=&quot;omkafka&quot;</span><br><span class="line">        topic=&quot;topicA&quot;</span><br><span class="line">        broker=&quot;cdh1:9092,cdh2:9092,cdh3:9092&quot;</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#如果保存到本地</span><br><span class="line">[root@bogon ~]# cat /etc/rsyslog.d/router.conf</span><br><span class="line">template (name=&quot;DynFile&quot; type=&quot;string&quot; string=&quot;/data/%fromhost-ip%.log&quot;)</span><br><span class="line">if $fromhost-ip startswith &apos;192.168.100.2&apos; and $programname != &apos;Type=SESSION;&apos; and $programname != &apos;Type=Login;&apos; and $programname != &apos;Type=AuthLog;&apos; and $programname != &apos;Type=Ftp&apos; then &#123;</span><br><span class="line">    action(type=&quot;omfile&quot; dynaFile=&quot;DynFile&quot;)</span><br><span class="line">    stop</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>低版本的rsyslog保存到本地配置如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]# rpm -qa|grep rsyslog</span><br><span class="line">rsyslog-5.8.10-6.el6.x86_64</span><br><span class="line"></span><br><span class="line">添加到 /etc/rsyslog.conf </span><br><span class="line">#### GLOBAL DIRECTIVES ####</span><br><span class="line">$template RemoteLogs,&quot;/data/var/log/%HOSTNAME%/%fromhost-ip%_%$YEAR%-%$MONTH%-%$DAY%.log&quot; *</span><br><span class="line">*.* ?RemoteLogs</span><br><span class="line">&amp;~</span><br></pre></td></tr></table></figure></p>
<p>通过kafka查看消息<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/kafka&amp;&amp; bin/kafka-console-consumer.sh  --bootstrap-server cdh1:9092,cdh2:9092,cdh3:9092   --topic topicA</span><br></pre></td></tr></table></figure></p>
<p>ref<br><a href="https://doc.yonyoucloud.com/doc/logstash-best-practice-cn/ecosystem/rsyslog.html" target="_blank" rel="noopener">Rsyslog</a><br><a href="http://wdxtub.com/2016/08/17/rsyslog-kafka-guide/" target="_blank" rel="noopener">Rsyslog 连接 Kafka 指南</a><br><a href="https://serverfault.com/questions/807108/how-to-call-template-so-rsyslog-8-creates-one-log-file-per-client" target="_blank" rel="noopener">How to call template so rsyslog 8 creates one log file per client</a><br><a href="http://wiki.rsyslog.com/index.php/DailyLogRotation" target="_blank" rel="noopener">DailyLogRotation</a><br><a href="http://blog.kompaz.win/2018/01/11/20180111%20CentOS7%20rsyslog%20+loganalyzer%E9%85%8D%E7%BD%AE/" target="_blank" rel="noopener">CentOS7 rsyslog +loganalyzer配置</a></p>
<hr>
<p>您的鼓励是我写作最大的动力</p>
<p>俗话说，投资效率是最好的投资。 如果您感觉我的文章质量不错，读后收获很大，预计能为您提高 10% 的工作效率，不妨小额捐助我一下，让我有动力继续写出更多好文章。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;运行环境&quot;&gt;&lt;a href=&quot;#运行环境&quot; class=&quot;headerlink&quot; title=&quot;运行环境&quot;&gt;&lt;/a&gt;&lt;b&gt;运行环境&lt;/b&gt;&lt;/h5&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutt
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Spark-Streaming with Kafka Programming</title>
    <link href="https://t1ger.github.io/2018/05/08/Spark-Streaming-with-Kafka-Programming/"/>
    <id>https://t1ger.github.io/2018/05/08/Spark-Streaming-with-Kafka-Programming/</id>
    <published>2018-05-08T07:55:06.000Z</published>
    <updated>2018-05-10T10:31:07.232Z</updated>
    
    <content type="html"><![CDATA[<h5 id="运行环境"><a href="#运行环境" class="headerlink" title="运行环境"></a><b>运行环境</b></h5><ol>
<li><p>jdk环境</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@cdh1 kafka]# java -version</span><br><span class="line">java version &quot;1.8.0_112&quot;</span><br><span class="line">Java(TM) SE Runtime Environment (build 1.8.0_112-b15)</span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM (build 25.112-b15, mixed mode)</span><br></pre></td></tr></table></figure>
</li>
<li><p>引入maven</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">&lt;artifactId&gt;spark-streaming_2.11&lt;/artifactId&gt;</span><br><span class="line">&lt;version&gt;2.3.0&lt;/version&gt;</span><br><span class="line"></span><br><span class="line">&lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">&lt;artifactId&gt;spark-streaming-kafka-0-10_2.11&lt;/artifactId&gt;</span><br><span class="line">&lt;version&gt;2.3.0&lt;/version&gt;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h5 id="示例WordCount"><a href="#示例WordCount" class="headerlink" title="示例WordCount"></a><b>示例WordCount</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br></pre></td><td class="code"><pre><span class="line">package cn.spark.streaming;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">import java.util.Arrays;</span><br><span class="line">import java.util.Collection;</span><br><span class="line">import java.util.HashMap;</span><br><span class="line">import java.util.HashSet;</span><br><span class="line">import java.util.Iterator;</span><br><span class="line">import java.util.Map;</span><br><span class="line">import org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line">import org.apache.spark.SparkConf;</span><br><span class="line">import org.apache.spark.TaskContext;</span><br><span class="line">import org.apache.spark.api.java.function.FlatMapFunction;</span><br><span class="line">import org.apache.spark.api.java.function.Function2;</span><br><span class="line">import org.apache.spark.api.java.function.PairFunction;</span><br><span class="line">import org.apache.spark.streaming.Durations;</span><br><span class="line">import org.apache.spark.streaming.api.java.JavaDStream;</span><br><span class="line">import org.apache.spark.streaming.api.java.JavaInputDStream;</span><br><span class="line">import org.apache.spark.streaming.api.java.JavaPairDStream;</span><br><span class="line">import org.apache.spark.streaming.api.java.JavaStreamingContext;</span><br><span class="line">import org.apache.spark.streaming.kafka010.CanCommitOffsets;</span><br><span class="line">import org.apache.spark.streaming.kafka010.ConsumerStrategies;</span><br><span class="line">import org.apache.spark.streaming.kafka010.HasOffsetRanges;</span><br><span class="line">import org.apache.spark.streaming.kafka010.KafkaUtils;</span><br><span class="line">import org.apache.spark.streaming.kafka010.LocationStrategies;</span><br><span class="line">import org.apache.spark.streaming.kafka010.OffsetRange;</span><br><span class="line"></span><br><span class="line">import scala.Tuple2;</span><br><span class="line"></span><br><span class="line">public class KafkaDirectWordCount &#123;</span><br><span class="line"></span><br><span class="line">	public static void main(String[] args) throws InterruptedException &#123;</span><br><span class="line">		// TODO Auto-generated method stub</span><br><span class="line">		</span><br><span class="line">		SparkConf  conf = new SparkConf()</span><br><span class="line">				.setAppName(&quot;KafkaReceiveWordCount&quot;)</span><br><span class="line">				.setMaster(&quot;local[2]&quot;);</span><br><span class="line">		JavaStreamingContext jssc = new JavaStreamingContext(conf,Durations.seconds(5));</span><br><span class="line">		</span><br><span class="line">		String brokers = &quot;192.168.10.140:9092,192.168.10.141:9092,192.168.10.142:9092&quot;;</span><br><span class="line">		</span><br><span class="line">		Map&lt;String, Object&gt; kafkaparams = new HashMap&lt;&gt;();</span><br><span class="line">		kafkaparams.put(&quot;metadata.broker.list&quot;, brokers);</span><br><span class="line">		kafkaparams.put(&quot;bootstrap.servers&quot;, brokers);</span><br><span class="line">		kafkaparams.put(&quot;group.id&quot;, &quot;KafkaDirectWordCount&quot;);</span><br><span class="line">		kafkaparams.put(&quot;key.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);</span><br><span class="line">		kafkaparams.put(&quot;key.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;);</span><br><span class="line">		kafkaparams.put(&quot;value.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;);</span><br><span class="line">		kafkaparams.put(&quot;enable.auto.commit&quot;, false);</span><br><span class="line">		kafkaparams.put(&quot;auto.offset.reset&quot;, &quot;latest&quot;); // earliest latest none </span><br><span class="line">		kafkaparams.put(&quot;offsets.storage&quot;, &quot;kafka&quot;);</span><br><span class="line">		</span><br><span class="line">		Collection&lt;String&gt; topics = new HashSet&lt;String&gt;();</span><br><span class="line">		topics.add(&quot;topicA&quot;);		</span><br><span class="line">		</span><br><span class="line">//		Map&lt;TopicPartition,Long&gt; offsets = new HashMap&lt;&gt;();</span><br><span class="line">//		offsets.put(new TopicPartition(&quot;topicA&quot;,0),2L);</span><br><span class="line">		</span><br><span class="line">		JavaInputDStream&lt;ConsumerRecord&lt;String,String&gt;&gt; lines = KafkaUtils.createDirectStream(</span><br><span class="line">				jssc,</span><br><span class="line">				LocationStrategies.PreferConsistent(),</span><br><span class="line">				ConsumerStrategies.Subscribe(topics, kafkaparams)</span><br><span class="line">				);</span><br><span class="line">		</span><br><span class="line">			lines.foreachRDD(rdd -&gt; &#123;</span><br><span class="line">			  OffsetRange[] offsetRanges = ((HasOffsetRanges) rdd.rdd()).offsetRanges();</span><br><span class="line">			  rdd.foreachPartition(consumerRecords -&gt; &#123;</span><br><span class="line">			    OffsetRange o = offsetRanges[TaskContext.get().partitionId()];</span><br><span class="line">			    System.out.println(</span><br><span class="line">			      o.topic() + &quot; &quot; + o.partition()  + &quot; &quot; + o.fromOffset() + &quot; &quot; + o.untilOffset());</span><br><span class="line">			  &#125;);</span><br><span class="line">			&#125;);</span><br><span class="line">		</span><br><span class="line">	    JavaDStream&lt;String&gt; words = lines.flatMap(new FlatMapFunction&lt;ConsumerRecord&lt;String,String&gt;,String&gt;()&#123;</span><br><span class="line">			private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">			@Override</span><br><span class="line">			public Iterator&lt;String&gt; call(ConsumerRecord&lt;String, String&gt; line) throws Exception &#123;</span><br><span class="line">				// TODO Auto-generated method stub</span><br><span class="line">				return Arrays.asList(line.value().split(&quot; &quot;)).iterator();</span><br><span class="line">			&#125;</span><br><span class="line">	    	</span><br><span class="line">	    &#125;);</span><br><span class="line"></span><br><span class="line">		</span><br><span class="line">		JavaPairDStream&lt;String,Integer&gt; paris = words.mapToPair(new PairFunction&lt;String,String,Integer&gt;()&#123;</span><br><span class="line">			private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">			@Override</span><br><span class="line">			public Tuple2&lt;String, Integer&gt; call(String word) throws Exception &#123;</span><br><span class="line">				// TODO Auto-generated method stub</span><br><span class="line">				return new Tuple2&lt;String,Integer&gt;(word,1);</span><br><span class="line">			&#125;</span><br><span class="line">			</span><br><span class="line">		&#125;);</span><br><span class="line">		</span><br><span class="line">	 JavaPairDStream&lt;String,Integer&gt; wordcount= paris.reduceByKey(new Function2&lt;Integer,Integer,Integer&gt;()&#123;</span><br><span class="line">		private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">		@Override</span><br><span class="line">		public Integer call(Integer v1, Integer v2) throws Exception &#123;</span><br><span class="line">			// TODO Auto-generated method stub</span><br><span class="line">			return v1 + v2;</span><br><span class="line">		&#125;</span><br><span class="line">		 </span><br><span class="line">	 &#125;);</span><br><span class="line">	 </span><br><span class="line">	 </span><br><span class="line">	 wordcount.print();</span><br><span class="line">	 jssc.start();</span><br><span class="line">	 jssc.awaitTermination();</span><br><span class="line">	 jssc.close();</span><br><span class="line">	 </span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>运行之前开启生产者：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/kafka</span><br><span class="line">bin/kafka-console-producer.sh --broker-list 192.168.10.140:9092,192.168.10.141:9092,192.168.10.142:9092 --topic topicA</span><br><span class="line">&gt; hello word hello me</span><br></pre></td></tr></table></figure></p>
<p>ref<br><a href="https://github.com/jaceklaskowski/spark-streaming-notebook/blob/master/spark-streaming-kafka-DirectKafkaInputDStream.adoc" target="_blank" rel="noopener">DirectKafkaInputDStream — Direct Kafka DStream</a><br><a href="http://spark.apache.org/docs/latest/streaming-kafka-0-10-integration.html" target="_blank" rel="noopener">Creating a Direct Stream</a><br><a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html#input-dstreams-and-receivers" target="_blank" rel="noopener">Spark Streaming Programming Guide</a><br><a href="http://blog.cloudera.com/blog/2017/06/offset-management-for-apache-kafka-with-apache-spark-streaming/" target="_blank" rel="noopener">Offset Management For Apache Kafka With Apache Spark Streaming</a><br><a href="https://blog.csdn.net/xueba207/article/details/51135423" target="_blank" rel="noopener">Spark Streaming ‘numRecords must not be negative’问题解决</a><br><a href="https://blog.csdn.net/lishuangzhe7047/article/details/74530417" target="_blank" rel="noopener">Kafka auto.offset.reset值详解</a><br><a href="https://blog.csdn.net/Dax1n/article/details/61614379" target="_blank" rel="noopener">Spark整合kafka0.10.0新特性(一)</a><br><a href="https://blog.csdn.net/sinat_27545249/article/details/78090872" target="_blank" rel="noopener">kafka0.8版本和sparkstreaming整合的两种不同方式</a><br><a href="https://blog.csdn.net/qfwyp0714/article/details/73998293" target="_blank" rel="noopener">Spark streaming 跟踪kafka offset的问题研究</a></p>
<hr>
<p>您的鼓励是我写作最大的动力</p>
<p>俗话说，投资效率是最好的投资。 如果您感觉我的文章质量不错，读后收获很大，预计能为您提高 10% 的工作效率，不妨小额捐助我一下，让我有动力继续写出更多好文章。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;运行环境&quot;&gt;&lt;a href=&quot;#运行环境&quot; class=&quot;headerlink&quot; title=&quot;运行环境&quot;&gt;&lt;/a&gt;&lt;b&gt;运行环境&lt;/b&gt;&lt;/h5&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;jdk环境&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;tab
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>how to config Filebeat6 quickly</title>
    <link href="https://t1ger.github.io/2018/04/11/how-to-config-Filebeat6-quickly/"/>
    <id>https://t1ger.github.io/2018/04/11/how-to-config-Filebeat6-quickly/</id>
    <published>2018-04-11T11:39:00.000Z</published>
    <updated>2018-04-19T08:04:50.200Z</updated>
    
    <content type="html"><![CDATA[<h5 id="环境介绍"><a href="#环境介绍" class="headerlink" title="环境介绍"></a><b>环境介绍</b></h5><p>系统为Centos6.8,相关软件版本如下：<br>filebeat-6.2.3<br>redis-3.0.7<br>logstash-6.2.3<br>kibana-6.2.3</p>
<p>架构为前端filebeat 读取nginx日志或其他日志（json格式），输出到中间redis，后端logstash从redis读取并解析</p>
<h5 id="filebeat-yml配置"><a href="#filebeat-yml配置" class="headerlink" title="filebeat.yml配置"></a><b>filebeat.yml配置</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line">#filebeat配置</span><br><span class="line">cat /etc/filebeat/filebeat.yml </span><br><span class="line">filebeat.config_dir: prospectors.d</span><br><span class="line">filebeat.config.prospectors:</span><br><span class="line">  enabled: true</span><br><span class="line">  path: prospectors.d/*.yml</span><br><span class="line">  reload.enabled: true</span><br><span class="line">  reload.period: 10s </span><br><span class="line">filebeat.prospectors:</span><br><span class="line">- type: log</span><br><span class="line">  enabled: false</span><br><span class="line">  paths:</span><br><span class="line">    - /var/log/message</span><br><span class="line">filebeat.config.modules:</span><br><span class="line">  path: $&#123;path.config&#125;/modules.d/*.yml</span><br><span class="line">  reload.enabled: true</span><br><span class="line">  reload.period: 10s</span><br><span class="line">setup.template.settings:</span><br><span class="line">  index.number_of_shards: 3</span><br><span class="line">setup.kibana:</span><br><span class="line"> </span><br><span class="line">output.file:   #主要用于调试</span><br><span class="line">   path: &quot;/tmp&quot;</span><br><span class="line">   filename: filebeat.out</span><br><span class="line">   number_of_files: 7</span><br><span class="line">   rotate_every_kb: 10000 </span><br><span class="line">   enabled: false   #关闭输出</span><br><span class="line">output.redis:</span><br><span class="line">   hosts: [&quot;192.168.90.147:6379&quot;]</span><br><span class="line">   password: &quot;password&quot;</span><br><span class="line">   key: &quot;filebeat&quot;</span><br><span class="line">   db: 0</span><br><span class="line">   timeout: 60</span><br><span class="line">   max_retires: 3</span><br><span class="line">   bulk_max_size: 4096</span><br><span class="line">   datatype: list</span><br><span class="line">   keys:</span><br><span class="line">     - key: &quot;%&#123;[fields.log_source]&#125;&quot;</span><br><span class="line">       mapping:</span><br><span class="line">         &quot;bash_history&quot;: &quot;command-log&quot;</span><br><span class="line">         &quot;nginx&quot;  : &quot;nginx-log&quot;</span><br><span class="line"></span><br><span class="line">#bash历史记录</span><br><span class="line">[root@localhost ~]# cat /etc/filebeat/prospectors.d/history.yml </span><br><span class="line">- type: log</span><br><span class="line">  enabled: true</span><br><span class="line">  paths:</span><br><span class="line">    - /var/log/command.log  </span><br><span class="line">  fields:</span><br><span class="line">    log_source: command-log</span><br><span class="line">#  tags: &quot;bash_history&quot;</span><br><span class="line">  json.keys_under_root: true</span><br><span class="line">  json.add_error_key: true</span><br><span class="line">  json.message_key: TIME</span><br><span class="line">  </span><br><span class="line">#nginx日志配置</span><br><span class="line">[root@localhost ~]# cat /etc/filebeat/prospectors.d/nginx.yml</span><br><span class="line">- type: log</span><br><span class="line">  enabled: true</span><br><span class="line">  paths:</span><br><span class="line">    - /usr/local/openresty/nginx/logs/cms_log.log</span><br><span class="line">  fields:</span><br><span class="line">    log_source: nginx-log</span><br><span class="line">  exclude_lines: [&quot;helo.html&quot;]</span><br></pre></td></tr></table></figure>
<h5 id="JSON文件格式"><a href="#JSON文件格式" class="headerlink" title="JSON文件格式"></a><b>JSON文件格式</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">#bash_history为json格式，添加到/etc/profile文件</span><br><span class="line">HISTDIR=&apos;/var/log/command.log&apos;</span><br><span class="line">if [ ! -f $HISTDIR ];then</span><br><span class="line">touch $HISTDIR</span><br><span class="line">chmod 666 $HISTDIR</span><br><span class="line">fi</span><br><span class="line">export HISTTIMEFORMAT=&quot;&#123;\&quot;TIME\&quot;:\&quot;%F %T\&quot;,\&quot;HOSTNAME\&quot;:\&quot;$HOSTNAME\&quot;,\&quot;LI\&quot;:\&quot;$(who -u am i 2&gt;/dev/null| awk &apos;&#123;print $NF&#125;&apos;|sed -e &apos;s/[()]//g&apos;)\&quot;,\&quot;LU\&quot;:\&quot;$(who am i|awk &apos;&#123;print $1&#125;&apos;)\&quot;,\&quot;NU\&quot;:\&quot;$&#123;USER&#125;\&quot;,\&quot;CMD\&quot;:\&quot;&quot;</span><br><span class="line">export PROMPT_COMMAND=&apos;history 1|tail -1|sed &quot;s/^[ ]\+[0-9]\+  //&quot;|sed &quot;s/$/\&quot;&#125;/&quot;&gt;&gt; /var/log/command.log&apos;</span><br><span class="line"></span><br><span class="line">#nginx日志格式为</span><br><span class="line">        log_format json &apos;&#123;&quot;@timestamp&quot;:&quot;$time_local&quot;,&apos;</span><br><span class="line">                &apos;&quot;source&quot;:&quot;nginx147&quot;,&apos;</span><br><span class="line">                &apos;&quot;serverAddr&quot;:&quot;$server_addr&quot;,&apos;</span><br><span class="line">                &apos;&quot;remoteAddr&quot;:&quot;$remote_addr&quot;,&apos;</span><br><span class="line">                &apos;&quot;remoteUser&quot;:&quot;$remote_user&quot;,&apos;</span><br><span class="line">                &apos;&quot;size&quot;:$body_bytes_sent,&apos;</span><br><span class="line">                &apos;&quot;status&quot;:$status,&apos;</span><br><span class="line">                &apos;&quot;time&quot;:$request_time,&apos;</span><br><span class="line">                &apos;&quot;method&quot;:&quot;$request_method&quot;,&apos;</span><br><span class="line">                &apos;&quot;protocol&quot;:&quot;$server_protocol&quot;,&apos;</span><br><span class="line">                &apos;&quot;url&quot;:&quot;$scheme://$host$request_uri&quot;,&apos;</span><br><span class="line">                &apos;&quot;host&quot;:&quot;$http_host&quot;,&apos;</span><br><span class="line">                &apos;&quot;uri&quot;:&quot;$uri&quot;,&apos;</span><br><span class="line">                &apos;&quot;referer&quot;:&quot;$http_referer&quot;,&apos;</span><br><span class="line">                &apos;&quot;xforwarded&quot;:&quot;$http_x_forwarded_for&quot;,&apos;</span><br><span class="line">                &apos;&quot;agent&quot;:&quot;$http_user_agent&quot;,&apos;</span><br><span class="line">                &apos;&quot;upsTime&quot;:&quot;$upstream_response_time&quot;,&apos;</span><br><span class="line">                &apos;&quot;sslPro&quot;:&quot;$ssl_protocol&quot;,&apos;</span><br><span class="line">                &apos;&quot;sslCip&quot;:&quot;$ssl_cipher&quot;,&apos;</span><br><span class="line">                &apos;&quot;upsStatus&quot;:&quot;$upstream_status&quot;&#125;&apos;;</span><br></pre></td></tr></table></figure>
<h5 id="logstash配置"><a href="#logstash配置" class="headerlink" title="logstash配置"></a><b>logstash配置</b></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]# cat /etc/logstash/conf.d/history-logstash.conf </span><br><span class="line">input &#123;</span><br><span class="line">    redis &#123;</span><br><span class="line">        data_type =&gt; &quot;list&quot;  </span><br><span class="line">        host =&gt; &quot;192.168.90.147&quot;</span><br><span class="line">        port =&gt; &quot;6379&quot;</span><br><span class="line">        password =&gt; &quot;password&quot;</span><br><span class="line">        key  =&gt; &quot;command-log&quot;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">output &#123;</span><br><span class="line">       if [fields][log_source] == &quot;command-log&quot; &#123; </span><br><span class="line">      elasticsearch &#123;</span><br><span class="line">          hosts   =&gt; [&quot;192.16.90.149:9200&quot;]</span><br><span class="line">          manage_template =&gt; false</span><br><span class="line">          action  =&gt; &quot;index&quot;</span><br><span class="line">          index   =&gt; &quot;command-log-%&#123;+YYYY.MM.dd&#125;&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">#nginx</span><br><span class="line">[root@localhost ~]# cat /etc/logstash/conf.d/nginx-logstash.conf </span><br><span class="line">input &#123;</span><br><span class="line">    redis &#123;</span><br><span class="line">        data_type =&gt; &quot;list&quot;  </span><br><span class="line">        host =&gt; &quot;192.168.90.147&quot;</span><br><span class="line">        port =&gt; &quot;6379&quot;</span><br><span class="line">        password =&gt; &quot;password&quot;</span><br><span class="line">        key  =&gt; &quot;nginx-log&quot;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">output &#123;</span><br><span class="line"></span><br><span class="line">	 if [fields][log_source] == &quot;nginx-log&quot; &#123;</span><br><span class="line">        file &#123;</span><br><span class="line">        path =&gt; &quot;/tmp/logs/nginx-%&#123;+YYYY-MM-dd&#125;.log&quot;</span><br><span class="line">        &#125;</span><br><span class="line">   &#125;</span><br><span class="line">   </span><br><span class="line">#     elasticsearch &#123;</span><br><span class="line">#        hosts   =&gt; [&quot;192.168.90.149:9200&quot;]</span><br><span class="line">#        action  =&gt; &quot;index&quot;</span><br><span class="line">#        index   =&gt; &quot;nginx-log-%&#123;+YYYY.MM.dd&#125;&quot;</span><br><span class="line">#    &#125;</span><br><span class="line"></span><br><span class="line">#        stdout &#123; codec =&gt; rubydebug &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>备注：<br>logstash安装完后需要执行以下命令，进行service服务安装<br>/usr/share/logstash/bin/system-install /etc/logstash/startup.options sysv</p>
<p>ref<br><a href="https://jkzhao.github.io/2017/10/24/Filebeat%E6%97%A5%E5%BF%97%E6%94%B6%E9%9B%86%E5%99%A8/" target="_blank" rel="noopener">Filebeat日志收集器</a><br><a href="https://blog.csdn.net/jianblog/article/details/54669203" target="_blank" rel="noopener">Elastic测试笔记</a><br><a href="https://www.elastic.co/guide/en/beats/filebeat/current/redis-output.html" target="_blank" rel="noopener">Configure the Redis output</a></p>
<hr>
<p>您的鼓励是我写作最大的动力</p>
<p>俗话说，投资效率是最好的投资。 如果您感觉我的文章质量不错，读后收获很大，预计能为您提高 10% 的工作效率，不妨小额捐助我一下，让我有动力继续写出更多好文章。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;环境介绍&quot;&gt;&lt;a href=&quot;#环境介绍&quot; class=&quot;headerlink&quot; title=&quot;环境介绍&quot;&gt;&lt;/a&gt;&lt;b&gt;环境介绍&lt;/b&gt;&lt;/h5&gt;&lt;p&gt;系统为Centos6.8,相关软件版本如下：&lt;br&gt;filebeat-6.2.3&lt;br&gt;redis-3.0.7
    
    </summary>
    
    
  </entry>
  
</feed>
